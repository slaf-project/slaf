{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SLAF (Sparse Lazy Array Format)","text":"SLAF is a high-performance format for single-cell data that combines the power of SQL with lazy evaluation, built on top of the Lance table format and Polars.    <ul> <li> <p> OLAP-Powered SQL   Embedded, in-process OLAP engines for blazing-fast, pushdown SQL queries and advanced analytics.</p> </li> <li> <p> Memory Efficient   Lazy evaluation means you only load what you need, perfect for large-scale single-cell analysis.</p> </li> <li> <p> Scanpy Compatible   Drop-in replacement for AnnData workflows with familiar numpy-like slicing.</p> </li> <li> <p> Concurrent, Cloud-Scale Access   Built for distributed teams and interactive exploration with high QPS, zero-copy storage.</p> </li> <li> <p> Foundation Model Ready   Designed for distributed ML training with SQL-level tokenization and pre-built dataloaders.</p> </li> <li> <p> Visualization Ready   Designed to keep your scGPT (or favorite) embeddings alongside your UMAPs and build interactive dashboards like cellxgene using vector search</p> </li> </ul> <p>See our detailed benchmarks for bioinformaticians and ML engineers.</p>"},{"location":"#why-slaf","title":"Why SLAF?","text":"<p>Single-cell datasets have scaled 2,000-fold in less than a decade.</p> <p>A typical study used to have 50k cells that could easily be copied from object store to a SSD across the network. It could then be read entirely into memory and processed with in-memory operations. At the 100M-cell scale: network, storage, and memory become bottlenecks.</p> <p>The analytic workload is stuck in in-memory single-node operations.</p> <p>Traditional bioinformatics workflows comprised cell and gene filtering, count normalization, visualization via PCA or UMAP, interactive cell typing, and statistical analysis of differential expression. Today, we need to do all those things at 2000x the scale.</p> <p>New fundamentally different AI-native workflows have arrived.</p> <p>Unlike before, we want to:</p> <ul> <li>Scale cell typing using nearest neighbor search on cell embeddings</li> <li>Rank gene-gene relationships using nearest neighbor search on gene embeddings</li> <li>Train transformer-like foundation models with efficient tokenization</li> <li>Distribute workloads across nodes or GPUs by streaming random batches concurrently</li> </ul> <p>We need cloud-native, zero-copy, query-in-place storage systems, rather than maintaining multiple copies of massive datasets per embedding model, node or experiment while continuing to experience the endorphins of numpy-like sparse matrix slicing, and the scanpy pipelines we've built over the years.</p>"},{"location":"#who-is-slaf-for","title":"Who is SLAF for?","text":"<p>SLAF is designed for the modern single-cell ecosystem facing scale challenges:</p> <p>Bioinformaticians</p> <p>Struggling with OOM errors and data transfer issues on 10M+ cell datasets. Can't do self-service analysis without infrastructure engineers. SLAF eliminates the human bottleneck with lazy evaluation.</p> <p>Foundation Model Builders</p> <p>Need to maximize experiments per unit time and resource. Currently copying data per node on attached storage. SLAF enables cloud-native streaming to eliminate data duplication.</p> <p>Tech Leaders &amp; Architects</p> <p>Managing storage/compute infrastructure for teams. 5 bioinformaticians \u00d7 500GB dataset = 2.5TB of duplicated data. SLAF provides zero-copy, query-in-place storage.</p> <p>Tool Builders</p> <p>Want to deliver better interactive experiences on massive datasets using commodity web services. SLAF enables concurrent, cloud-scale access with high QPS.</p> <p>Atlas Builders</p> <p>Need to serve massive datasets to the research community. SLAF provides cloud-native, zero-copy storage for global distribution.</p> <p>Data Integrators</p> <p>Harmonizing PB-scale datasets across atlases. SLAF's SQL-native design enables complex data integration with pushdown optimization.</p>"},{"location":"#win-with-slaf","title":"Win with SLAF","text":""},{"location":"#leverage-pushdown-filtering-and-sql-query-optimization-from-cloud-native-olap-databases","title":"Leverage pushdown filtering and SQL query optimization from cloud-native OLAP databases","text":"<p>Do this:</p> <pre><code>from slaf import SLAFArray\n\n# Access your data directly from the cloud\nslaf_array = SLAFArray(\"s3://bucket/large_dataset.slaf\")\n\n# Query just what you need with SQL\nresults = slaf_array.query(\"\"\"\n    SELECT\n      cell_type,\n      AVG(total_counts) as avg_counts\n    FROM cells\n    WHERE batch = \"batch1\"\n      AND cell_type IN (\"T cells\", \"B cells\")\n    GROUP BY cell_type\n    ORDER BY avg_counts DESC\n\"\"\")\n</code></pre> <p>Instead of:</p> <pre><code>import scanpy as sc\n\n# Download large dataset from the cloud\n!aws s3 cp s3://bucket/large_dataset.h5ad .\n\n# Load entire dataset into memory\nadata = sc.read_h5ad(\"large_dataset.h5ad\")\n\n# Filter in memory\nsubset = adata[adata.obs.batch == \"batch1\"]\nsubset = subset[subset.obs.cell_type.isin([\"T cells\", \"B cells\"])]\n\n# Aggregate in memory\nresults = subset.obs.groupby(\"cell_type\")[\"total_counts\"].mean()\n</code></pre>"},{"location":"#evaluate-lazily-with-numpy-like-slicing-using-familiar-scanpy-idioms","title":"Evaluate lazily with numpy-like slicing using familiar scanpy idioms","text":"<p>Do this:</p> <pre><code>from slaf.integrations import read_slaf\n\n# Load as lazy AnnData\nadata = read_slaf(\"s3://bucket/large_dataset.slaf\")\n\n# Operations are lazy until you call .compute()\nsubset = adata[adata.obs.cell_type == \"T cells\", :]\nfirst_ten_cells = subset[:10, :]\nexpression = first_ten_cells.X.compute()  # Only now is data loaded\n</code></pre> <p>Instead of:</p> <pre><code># Download large dataset from the cloud\n!aws s3 cp s3://bucket/large_dataset.h5ad .\n\n# Load entire dataset into memory\nadata = sc.read_h5ad(\"large_dataset.h5ad\")\n\n# Filter in memory (expensive)\nsubset = adata[adata.obs.cell_type == \"T cells\", :]\nfirst_ten_cells = subset[:10, :]\nexpression = subset.X  # Always loads full data\n</code></pre>"},{"location":"#stream-tokenized-batches-using-pre-built-dataloaders-directly-from-the-cloud-to-gpu","title":"Stream tokenized batches using pre-built dataloaders directly from the cloud to GPU","text":"<p>Do this:</p> <pre><code>from slaf import SLAFArray\nfrom slaf.ml.dataloaders import SLAFDataLoader\n\n# Access your data directly from the cloud\nslaf_array = slaf.SLAFArray(\"s3://bucket/large_dataset.slaf\")\n\n# Create production-ready DataLoader\ndataloader = SLAFDataLoader(\n    slaf_array=slaf_array,\n    tokenizer_type=\"geneformer\",\n    batch_size=32,\n    max_genes=2048,\n    vocab_size=50000\n)\n\n# Stream batches for training\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]      # Already tokenized\n    attention_mask = batch[\"attention_mask\"]\n    cell_ids = batch[\"cell_ids\"]\n    # Your training code here\n</code></pre> <p>Instead of:</p> <pre><code>import scanpy as sc\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\n\n# Download large dataset from the cloud\n!aws s3 cp s3://bucket/large_dataset.h5ad .\n\n# Load entire dataset into memory\nadata = sc.read_h5ad(\"large_dataset.h5ad\")\n\n# Build gene vocabulary (manual implementation)\ngene_counts = adata.var.index.tolist()\nvocab_size = 50000\ngene_vocab = {gene: i + 4 for i, gene in enumerate(gene_counts[:vocab_size])}\nspecial_tokens = {\"PAD\": 0, \"CLS\": 1, \"SEP\": 2, \"UNK\": 3}\n\n# Custom tokenization function\ndef tokenize_geneformer(expression_row, max_genes=2048):\n    # Rank genes by expression\n    nonzero_indices = np.nonzero(expression_row)[0]\n    if len(nonzero_indices) == 0:\n        return [special_tokens[\"PAD\"]] * max_genes\n\n    # Sort by expression level\n    sorted_indices = sorted(nonzero_indices,\n                          key=lambda i: expression_row[i], reverse=True)\n\n    # Convert to tokens\n    tokens = []\n    for gene_idx in sorted_indices[:max_genes]:\n        gene_id = adata.var.index[gene_idx]\n        token = gene_vocab.get(gene_id, special_tokens[\"UNK\"])\n        tokens.append(token)\n\n    # Pad to max_genes\n    while len(tokens) &lt; max_genes:\n        tokens.append(special_tokens[\"PAD\"])\n\n    return tokens[:max_genes]\n\n# Create custom dataset class\nclass SingleCellDataset(Dataset):\n    def __init__(self, adata, max_genes=2048):\n        self.adata = adata\n        self.max_genes = max_genes\n        self.expression_matrix = adata.X.toarray()  # Convert to dense\n\n    def __len__(self):\n        return len(self.adata)\n\n    def __getitem__(self, idx):\n        # Tokenize on-the-fly (expensive)\n        expression_row = self.expression_matrix[idx]\n        tokens = tokenize_geneformer(expression_row, self.max_genes)\n\n        # Create attention mask\n        attention_mask = [1 if token != special_tokens[\"PAD\"] else 0\n                        for token in tokens]\n\n        return {\n            \"input_ids\": torch.tensor(tokens, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.bool),\n            \"cell_ids\": torch.tensor([idx], dtype=torch.long)\n        }\n\n# Create dataset and dataloader\ndataset = SingleCellDataset(adata)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Training loop\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    cell_ids = batch[\"cell_ids\"]\n    # Your training code here\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>Browse the SLAF API documentation by module:</p> <ul> <li>Core: Core SLAF classes and functions.</li> <li>Data: Data loading and conversion utilities.</li> <li>Integrations: Integrations with other tools and libraries.</li> <li>ML: Machine learning utilities and tokenizers.</li> </ul> <p>Use the navigation bar above to access other documentation sections.</p>"},{"location":"api/core/","title":"Core API","text":"<p>Core SLAF functionality including SLAFArray and query optimization.</p>"},{"location":"api/core/#slafarray","title":"SLAFArray","text":""},{"location":"api/core/#slaf.core.slaf.SLAFArray","title":"<code>slaf.core.slaf.SLAFArray</code>","text":"<p>High-performance single-cell data storage and querying format.</p> <p>SLAFArray provides SQL-native access to single-cell data with lazy evaluation. Data is stored in a relational format with three main tables: cells, genes, and expression. The class enables direct SQL queries, efficient filtering, and seamless integration with the single-cell analysis ecosystem.</p> Key Features <ul> <li>SQL-native querying with Polars integration</li> <li>Lazy evaluation for memory efficiency</li> <li>Direct access to cell and gene metadata</li> <li>High-performance storage with Lance format</li> <li>Scanpy/AnnData compatibility</li> <li>Cloud storage support (S3, GCS, Azure, etc.)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load a SLAF dataset\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; print(f\"Dataset shape: {slaf_array.shape}\")\nDataset shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Access metadata\n&gt;&gt;&gt; print(f\"Cell metadata columns: {list(slaf_array.obs.columns)}\")\nCell metadata columns: ['cell_type', 'total_counts', 'batch']\n&gt;&gt;&gt; print(f\"Gene metadata columns: {list(slaf_array.var.columns)}\")\nGene metadata columns: ['gene_type', 'chromosome']\n</code></pre> <pre><code>&gt;&gt;&gt; # Filter cells by metadata\n&gt;&gt;&gt; t_cells = slaf_array.filter_cells(cell_type=\"T cells\")\n&gt;&gt;&gt; print(f\"Found {len(t_cells)} T cells\")\nFound 250 T cells\n</code></pre> <pre><code>&gt;&gt;&gt; # Execute SQL query\n&gt;&gt;&gt; results = slaf_array.query(\"\n...     SELECT cell_type, AVG(total_counts) as avg_counts\n...     FROM cells\n...     GROUP BY cell_type\n...     ORDER BY avg_counts DESC\n... \")\n&gt;&gt;&gt; print(results)\n   cell_type  avg_counts\n0  T cells      1250.5\n1  B cells      1100.2\n2  Monocytes     950.8\n</code></pre> <pre><code>&gt;&gt;&gt; # Get expression data\n&gt;&gt;&gt; expression = slaf_array.get_cell_expression([\"cell_001\", \"cell_002\"])\n&gt;&gt;&gt; print(f\"Expression matrix shape: {expression.shape}\")\nExpression matrix shape: (2, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from cloud storage\n&gt;&gt;&gt; slaf_array = SLAFArray(\"s3://bucket/data.slaf\")\n&gt;&gt;&gt; print(f\"Cloud dataset: {slaf_array.shape}\")\nCloud dataset: (5000, 25000)\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>class SLAFArray:\n    \"\"\"\n    High-performance single-cell data storage and querying format.\n\n    SLAFArray provides SQL-native access to single-cell data with lazy evaluation.\n    Data is stored in a relational format with three main tables: cells, genes, and expression.\n    The class enables direct SQL queries, efficient filtering, and seamless integration\n    with the single-cell analysis ecosystem.\n\n    Key Features:\n        - SQL-native querying with Polars integration\n        - Lazy evaluation for memory efficiency\n        - Direct access to cell and gene metadata\n        - High-performance storage with Lance format\n        - Scanpy/AnnData compatibility\n        - Cloud storage support (S3, GCS, Azure, etc.)\n\n    Examples:\n        &gt;&gt;&gt; # Load a SLAF dataset\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; print(f\"Dataset shape: {slaf_array.shape}\")\n        Dataset shape: (1000, 20000)\n\n        &gt;&gt;&gt; # Access metadata\n        &gt;&gt;&gt; print(f\"Cell metadata columns: {list(slaf_array.obs.columns)}\")\n        Cell metadata columns: ['cell_type', 'total_counts', 'batch']\n        &gt;&gt;&gt; print(f\"Gene metadata columns: {list(slaf_array.var.columns)}\")\n        Gene metadata columns: ['gene_type', 'chromosome']\n\n        &gt;&gt;&gt; # Filter cells by metadata\n        &gt;&gt;&gt; t_cells = slaf_array.filter_cells(cell_type=\"T cells\")\n        &gt;&gt;&gt; print(f\"Found {len(t_cells)} T cells\")\n        Found 250 T cells\n\n        &gt;&gt;&gt; # Execute SQL query\n        &gt;&gt;&gt; results = slaf_array.query(\"\n        ...     SELECT cell_type, AVG(total_counts) as avg_counts\n        ...     FROM cells\n        ...     GROUP BY cell_type\n        ...     ORDER BY avg_counts DESC\n        ... \")\n        &gt;&gt;&gt; print(results)\n           cell_type  avg_counts\n        0  T cells      1250.5\n        1  B cells      1100.2\n        2  Monocytes     950.8\n\n        &gt;&gt;&gt; # Get expression data\n        &gt;&gt;&gt; expression = slaf_array.get_cell_expression([\"cell_001\", \"cell_002\"])\n        &gt;&gt;&gt; print(f\"Expression matrix shape: {expression.shape}\")\n        Expression matrix shape: (2, 20000)\n\n        &gt;&gt;&gt; # Load from cloud storage\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"s3://bucket/data.slaf\")\n        &gt;&gt;&gt; print(f\"Cloud dataset: {slaf_array.shape}\")\n        Cloud dataset: (5000, 25000)\n    \"\"\"\n\n    # Type annotations for instance variables\n    _cached_config_path: str | None\n    _cache_dir: str | None\n\n    def _is_cloud_path(self, path: str) -&gt; bool:\n        \"\"\"Check if path is a cloud storage path.\"\"\"\n        return path.startswith((\"s3://\", \"gs://\", \"azure://\", \"r2://\"))\n\n    def _is_hf_path(self, path: str) -&gt; bool:\n        \"\"\"Check if path is a huggingface storage path.\"\"\"\n        return path.startswith(\"hf://\")\n\n    def _parse_hf_path(self, path: str) -&gt; tuple[str, str]:\n        \"\"\"\n        Parse HuggingFace path to extract repo_id and file path.\n\n        Args:\n            path: HuggingFace path in format hf://datasets/username/repo-name[/path/to/slaf]\n                 The \"datasets\" prefix is stripped, repo_id is extracted as username/repo-name,\n                 and the remaining path is the path within the repo.\n\n        Returns:\n            Tuple of (repo_id, file_path) where repo_id is \"username/repo-name\"\n            and file_path is the path within the repo\n\n        Examples:\n            &gt;&gt;&gt; _parse_hf_path(\"hf://datasets/username/repo-name\")\n            ('username/repo-name', '')\n            &gt;&gt;&gt; _parse_hf_path(\"hf://datasets/username/repo-name/path/to/slaf\")\n            ('username/repo-name', 'path/to/slaf')\n            &gt;&gt;&gt; _parse_hf_path(\"hf://datasets/username/repo-name/path/to/slaf/config.json\")\n            ('username/repo-name', 'path/to/slaf/config.json')\n        \"\"\"\n        if not path.startswith(\"hf://\"):\n            raise ValueError(f\"Not a HuggingFace path: {path}\")\n\n        # Remove hf:// prefix\n        path_without_prefix = path[5:]  # len(\"hf://\") = 5\n\n        # Split into parts\n        parts = path_without_prefix.split(\"/\")\n\n        # Check if first part is \"datasets\" and strip it\n        if parts and parts[0] == \"datasets\":\n            parts = parts[1:]\n\n        # HuggingFace repo_id is always username/repo-name (2 parts)\n        if len(parts) &lt; 2:\n            raise ValueError(\n                f\"Invalid HuggingFace path: {path}. \"\n                \"Expected format: hf://datasets/username/repo-name[/path/to/slaf]\"\n            )\n\n        # First 2 parts form the repo_id (username/repo-name)\n        repo_id = \"/\".join(parts[:2])\n        # Rest is the path within the repo\n        file_path = \"/\".join(parts[2:]) if len(parts) &gt; 2 else \"\"\n\n        return repo_id, file_path\n\n    def _download_hf_file(\n        self, hf_path: str, filename: str | None = None, cache_dir: str | None = None\n    ) -&gt; str:\n        \"\"\"\n        Download a file from HuggingFace to local cache.\n\n        Args:\n            hf_path: HuggingFace path (e.g., \"hf://datasets/repo/path/to/slaf\")\n            filename: Optional filename to append to the path (e.g., \"config.json\").\n                     If provided, it will be joined with the path from hf_path.\n                     If None, uses the file_path from hf_path directly.\n            cache_dir: Optional cache directory. If None, uses huggingface_hub's default cache.\n\n        Returns:\n            Local path to the downloaded file\n        \"\"\"\n        if not HF_HUB_AVAILABLE:\n            raise ImportError(\n                \"huggingface_hub required for HuggingFace dataset support. \"\n                \"Install with: pip install huggingface_hub\"\n            )\n\n        repo_id, file_path = self._parse_hf_path(hf_path)\n\n        # Construct the full filename within the repo\n        if filename:\n            # Join the path from hf_path with the filename\n            full_filename = \"/\".join([file_path, filename]) if file_path else filename\n        else:\n            # Use the file_path directly (should be a full file path)\n            if not file_path:\n                raise ValueError(f\"HuggingFace path must include file path: {hf_path}\")\n            full_filename = file_path\n\n        # Download file to HuggingFace cache (managed by huggingface_hub)\n        try:\n            # Use cache_dir parameter if provided, otherwise fall back to instance cache_dir\n            effective_cache_dir = (\n                cache_dir\n                if cache_dir is not None\n                else getattr(self, \"_cache_dir\", None)\n            )\n\n            # hf_hub_download expects repo_id and filename as positional arguments\n            download_kwargs: dict[str, Any] = {\n                \"repo_type\": \"dataset\",\n            }\n            if effective_cache_dir is not None:\n                download_kwargs[\"cache_dir\"] = effective_cache_dir\n\n            local_path = hf_hub_download(repo_id, full_filename, **download_kwargs)\n        except Exception as e:\n            # Provide more context in the error message\n            raise FileNotFoundError(\n                f\"Failed to download file '{full_filename}' from HuggingFace dataset '{repo_id}': {e}\"\n            ) from e\n\n        return local_path\n\n    def _path_exists(self, path: str) -&gt; bool:\n        \"\"\"Check if path exists, works with both local and cloud paths.\"\"\"\n        if self._is_cloud_path(path):\n            if not SMART_OPEN_AVAILABLE:\n                logger.warning(\n                    \"smart-open not available, cannot check cloud path existence\"\n                )\n                return False\n            try:\n                # For SLAF datasets, check if config.json exists in the directory\n                config_path = self._join_path(path, \"config.json\")\n                with smart_open(config_path, \"r\") as f:\n                    f.read(1)  # Try to read 1 byte\n                return True\n            except Exception:\n                return False\n        elif self._is_hf_path(path):\n            try:\n                # Try to download config.json to validate existence\n                # The path is the base path, so we append config.json\n                # Use the instance's cache_dir if available\n                cache_dir = getattr(self, \"_cache_dir\", None)\n                self._download_hf_file(path, \"config.json\", cache_dir=cache_dir)\n                return True\n            except Exception as e:\n                # Log the error for debugging\n                logger.debug(f\"Failed to download config.json from {path}: {e}\")\n                return False\n        else:\n            # For local paths, check if it's a directory\n            return os.path.exists(path) and os.path.isdir(path)\n\n    def _open_file(self, path: str, mode: str = \"r\"):\n        \"\"\"\n        Open file with cloud storage and HuggingFace compatibility.\n\n        For HuggingFace paths, downloads the file to local cache first.\n        \"\"\"\n        if self._is_cloud_path(path):\n            if not SMART_OPEN_AVAILABLE:\n                raise ImportError(\"smart-open required for cloud storage operations\")\n            return smart_open(path, mode)\n        elif self._is_hf_path(path):\n            # For HuggingFace paths, download to cache and open locally\n            # The path should already include the full file path\n            # If it's a joined path (base + filename), parse and download\n            # Check if this is config.json and we have a cached path\n            if path.endswith(\"config.json\") and self._cached_config_path:\n                local_path = self._cached_config_path\n            else:\n                cache_dir = getattr(self, \"_cache_dir\", None)\n                local_path = self._download_hf_file(path, cache_dir=cache_dir)\n                # Cache the config.json path for future use\n                if path.endswith(\"config.json\"):\n                    self._cached_config_path = local_path\n            return open(local_path, mode)\n        else:\n            return open(path, mode)\n\n    def _join_path(self, base_path: str, *paths: str) -&gt; str:\n        \"\"\"Join paths, works with local, cloud, and HuggingFace paths.\"\"\"\n        if self._is_cloud_path(base_path):\n            # For cloud paths, use forward slashes\n            return \"/\".join([base_path.rstrip(\"/\")] + [p.lstrip(\"/\") for p in paths])\n        elif self._is_hf_path(base_path):\n            # For HuggingFace paths, use forward slashes\n            return \"/\".join([base_path.rstrip(\"/\")] + [p.lstrip(\"/\") for p in paths])\n        else:\n            # For local paths, use os.path.join\n            return os.path.join(base_path, *paths)\n\n    def __init__(\n        self, slaf_path: str, load_metadata: bool = True, cache_dir: str | None = None\n    ):\n        \"\"\"\n        Initialize SLAF array from a SLAF dataset directory.\n\n        Args:\n            slaf_path: Path to SLAF directory containing config.json and .lance files.\n                       The directory should contain the dataset configuration and Lance tables.\n                       Supports local paths, cloud storage (S3, GCS, Azure, etc.), and\n                       HuggingFace datasets (hf:// protocol).\n            load_metadata: If True, load cell and gene metadata (obs/var) in background.\n                          If False, skip metadata loading for faster initialization, especially\n                          useful for cloud datasets. Metadata can still be loaded on-demand when\n                          accessing obs/var properties. Default: True.\n            cache_dir: Optional cache directory for HuggingFace datasets. If None, uses\n                      huggingface_hub's default cache location. Only used for hf:// paths.\n\n        Raises:\n            FileNotFoundError: If the SLAF config file is not found at the specified path.\n            ValueError: If the config file is invalid or missing required tables.\n            KeyError: If required configuration keys are missing.\n\n        Examples:\n            &gt;&gt;&gt; # Load from local directory\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"./data/pbmc3k.slaf\")\n            &gt;&gt;&gt; print(f\"Loaded dataset: {slaf_array.shape}\")\n            Loaded dataset: (2700, 32738)\n\n            &gt;&gt;&gt; # Load from cloud storage without metadata (faster initialization)\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"s3://bucket/data.slaf\", load_metadata=False)\n            &gt;&gt;&gt; print(f\"Cloud dataset: {slaf_array.shape}\")\n            Cloud dataset: (5000, 25000)\n            &gt;&gt;&gt; print(f\"Metadata loaded: {slaf_array.is_metadata_ready()}\")\n            Metadata loaded: False\n\n            &gt;&gt;&gt; # Load metadata on-demand\n            &gt;&gt;&gt; slaf_array.wait_for_metadata()\n            &gt;&gt;&gt; print(f\"Metadata loaded: {slaf_array.is_metadata_ready()}\")\n            Metadata loaded: True\n\n            &gt;&gt;&gt; # Load from HuggingFace dataset\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"hf://datasets/repo/test-slaf\")\n            &gt;&gt;&gt; print(f\"HuggingFace dataset: {slaf_array.shape}\")\n            HuggingFace dataset: (1000, 20000)\n\n            &gt;&gt;&gt; # Error handling for missing directory\n            &gt;&gt;&gt; try:\n            ...     slaf_array = SLAFArray(\"nonexistent/path\")\n            ... except FileNotFoundError as e:\n            ...     print(f\"Error: {e}\")\n            Error: SLAF dataset not found: nonexistent/path\n        \"\"\"\n        # Convert Path objects to strings for cloud compatibility\n        if hasattr(slaf_path, \"__fspath__\"):\n            slaf_path = str(slaf_path)\n\n        self.slaf_path = slaf_path\n        self._load_metadata_enabled = load_metadata\n        self._cache_dir = cache_dir\n        # Store cached config.json path for HuggingFace datasets\n        self._cached_config_path = None\n\n        # Validate dataset exists (after setting cache_dir so it can be used)\n        if not self._path_exists(self.slaf_path):\n            raise FileNotFoundError(f\"SLAF dataset not found: {self.slaf_path}\")\n\n        # Load configuration\n        config_path = self._join_path(self.slaf_path, \"config.json\")\n        with self._open_file(config_path) as f:\n            self.config = json.load(f)\n\n        # Initialize shape\n        self.shape = tuple(self.config[\"array_shape\"])\n\n        # Setup datasets\n        self._setup_datasets()\n\n        # Initialize metadata loading\n        self._obs = None\n        self._var = None\n        self._metadata_loaded = False\n        self._metadata_loading = False\n        self._metadata_thread = None\n        self._metadata_error = None\n\n        # Initialize row mapper for cell-based queries\n        from slaf.core.query_optimizer import RowIndexMapper\n\n        self.row_mapper = RowIndexMapper(self)\n\n        # Start async metadata loading in background only if enabled\n        if self._load_metadata_enabled:\n            self._start_async_metadata_loading()\n\n        # Display helpful initialization message\n        self._display_initialization_message()\n\n    def _display_initialization_message(self):\n        \"\"\"Display helpful initialization message with basic metadata\"\"\"\n        # Display ASCII art\n        display_ascii_art()\n        logger.info(\"\")\n\n        n_cells, n_genes = self.shape\n\n        # Format large numbers with commas\n        cells_str = f\"{n_cells:,}\" if n_cells &gt;= 1000 else str(n_cells)\n        genes_str = f\"{n_genes:,}\" if n_genes &gt;= 1000 else str(n_genes)\n\n        # Get format version\n        format_version = self.config.get(\"format_version\", \"unknown\")\n\n        # Get dataset name from path\n        if self._is_cloud_path(self.slaf_path):\n            # For cloud paths, extract the last part after the last slash\n            dataset_name = self.slaf_path.split(\"/\")[-1]\n        elif self._is_hf_path(self.slaf_path):\n            # For HuggingFace paths, extract repo_id and path\n            repo_id, file_path = self._parse_hf_path(self.slaf_path)\n            # Use the last part of the path as dataset name, or repo_id if no path\n            dataset_name = file_path.split(\"/\")[-1] if file_path else repo_id\n        else:\n            # For local paths, use os.path.basename\n            dataset_name = os.path.basename(self.slaf_path)\n\n        # Display message\n        logger.info(f\"\ud83d\udcca SLAF Dataset Loaded: {dataset_name}\")\n        logger.info(f\"   \u2022 Shape: {cells_str} cells \u00d7 {genes_str} genes\")\n        logger.info(f\"   \u2022 Format: SLAF v{format_version}\")\n\n        # Add metadata info if available\n        if \"metadata\" in self.config:\n            metadata = self.config[\"metadata\"]\n            if \"expression_count\" in metadata:\n                expr_count = metadata[\"expression_count\"]\n                expr_str = f\"{expr_count:,}\" if expr_count &gt;= 1000 else str(expr_count)\n                logger.info(f\"   \u2022 Expression records: {expr_str}\")\n\n            if \"sparsity\" in metadata:\n                sparsity = metadata[\"sparsity\"]\n                logger.info(f\"   \u2022 Sparsity: {sparsity:.1%}\")\n\n        # Add optimization info if available\n        optimizations = self.config.get(\"optimizations\", {})\n        if optimizations:\n            opt_info = \", \".join([f\"{k}: {v}\" for k, v in optimizations.items()])\n            logger.info(f\"   \u2022 Optimizations: {opt_info}\")\n\n        # Status message\n        logger.info(\"   \u2022 Status: Ready for queries (metadata loading in background)\")\n        logger.info(\"\")\n\n    def _start_async_metadata_loading(self):\n        \"\"\"Start async metadata loading in background thread\"\"\"\n        if self._metadata_thread and self._metadata_thread.is_alive():\n            return  # Already loading\n\n        self._metadata_loading = True\n        self._metadata_thread = threading.Thread(\n            target=self._load_metadata_async, daemon=True\n        )\n        self._metadata_thread.start()\n\n    def _load_metadata_async(self):\n        \"\"\"Load metadata in background thread\"\"\"\n        try:\n            self._load_metadata()\n            self._metadata_loaded = True\n        except Exception as e:\n            # Ignore Lance IO errors that occur during dataset modifications\n            # These can happen when tables are being modified (e.g., adding obsm/varm columns)\n            # and the background thread tries to read stale references\n            error_str = str(e)\n            if \"not found\" in error_str and \".lance\" in error_str:\n                # This is a transient error during dataset modification, log as warning instead\n                logger.warning(\n                    f\"Transient error loading metadata (likely due to concurrent dataset modification): {e}\"\n                )\n                # Don't set metadata_error for transient errors, allow retry\n                return\n            self._metadata_error = e\n            logger.error(f\"Error loading metadata: {e}\")\n        finally:\n            self._metadata_loading = False\n\n    def _setup_datasets(self):\n        \"\"\"Setup Lance datasets for the new table structure\"\"\"\n        # Load all Lance datasets using cloud-compatible path joining\n        self.expression = lance.dataset(\n            self._join_path(self.slaf_path, self.config[\"tables\"][\"expression\"])\n        )\n        self.cells = lance.dataset(\n            self._join_path(self.slaf_path, self.config[\"tables\"][\"cells\"])\n        )\n        self.genes = lance.dataset(\n            self._join_path(self.slaf_path, self.config[\"tables\"][\"genes\"])\n        )\n\n        # NEW: Setup layers dataset if it exists (format version 0.4+)\n        if \"layers\" in self.config.get(\"tables\", {}):\n            try:\n                self.layers = lance.dataset(\n                    self._join_path(self.slaf_path, self.config[\"tables\"][\"layers\"])\n                )\n            except Exception as e:\n                logger.warning(f\"Could not load layers dataset: {e}\")\n                self.layers = None\n        else:\n            self.layers = None\n\n    def _ensure_metadata_loaded(self):\n        \"\"\"Ensure metadata is loaded (lazy loading with async support)\"\"\"\n        if self._metadata_loaded:\n            return\n\n        if self._metadata_loading:\n            # Wait for async loading to complete\n            if self._metadata_thread and self._metadata_thread.is_alive():\n                logger.info(\n                    \"Loading metadata in background... (this may take a few seconds)\"\n                )\n                self._metadata_thread.join()\n\n        if not self._metadata_loaded:\n            # Fallback to synchronous loading if async failed\n            self._load_metadata()\n            self._metadata_loaded = True\n\n    @property\n    def obs(self):\n        \"\"\"Cell metadata (lazy loaded)\"\"\"\n        self._ensure_metadata_loaded()\n        return self._obs\n\n    @property\n    def var(self):\n        \"\"\"Gene metadata (lazy loaded)\"\"\"\n        self._ensure_metadata_loaded()\n        return self._var\n\n    def is_metadata_ready(self) -&gt; bool:\n        \"\"\"Check if metadata is ready for use\"\"\"\n        return self._metadata_loaded\n\n    def is_metadata_loading(self) -&gt; bool:\n        \"\"\"Check if metadata is currently loading\"\"\"\n        return self._metadata_loading\n\n    def wait_for_metadata(self, timeout: float = None):\n        \"\"\"Wait for metadata to be loaded (with optional timeout)\"\"\"\n        if self._metadata_loaded:\n            return\n\n        if self._metadata_loading and self._metadata_thread:\n            if timeout:\n                self._metadata_thread.join(timeout=timeout)\n            else:\n                self._metadata_thread.join()\n\n        if not self._metadata_loaded:\n            # Fallback to synchronous loading\n            self._load_metadata()\n            self._metadata_loaded = True\n\n    def _load_metadata(self):\n        \"\"\"Load cell and gene metadata into polars DataFrames for fast operations\"\"\"\n        import pyarrow as pa\n\n        # Load cell metadata using optimized Lance operations\n        # Use more efficient loading strategy for large datasets\n        # Load in chunks if dataset is very large\n        if self.shape[0] &gt; 1000000:  # 1M+ cells\n            # Use scan() for large datasets to avoid loading everything at once\n            cells_df = pl.from_arrow(self.cells.scanner().to_table())\n        else:\n            # Use direct to_table() for smaller datasets\n            cells_df = pl.from_arrow(self.cells.to_table())\n\n        # Filter out vector columns (obsm) - these are FixedSizeListArray columns\n        # Vector columns should only be accessed via adata.obsm, not adata.obs\n        schema = self.cells.schema\n        vector_column_names = {\n            field.name\n            for field in schema\n            if isinstance(field.type, pa.FixedSizeListType)\n        }\n        if vector_column_names:\n            # Drop vector columns from obs\n            columns_to_drop = [\n                col for col in cells_df.columns if col in vector_column_names\n            ]\n            if columns_to_drop:\n                cells_df = cells_df.drop(columns_to_drop)\n\n        self._obs = cells_df.sort(\"cell_integer_id\")\n\n        # Load gene metadata using optimized Lance operations\n        # Genes are typically smaller, so use direct loading\n        genes_df = pl.from_arrow(self.genes.to_table())\n\n        # Filter out vector columns (varm) - these are FixedSizeListArray columns\n        # Vector columns should only be accessed via adata.varm, not adata.var\n        schema = self.genes.schema\n        vector_column_names = {\n            field.name\n            for field in schema\n            if isinstance(field.type, pa.FixedSizeListType)\n        }\n        if vector_column_names:\n            # Drop vector columns from var\n            columns_to_drop = [\n                col for col in genes_df.columns if col in vector_column_names\n            ]\n            if columns_to_drop:\n                genes_df = genes_df.drop(columns_to_drop)\n\n        self._var = genes_df.sort(\"gene_integer_id\")\n\n        # Cache column information for faster access\n        self._obs_columns = list(self._obs.columns)\n        self._var_columns = list(self._var.columns)\n\n        # Build cell start indices for efficient row access (zero-copy Polars)\n        if \"cell_start_index\" in self._obs.columns:\n            # Use precomputed cell_start_index column\n            logger.info(\"Using precomputed cell_start_index from cells table\")\n            # The cell_start_index column contains n_cells values (cumulative sums with leading 0)\n            # We need to append the total expression count for boundary checking\n            total_expression_count = self.expression.count_rows()\n            self._cell_start_index = pl.concat(\n                [self._obs[\"cell_start_index\"], pl.Series([total_expression_count])]\n            )\n        elif \"n_genes\" in self._obs.columns:\n            # Use existing n_genes column\n            cumsum = self._obs[\"n_genes\"].cum_sum()\n            # Ensure dtype consistency: cast to Int64\n            cumsum = cumsum.cast(pl.Int64)\n            self._cell_start_index = pl.concat([pl.Series([0], dtype=pl.Int64), cumsum])\n        elif \"gene_count\" in self._obs.columns:\n            # Use existing gene_count column\n            cumsum = self._obs[\"gene_count\"].cum_sum()\n            # Ensure dtype consistency: cast to Int64\n            cumsum = cumsum.cast(pl.Int64)\n            self._cell_start_index = pl.concat([pl.Series([0], dtype=pl.Int64), cumsum])\n        else:\n            # Calculate n_genes per cell from expression data\n            logger.info(\n                \"No precomputed cell_start_index found, calculating from expression data...\"\n            )\n\n            # Query expression data to count genes per cell\n            gene_counts_query = \"\"\"\n            SELECT cell_integer_id, COUNT(*) as n_genes\n            FROM expression\n            GROUP BY cell_integer_id\n            ORDER BY cell_integer_id\n            \"\"\"\n            gene_counts_df = self.query(gene_counts_query)\n\n            # Convert to polars if needed and join with obs\n            if hasattr(gene_counts_df, \"to_pandas\"):\n                gene_counts_df = gene_counts_df.to_pandas()\n            gene_counts_pl = pl.from_pandas(gene_counts_df)\n\n            # Join with obs to get n_genes for each cell\n            obs_with_counts = self._obs.join(\n                gene_counts_pl.select([\"cell_integer_id\", \"n_genes\"]),\n                on=\"cell_integer_id\",\n                how=\"left\",\n            )\n\n            # Fill missing values with 0 (cells with no expression)\n            obs_with_counts = obs_with_counts.with_columns(\n                pl.col(\"n_genes\").fill_null(0)\n            )\n\n            cumsum = obs_with_counts[\"n_genes\"].cum_sum()\n            # Ensure dtype consistency: cast both to Int64 to match other code paths\n            cumsum = cumsum.cast(pl.Int64)\n            self._cell_start_index = pl.concat([pl.Series([0], dtype=pl.Int64), cumsum])\n\n        # Restore dtypes for obs using polars\n        obs_dtypes = self.config.get(\"obs_dtypes\", {})\n        for col, dtype_info in obs_dtypes.items():\n            if col in self._obs.columns:\n                if dtype_info[\"dtype\"] == \"category\":\n                    # Convert to polars categorical\n                    self._obs = self._obs.with_columns(\n                        pl.col(col).cast(pl.Categorical).cast(pl.Utf8)\n                    )\n                else:\n                    # Map pandas dtypes to polars dtypes\n                    polars_dtype = self._map_pandas_to_polars_dtype(dtype_info[\"dtype\"])\n                    self._obs = self._obs.with_columns(pl.col(col).cast(polars_dtype))\n\n        # Restore dtypes for var using polars\n        var_dtypes = self.config.get(\"var_dtypes\", {})\n        for col, dtype_info in var_dtypes.items():\n            if col in self._var.columns:\n                if dtype_info[\"dtype\"] == \"category\":\n                    # Convert to polars categorical\n                    self._var = self._var.with_columns(\n                        pl.col(col).cast(pl.Categorical).cast(pl.Utf8)\n                    )\n                else:\n                    # Map pandas dtypes to polars dtypes\n                    polars_dtype = self._map_pandas_to_polars_dtype(dtype_info[\"dtype\"])\n                    self._var = self._var.with_columns(pl.col(col).cast(polars_dtype))\n\n        # Infer categorical columns if not in config\n        self._infer_categorical_columns()\n\n    def _map_pandas_to_polars_dtype(self, pandas_dtype: str) -&gt; type[pl.DataType]:\n        \"\"\"Map pandas dtype to polars dtype\"\"\"\n        dtype_mapping = {\n            \"int64\": pl.Int64,\n            \"int32\": pl.Int32,\n            \"int16\": pl.Int16,\n            \"int8\": pl.Int8,\n            \"uint64\": pl.UInt64,\n            \"uint32\": pl.UInt32,\n            \"uint16\": pl.UInt16,\n            \"uint8\": pl.UInt8,\n            \"float64\": pl.Float64,\n            \"float32\": pl.Float32,\n            \"bool\": pl.Boolean,\n            \"object\": pl.Utf8,\n            \"string\": pl.Utf8,\n        }\n        return dtype_mapping.get(pandas_dtype, pl.Utf8)\n\n    def _infer_categorical_columns(self):\n        \"\"\"Infer categorical columns based on data characteristics using polars\"\"\"\n        # For obs: infer categoricals for string columns with few unique values\n        for col in self._obs.columns:\n            if col not in self.config.get(\"obs_dtypes\", {}):\n                if self._obs[col].dtype == pl.Utf8:\n                    unique_count = self._obs[col].n_unique()\n                    total_count = len(self._obs)\n                    unique_ratio = unique_count / total_count\n                    # If less than 20% unique values, likely categorical\n                    if unique_ratio &lt; 0.2 and unique_count &lt; 50:\n                        self._obs = self._obs.with_columns(\n                            pl.col(col).cast(pl.Categorical).cast(pl.Utf8)\n                        )\n\n        # For var: infer categoricals for string columns with few unique values\n        for col in self._var.columns:\n            if col not in self.config.get(\"var_dtypes\", {}):\n                if self._var[col].dtype == pl.Utf8:\n                    unique_count = self._var[col].n_unique()\n                    total_count = len(self._var)\n                    unique_ratio = unique_count / total_count\n                    # If less than 20% unique values, likely categorical\n                    if unique_ratio &lt; 0.2 and unique_count &lt; 50:\n                        self._var = self._var.with_columns(\n                            pl.col(col).cast(pl.Categorical).cast(pl.Utf8)\n                        )\n\n    def query(self, sql: str) -&gt; pl.DataFrame:\n        \"\"\"\n        Execute SQL query on the SLAF dataset.\n\n        Executes SQL queries directly on the underlying Lance tables using Polars.\n        The query can reference tables: 'cells', 'genes', 'expression', and optionally 'layers'.\n        This enables complex aggregations, joins, and filtering operations.\n\n        Args:\n            sql: SQL query string to execute. Can reference tables: cells, genes, expression, layers.\n                 Supports standard SQL operations including WHERE, GROUP BY, ORDER BY, etc.\n                 The 'layers' table is only available if the dataset has layers (format version 0.4+).\n\n        Returns:\n            Polars DataFrame containing the query results.\n\n        Raises:\n            ValueError: If the SQL query is malformed or references non-existent tables.\n            RuntimeError: If the query execution fails.\n\n        Examples:\n            &gt;&gt;&gt; # Basic query to count cells\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; result = slaf_array.query(\"SELECT COUNT(*) as total_cells FROM cells\")\n            &gt;&gt;&gt; print(f\"Total cells: {result['total_cells'][0]}\")\n            Total cells: 1000\n\n            &gt;&gt;&gt; # Complex aggregation query\n            &gt;&gt;&gt; result = slaf_array.query(\"\n            ...     SELECT cell_type,\n            ...            COUNT(*) as cell_count,\n            ...            AVG(total_counts) as avg_counts\n            ...     FROM cells\n            ...     WHERE total_counts &gt; 500\n            ...     GROUP BY cell_type\n            ...     ORDER BY avg_counts DESC\n            ... \")\n            &gt;&gt;&gt; print(result)\n            shape: (3, 3)\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 cell_type  \u2506 cell_count \u2506 avg_counts \u2502\n            \u2502 ---        \u2506 ---        \u2506 ---        \u2502\n            \u2502 str        \u2506 i64        \u2506 f64        \u2502\n            \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n            \u2502 T cells    \u2506 250        \u2506 1250.5     \u2502\n            \u2502 B cells    \u2506 200        \u2506 1100.2     \u2502\n            \u2502 Monocytes  \u2506 150        \u2506 950.8      \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n            &gt;&gt;&gt; # Join query across tables\n            &gt;&gt;&gt; result = slaf_array.query(\"\n            ...     SELECT c.cell_type, g.gene_type, AVG(e.value) as avg_expression\n            ...     FROM cells c\n            ...     JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n            ...     JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n            ...     WHERE c.cell_type = 'T cells'\n            ...     GROUP BY c.cell_type, g.gene_type\n            ... \")\n            &gt;&gt;&gt; print(f\"Found {len(result)} expression patterns\")\n            Found 5 expression patterns\n        \"\"\"\n        # Create Polars context with all Lance datasets\n        ctx = pl.SQLContext()\n\n        # Register Lance datasets with Polars context\n        ctx.register(\"expression\", pl.scan_pyarrow_dataset(self.expression))\n        ctx.register(\"cells\", pl.scan_pyarrow_dataset(self.cells))\n        ctx.register(\"genes\", pl.scan_pyarrow_dataset(self.genes))\n\n        # NEW: Register layers table if it exists (format version 0.4+)\n        if self.layers is not None:\n            ctx.register(\"layers\", pl.scan_pyarrow_dataset(self.layers))\n\n        # Execute the query using Polars SQL\n        result = ctx.execute(sql).collect()\n\n        return result\n\n    def filter_cells(self, **filters: Any) -&gt; pl.DataFrame:\n        \"\"\"\n        Filter cells based on metadata columns.\n\n        Provides a convenient interface for filtering cells using metadata columns.\n        Supports exact matches, list values, and range queries with operators.\n        Uses in-memory polars filtering when metadata is loaded, falls back to SQL otherwise.\n\n        Args:\n            **filters: Column name and filter value pairs. Supports:\n                - Exact matches: cell_type=\"T cells\"\n                - List values: cell_type=[\"T cells\", \"B cells\"]\n                - Range queries: total_counts=\"&gt;1000\", total_counts=\"&lt;=2000\"\n                - Multiple conditions: cell_type=\"T cells\", total_counts=\"&gt;500\"\n\n        Returns:\n            Polars DataFrame containing filtered cell metadata.\n\n        Raises:\n            ValueError: If a specified column is not found in cell metadata.\n            TypeError: If filter values are of unsupported types.\n\n        Examples:\n            &gt;&gt;&gt; # Filter by cell type\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; t_cells = slaf_array.filter_cells(cell_type=\"T cells\")\n            &gt;&gt;&gt; print(f\"Found {len(t_cells)} T cells\")\n            Found 250 T cells\n\n            &gt;&gt;&gt; # Filter by multiple criteria\n            &gt;&gt;&gt; high_quality_t_cells = slaf_array.filter_cells(\n            ...     cell_type=\"T cells\",\n            ...     total_counts=\"&gt;1000\",\n            ...     batch=[\"batch1\", \"batch2\"]\n            ... )\n            &gt;&gt;&gt; print(f\"Found {len(high_quality_t_cells)} high-quality T cells\")\n            Found 180 high-quality T cells\n\n            &gt;&gt;&gt; # Range query\n            &gt;&gt;&gt; medium_counts = slaf_array.filter_cells(\n            ...     total_counts=\"&gt;=500\",\n            ...     total_counts=\"&lt;=2000\"\n            ... )\n            &gt;&gt;&gt; print(f\"Found {len(medium_counts)} cells with medium counts\")\n            Found 450 cells with medium counts\n\n            &gt;&gt;&gt; # Error handling for invalid column\n            &gt;&gt;&gt; try:\n            ...     result = slaf_array.filter_cells(invalid_column=\"value\")\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Column 'invalid_column' not found in cell metadata\n        \"\"\"\n        return self._filter(\"cells\", **filters)\n\n    def filter_genes(self, **filters: Any) -&gt; pl.DataFrame:\n        \"\"\"\n        Filter genes based on metadata columns.\n\n        Provides a convenient interface for filtering genes using metadata columns.\n        Supports exact matches, list values, and range queries with operators.\n        Uses in-memory polars filtering when metadata is loaded, falls back to SQL otherwise.\n\n        Args:\n            **filters: Column name and filter value pairs. Supports:\n                - Exact matches: gene_type=\"protein_coding\"\n                - List values: gene_type=[\"protein_coding\", \"lncRNA\"]\n                - Range queries: expression_mean=\"&gt;5.0\", expression_mean=\"&lt;=10.0\"\n                - Multiple conditions: gene_type=\"protein_coding\", chromosome=\"chr1\"\n\n        Returns:\n            Polars DataFrame containing filtered gene metadata.\n\n        Raises:\n            ValueError: If a specified column is not found in gene metadata.\n            TypeError: If filter values are of unsupported types.\n\n        Examples:\n            &gt;&gt;&gt; # Filter by gene type\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; protein_coding = slaf_array.filter_genes(gene_type=\"protein_coding\")\n            &gt;&gt;&gt; print(f\"Found {len(protein_coding)} protein-coding genes\")\n            Found 15000 protein-coding genes\n\n            &gt;&gt;&gt; # Filter by multiple criteria\n            &gt;&gt;&gt; high_expr_proteins = slaf_array.filter_genes(\n            ...     gene_type=\"protein_coding\",\n            ...     expression_mean=\"&gt;5.0\",\n            ...     chromosome=[\"chr1\", \"chr2\"]\n            ... )\n            &gt;&gt;&gt; print(f\"Found {len(high_expr_proteins)} high-expression protein genes\")\n            Found 2500 high-expression protein genes\n\n            &gt;&gt;&gt; # Range query for expression\n            &gt;&gt;&gt; medium_expr = slaf_array.filter_genes(\n            ...     expression_mean=\"&gt;=2.0\",\n            ...     expression_mean=\"&lt;=8.0\"\n            ... )\n            &gt;&gt;&gt; print(f\"Found {len(medium_expr)} genes with medium expression\")\n            Found 8000 genes with medium expression\n\n            &gt;&gt;&gt; # Error handling for invalid column\n            &gt;&gt;&gt; try:\n            ...     result = slaf_array.filter_genes(invalid_column=\"value\")\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Column 'invalid_column' not found in gene metadata\n        \"\"\"\n        return self._filter(\"genes\", **filters)\n\n    def _filter(self, table_name: str, **filters: Any) -&gt; pl.DataFrame:\n        \"\"\"\n        Generic filtering method that uses polars for metadata operations.\n\n        Args:\n            table_name: Either \"cells\" or \"genes\"\n            **filters: Filter conditions\n\n        Returns:\n            Filtered polars DataFrame\n        \"\"\"\n        if not filters:\n            # Return all metadata if no filters\n            if table_name == \"cells\":\n                return (\n                    self.obs\n                    if self._metadata_loaded\n                    else self.query(\"SELECT * FROM cells\")\n                )\n            else:\n                return (\n                    self.var\n                    if self._metadata_loaded\n                    else self.query(\"SELECT * FROM genes\")\n                )\n\n        # For filtering, we need metadata to be loaded to validate columns\n        # If metadata is not loaded, load it first\n        if not self._metadata_loaded:\n            self._ensure_metadata_loaded()\n\n        # Use polars filtering for metadata operations\n        if table_name == \"cells\":\n            return self._filter_with_polars(self.obs, **filters)\n        elif table_name == \"genes\":\n            return self._filter_with_polars(self.var, **filters)\n        else:\n            raise ValueError(f\"Invalid table name: {table_name}\")\n\n    def _filter_with_polars(\n        self, metadata_df: pl.DataFrame, **filters: Any\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Filter metadata using polars operations for high performance.\n\n        Args:\n            metadata_df: Polars DataFrame to filter (self.obs or self.var)\n            **filters: Filter conditions\n\n        Returns:\n            Filtered polars DataFrame\n        \"\"\"\n        # Start with all rows\n        result = metadata_df\n\n        for column, value in filters.items():\n            if column not in metadata_df.columns:\n                raise ValueError(f\"Column '{column}' not found in metadata\")\n\n            if isinstance(value, str) and value.startswith((\"&gt;\", \"&lt;\", \"&gt;=\", \"&lt;=\")):\n                # Handle range queries\n                operator = value[:2] if value.startswith((\"&gt;=\", \"&lt;=\")) else value[0]\n                filter_value = (\n                    value[2:] if value.startswith((\"&gt;=\", \"&lt;=\")) else value[1:]\n                )\n\n                # Convert to numeric for comparison\n                try:\n                    numeric_filter_value = float(filter_value)\n\n                    # Create boolean mask for the comparison using polars expressions\n                    if operator == \"&gt;\":\n                        result = result.filter(pl.col(column) &gt; numeric_filter_value)\n                    elif operator == \"&lt;\":\n                        result = result.filter(pl.col(column) &lt; numeric_filter_value)\n                    elif operator == \"&gt;=\":\n                        result = result.filter(pl.col(column) &gt;= numeric_filter_value)\n                    elif operator == \"&lt;=\":\n                        result = result.filter(pl.col(column) &lt;= numeric_filter_value)\n                    else:\n                        raise ValueError(f\"Unsupported operator: {operator}\")\n\n                except Exception as err:\n                    raise ValueError(\n                        f\"Cannot perform numeric comparison on non-numeric column '{column}'\"\n                    ) from err\n\n            elif isinstance(value, list):\n                # Handle list values using polars is_in\n                result = result.filter(pl.col(column).is_in(value))\n            else:\n                # Handle exact matches\n                result = result.filter(pl.col(column) == value)\n\n        return result\n\n    def _normalize_entity_ids(\n        self, entity_ids: str | list[str], entity_type: str\n    ) -&gt; list[int]:\n        \"\"\"Convert string entity IDs to integer IDs using metadata mappings\"\"\"\n        if entity_type == \"cell\":\n            metadata = self.obs\n            id_col = \"cell_integer_id\"\n            index_col = \"cell_id\"\n        else:\n            metadata = self.var\n            id_col = \"gene_integer_id\"\n            index_col = \"gene_id\"\n\n        # Convert to list if single ID\n        if isinstance(entity_ids, str):\n            entity_ids = [entity_ids]\n\n        # Map string IDs to integer IDs using polars\n        # Get the index values and corresponding integer IDs\n        index_values = metadata[index_col].to_list()\n        integer_id_values = metadata[id_col].to_list()\n\n        # Create a mapping from index to integer ID\n        id_mapping = dict(zip(index_values, integer_id_values, strict=False))\n\n        # Map string IDs to integer IDs\n        integer_ids = []\n        for entity_id in entity_ids:\n            if entity_id in id_mapping:\n                integer_ids.append(id_mapping[entity_id])\n            else:\n                raise ValueError(f\"{entity_type} ID '{entity_id}' not found\")\n\n        return integer_ids\n\n    def get_cell_expression(self, cell_ids: str | list[str]) -&gt; pl.DataFrame:\n        \"\"\"\n        Get expression data for specific cells using Lance take() and Polars.\n\n        Retrieves expression data for specified cells using efficient Lance row access\n        and Polars for in-memory operations. This method provides significant\n        performance improvements over SQL-based queries.\n\n        Args:\n            cell_ids: Single cell ID (string) or list of cell IDs to retrieve.\n                     Can be string identifiers or integer IDs.\n\n        Returns:\n            Polars DataFrame containing expression data for the specified cells.\n            Columns include cell_id, gene_id, and expression values.\n\n        Raises:\n            ValueError: If any cell ID is not found in the dataset.\n            RuntimeError: If the query execution fails.\n\n        Examples:\n            &gt;&gt;&gt; # Get expression for a single cell\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; expression = slaf_array.get_cell_expression(\"cell_001\")\n            &gt;&gt;&gt; print(f\"Expression data shape: {expression.shape}\")\n            Expression data shape: (15000, 3)\n\n            &gt;&gt;&gt; # Get expression for multiple cells\n            &gt;&gt;&gt; expression = slaf_array.get_cell_expression([\"cell_001\", \"cell_002\", \"cell_003\"])\n            &gt;&gt;&gt; print(f\"Expression data shape: {expression.shape}\")\n            Expression data shape: (45000, 3)\n\n            &gt;&gt;&gt; # Error handling for invalid cell ID\n            &gt;&gt;&gt; try:\n            ...     expression = slaf_array.get_cell_expression(\"invalid_cell\")\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: cell ID 'invalid_cell' not found\n        \"\"\"\n        # Convert to integer IDs\n        integer_ids = self._normalize_entity_ids(cell_ids, \"cell\")\n\n        if not integer_ids:\n            return pl.DataFrame({\"cell_id\": [], \"gene_id\": [], \"value\": []})\n\n        # Get row indices using RowIndexMapper\n        row_indices = self.row_mapper.get_cell_row_ranges(integer_ids)\n\n        # Load data with Lance take()\n        expression_data = self.expression.take(row_indices)\n\n        # Convert PyArrow Table to Polars DataFrame and join with metadata\n        expression_df = pl.from_arrow(expression_data)\n        return self._join_with_metadata(expression_df)\n\n    def get_gene_expression(self, gene_ids: str | list[str]) -&gt; pl.DataFrame:\n        \"\"\"\n        Get gene expression data for specified genes.\n\n        This method uses QueryOptimizer to generate optimal SQL queries based on the\n        gene ID distribution (consecutive ranges use BETWEEN, scattered values use IN).\n\n        Args:\n            gene_ids: Gene ID(s) to query. Can be a single gene ID string or list of gene IDs.\n\n        Returns:\n            Polars DataFrame containing expression data for the specified genes.\n            Columns include cell_id, gene_id, and expression values.\n\n        Raises:\n            ValueError: If any gene ID is not found in the dataset.\n            RuntimeError: If the query execution fails.\n\n        Examples:\n            &gt;&gt;&gt; # Get expression for a single gene\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; expression = slaf_array.get_gene_expression(\"GENE1\")\n        \"\"\"\n\n        # Convert to list if single gene ID\n        if isinstance(gene_ids, str):\n            gene_ids = [gene_ids]\n\n        # Convert string gene IDs to integer IDs\n        integer_ids = self._normalize_entity_ids(gene_ids, \"gene\")\n\n        if not integer_ids:\n            return pl.DataFrame({\"cell_id\": [], \"gene_id\": [], \"value\": []})\n\n        # Use QueryOptimizer for optimal performance\n        from slaf.core.query_optimizer import QueryOptimizer\n\n        # Build optimized SQL query using QueryOptimizer\n        sql_query = QueryOptimizer.build_optimized_query(\n            entity_ids=integer_ids,\n            entity_type=\"gene\",\n            use_adaptive_batching=True,\n            max_batch_size=100,\n        )\n\n        # Fix the table name for Polars scan (use 'self' instead of 'expression')\n        sql_query = sql_query.replace(\"expression\", \"self\")\n\n        # Execute the optimized query\n        ldf = pl.scan_pyarrow_dataset(self.expression)\n        expression_df = ldf.sql(sql_query).collect()\n\n        # Join with metadata using Polars\n        return self._join_with_metadata(expression_df)\n\n    def get_submatrix(\n        self,\n        cell_selector: Any | None = None,\n        gene_selector: Any | None = None,\n        table_name: str = \"expression\",\n        layer_name: str | None = None,\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Get expression or layer data using cell/gene selectors with Lance take() and Polars.\n\n        Retrieves a subset of data based on cell and gene selectors.\n        The selectors can be slices, lists, boolean masks, or None for all cells/genes.\n        This method provides a flexible interface for subsetting data with\n        significant performance improvements over SQL-based queries.\n\n        Args:\n            cell_selector: Cell selector for subsetting. Can be:\n                - None: Include all cells\n                - slice: e.g., slice(0, 100) for first 100 cells\n                - list: e.g., [0, 5, 10] for specific cell indices\n                - boolean mask: e.g., [True, False, True, ...] for boolean selection\n            gene_selector: Gene selector for subsetting. Can be:\n                - None: Include all genes\n                - slice: e.g., slice(0, 5000) for first 5000 genes\n                - list: e.g., [0, 100, 200] for specific gene indices\n                - boolean mask: e.g., [True, False, True, ...] for boolean selection\n            table_name: Table to query (\"expression\" or \"layers\"). Default: \"expression\"\n            layer_name: Layer name for layers table (required when table_name=\"layers\").\n                       Default: None\n\n        Returns:\n            Polars DataFrame containing data for the selected subset.\n            Columns include cell_id, gene_id, and value (or layer column name).\n\n        Raises:\n            ValueError: If selectors are invalid or out of bounds.\n            RuntimeError: If the query execution fails.\n\n        Examples:\n            &gt;&gt;&gt; # Get first 100 cells and first 5000 genes from expression\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; submatrix = slaf_array.get_submatrix(\n            ...     cell_selector=slice(0, 100),\n            ...     gene_selector=slice(0, 5000)\n            ... )\n            &gt;&gt;&gt; print(f\"Submatrix shape: {submatrix.shape}\")\n            Submatrix shape: (500000, 3)\n\n            &gt;&gt;&gt; # Get data from a layer\n            &gt;&gt;&gt; submatrix = slaf_array.get_submatrix(\n            ...     cell_selector=slice(0, 100),\n            ...     table_name=\"layers\",\n            ...     layer_name=\"spliced\"\n            ... )\n            &gt;&gt;&gt; print(f\"Layer submatrix shape: {submatrix.shape}\")\n            Layer submatrix shape: (500000, 3)\n        \"\"\"\n        # For layers table, use SQL query (RowIndexMapper is expression-specific)\n        if table_name == \"layers\":\n            if layer_name is None:\n                raise ValueError(\"layer_name must be provided when table_name='layers'\")\n            if self.layers is None:\n                raise ValueError(\"Layers table not available in this dataset\")\n\n            # Use QueryOptimizer to build SQL query\n            from slaf.core.query_optimizer import QueryOptimizer\n\n            sql_query = QueryOptimizer.build_submatrix_query(\n                cell_selector=cell_selector,\n                gene_selector=gene_selector,\n                cell_count=self.shape[0],\n                gene_count=self.shape[1],\n                table_name=table_name,\n                layer_name=layer_name,\n            )\n\n            # Execute query\n            result_df = self.query(sql_query)\n\n            # Join with metadata\n            return self._join_with_metadata(result_df)\n\n        # For expression table, use existing optimized path with RowIndexMapper\n        # Get row indices for cells using RowIndexMapper\n        cell_indices = self.row_mapper.get_cell_row_ranges_by_selector(cell_selector)\n\n        # Load data with Lance take()\n        expression_data = self.expression.take(cell_indices)\n        expression_df = pl.from_arrow(expression_data)\n\n        # Filter by genes if specified\n        if gene_selector is not None:\n            # Convert gene selector to integer IDs\n            if isinstance(gene_selector, int):\n                if gene_selector &lt; 0:\n                    gene_selector = self.shape[1] + gene_selector\n                if 0 &lt;= gene_selector &lt; self.shape[1]:\n                    gene_integer_id = self.var[\"gene_integer_id\"][gene_selector]\n                    expression_df = expression_df.filter(\n                        pl.col(\"gene_integer_id\") == gene_integer_id  # type: ignore[arg-type]\n                    )\n                else:\n                    raise ValueError(f\"Gene index {gene_selector} out of bounds\")\n            elif isinstance(gene_selector, slice):\n                start = gene_selector.start or 0\n                stop = gene_selector.stop or self.shape[1]\n                step = gene_selector.step or 1\n\n                # Handle negative indices\n                if start &lt; 0:\n                    start = self.shape[1] + start\n                if stop &lt; 0:\n                    stop = self.shape[1] + stop\n\n                # Clamp bounds\n                start = max(0, min(start, self.shape[1]))\n                stop = max(0, min(stop, self.shape[1]))\n\n                gene_integer_ids = self.var[\"gene_integer_id\"][\n                    start:stop:step\n                ].to_list()\n                expression_df = expression_df.filter(\n                    pl.col(\"gene_integer_id\").is_in(gene_integer_ids)  # type: ignore[arg-type]\n                )\n            elif isinstance(gene_selector, list):\n                gene_integer_ids = []\n                for idx in gene_selector:\n                    if isinstance(idx, int):\n                        if idx &lt; 0:\n                            idx = self.shape[1] + idx\n                        if 0 &lt;= idx &lt; self.shape[1]:\n                            gene_integer_id = self.var[\"gene_integer_id\"][idx]\n                            gene_integer_ids.append(gene_integer_id)\n                        else:\n                            raise ValueError(f\"Gene index {idx} out of bounds\")\n                    else:\n                        raise ValueError(f\"Invalid gene index type: {type(idx)}\")\n                expression_df = expression_df.filter(\n                    pl.col(\"gene_integer_id\").is_in(gene_integer_ids)  # type: ignore[arg-type]\n                )\n            elif isinstance(gene_selector, np.ndarray) and gene_selector.dtype == bool:\n                if len(gene_selector) != self.shape[1]:\n                    raise ValueError(\n                        f\"Boolean mask length {len(gene_selector)} doesn't match gene count {self.shape[1]}\"\n                    )\n                gene_integer_ids = self.var[\"gene_integer_id\"][gene_selector].to_list()\n                expression_df = expression_df.filter(\n                    pl.col(\"gene_integer_id\").is_in(gene_integer_ids)  # type: ignore[arg-type]\n                )\n            else:\n                raise ValueError(\n                    f\"Unsupported gene selector type: {type(gene_selector)}\"\n                )\n\n        # Join with metadata using Polars\n        return self._join_with_metadata(expression_df)\n\n    def _join_with_metadata(\n        self, expression_data: pl.DataFrame | pl.Series\n    ) -&gt; pl.DataFrame:\n        \"\"\"\n        Join expression data with cell/gene metadata using Polars.\n\n        Args:\n            expression_data: Polars DataFrame or Series containing expression data\n\n        Returns:\n            Polars DataFrame with cell_id, gene_id, and value columns\n        \"\"\"\n        # Convert Series to DataFrame if needed\n        if isinstance(expression_data, pl.Series):\n            expression_data = expression_data.to_frame()\n\n        # Join with cell metadata\n        result = expression_data.join(\n            self.obs.select([\"cell_integer_id\", \"cell_id\"]),\n            on=\"cell_integer_id\",\n            how=\"left\",\n        )\n\n        # Join with gene metadata\n        result = result.join(\n            self.var.select([\"gene_integer_id\", \"gene_id\"]),\n            on=\"gene_integer_id\",\n            how=\"left\",\n        )\n\n        # Select final columns\n        return result.select([\"cell_id\", \"gene_id\", \"value\"])\n\n    def info(self):\n        \"\"\"Print information about the SLAF dataset\"\"\"\n        # Build output string for both logging and printing\n        output_lines = []\n\n        output_lines.append(\"SLAF Dataset\")\n        output_lines.append(f\"  Shape: {self.shape[0]} cells \u00d7 {self.shape[1]} genes\")\n        output_lines.append(\n            f\"  Format version: {self.config.get('format_version', 'unknown')}\"\n        )\n\n        # Cell metadata columns\n        cell_cols = self.obs.columns\n        output_lines.append(f\"  Cell metadata columns: {len(cell_cols)}\")\n        if cell_cols:\n            output_lines.append(\n                f\"    {', '.join(cell_cols[:5])}{'...' if len(cell_cols) &gt; 5 else ''}\"\n            )\n\n        # Gene metadata columns\n        gene_cols = self.var.columns\n        output_lines.append(f\"  Gene metadata columns: {len(gene_cols)}\")\n        if gene_cols:\n            output_lines.append(\n                f\"    {', '.join(gene_cols[:5])}{'...' if len(gene_cols) &gt; 5 else ''}\"\n            )\n\n        # Record counts\n        output_lines.append(\"  Record counts:\")\n        output_lines.append(f\"    Cells: {len(self.obs):,}\")\n        output_lines.append(f\"    Genes: {len(self.var):,}\")\n\n        # Expression metadata - use pre-computed if available, otherwise query\n        format_version = self.config.get(\"format_version\", \"0.1\")\n        if format_version &gt;= \"0.2\" and \"metadata\" in self.config:\n            # Use pre-computed metadata from config\n            metadata = self.config[\"metadata\"]\n            expression_count = metadata[\"expression_count\"]\n            sparsity = metadata[\"sparsity\"]\n            density = metadata[\"density\"]\n\n            output_lines.append(f\"    Expression records: {expression_count:,}\")\n            output_lines.append(f\"    Sparsity: {sparsity:.1%}\")\n            output_lines.append(f\"    Density: {density:.1%}\")\n\n            # Show expression statistics if available\n            if \"expression_stats\" in metadata:\n                stats = metadata[\"expression_stats\"]\n                output_lines.append(\"  Expression statistics:\")\n                output_lines.append(f\"    Min value: {stats['min_value']:.3f}\")\n                output_lines.append(f\"    Max value: {stats['max_value']:.3f}\")\n                output_lines.append(f\"    Mean value: {stats['mean_value']:.3f}\")\n                output_lines.append(f\"    Std value: {stats['std_value']:.3f}\")\n        else:\n            # Backward compatibility: query expression count for older format versions\n            output_lines.append(\"    Expression records: computing...\")\n            expression_count = self.query(\"SELECT COUNT(*) as count FROM expression\")\n            output_lines.append(\n                f\"    Expression records: {expression_count.item(0, 0):,}\"\n            )\n\n        # Optimization info\n        optimizations = self.config.get(\"optimizations\", {})\n        if optimizations:\n            output_lines.append(\"  Optimizations:\")\n            for opt, value in optimizations.items():\n                output_lines.append(f\"    {opt}: {value}\")\n\n        # Print to stdout for backward compatibility\n        output = \"\\n\".join(output_lines)\n        print(output)\n\n        # Also log for consistency with other methods\n        logger.info(\"SLAF Dataset info displayed\")\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray-attributes","title":"Attributes","text":""},{"location":"api/core/#slaf.core.slaf.SLAFArray.obs","title":"<code>obs</code>  <code>property</code>","text":"<p>Cell metadata (lazy loaded)</p>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.var","title":"<code>var</code>  <code>property</code>","text":"<p>Gene metadata (lazy loaded)</p>"},{"location":"api/core/#slaf.core.slaf.SLAFArray-functions","title":"Functions","text":""},{"location":"api/core/#slaf.core.slaf.SLAFArray.__init__","title":"<code>__init__(slaf_path: str, load_metadata: bool = True, cache_dir: str | None = None)</code>","text":"<p>Initialize SLAF array from a SLAF dataset directory.</p> <p>Parameters:</p> Name Type Description Default <code>slaf_path</code> <code>str</code> <p>Path to SLAF directory containing config.json and .lance files.        The directory should contain the dataset configuration and Lance tables.        Supports local paths, cloud storage (S3, GCS, Azure, etc.), and        HuggingFace datasets (hf:// protocol).</p> required <code>load_metadata</code> <code>bool</code> <p>If True, load cell and gene metadata (obs/var) in background.           If False, skip metadata loading for faster initialization, especially           useful for cloud datasets. Metadata can still be loaded on-demand when           accessing obs/var properties. Default: True.</p> <code>True</code> <code>cache_dir</code> <code>str | None</code> <p>Optional cache directory for HuggingFace datasets. If None, uses       huggingface_hub's default cache location. Only used for hf:// paths.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the SLAF config file is not found at the specified path.</p> <code>ValueError</code> <p>If the config file is invalid or missing required tables.</p> <code>KeyError</code> <p>If required configuration keys are missing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load from local directory\n&gt;&gt;&gt; slaf_array = SLAFArray(\"./data/pbmc3k.slaf\")\n&gt;&gt;&gt; print(f\"Loaded dataset: {slaf_array.shape}\")\nLoaded dataset: (2700, 32738)\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from cloud storage without metadata (faster initialization)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"s3://bucket/data.slaf\", load_metadata=False)\n&gt;&gt;&gt; print(f\"Cloud dataset: {slaf_array.shape}\")\nCloud dataset: (5000, 25000)\n&gt;&gt;&gt; print(f\"Metadata loaded: {slaf_array.is_metadata_ready()}\")\nMetadata loaded: False\n</code></pre> <pre><code>&gt;&gt;&gt; # Load metadata on-demand\n&gt;&gt;&gt; slaf_array.wait_for_metadata()\n&gt;&gt;&gt; print(f\"Metadata loaded: {slaf_array.is_metadata_ready()}\")\nMetadata loaded: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Load from HuggingFace dataset\n&gt;&gt;&gt; slaf_array = SLAFArray(\"hf://datasets/repo/test-slaf\")\n&gt;&gt;&gt; print(f\"HuggingFace dataset: {slaf_array.shape}\")\nHuggingFace dataset: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for missing directory\n&gt;&gt;&gt; try:\n...     slaf_array = SLAFArray(\"nonexistent/path\")\n... except FileNotFoundError as e:\n...     print(f\"Error: {e}\")\nError: SLAF dataset not found: nonexistent/path\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def __init__(\n    self, slaf_path: str, load_metadata: bool = True, cache_dir: str | None = None\n):\n    \"\"\"\n    Initialize SLAF array from a SLAF dataset directory.\n\n    Args:\n        slaf_path: Path to SLAF directory containing config.json and .lance files.\n                   The directory should contain the dataset configuration and Lance tables.\n                   Supports local paths, cloud storage (S3, GCS, Azure, etc.), and\n                   HuggingFace datasets (hf:// protocol).\n        load_metadata: If True, load cell and gene metadata (obs/var) in background.\n                      If False, skip metadata loading for faster initialization, especially\n                      useful for cloud datasets. Metadata can still be loaded on-demand when\n                      accessing obs/var properties. Default: True.\n        cache_dir: Optional cache directory for HuggingFace datasets. If None, uses\n                  huggingface_hub's default cache location. Only used for hf:// paths.\n\n    Raises:\n        FileNotFoundError: If the SLAF config file is not found at the specified path.\n        ValueError: If the config file is invalid or missing required tables.\n        KeyError: If required configuration keys are missing.\n\n    Examples:\n        &gt;&gt;&gt; # Load from local directory\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"./data/pbmc3k.slaf\")\n        &gt;&gt;&gt; print(f\"Loaded dataset: {slaf_array.shape}\")\n        Loaded dataset: (2700, 32738)\n\n        &gt;&gt;&gt; # Load from cloud storage without metadata (faster initialization)\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"s3://bucket/data.slaf\", load_metadata=False)\n        &gt;&gt;&gt; print(f\"Cloud dataset: {slaf_array.shape}\")\n        Cloud dataset: (5000, 25000)\n        &gt;&gt;&gt; print(f\"Metadata loaded: {slaf_array.is_metadata_ready()}\")\n        Metadata loaded: False\n\n        &gt;&gt;&gt; # Load metadata on-demand\n        &gt;&gt;&gt; slaf_array.wait_for_metadata()\n        &gt;&gt;&gt; print(f\"Metadata loaded: {slaf_array.is_metadata_ready()}\")\n        Metadata loaded: True\n\n        &gt;&gt;&gt; # Load from HuggingFace dataset\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"hf://datasets/repo/test-slaf\")\n        &gt;&gt;&gt; print(f\"HuggingFace dataset: {slaf_array.shape}\")\n        HuggingFace dataset: (1000, 20000)\n\n        &gt;&gt;&gt; # Error handling for missing directory\n        &gt;&gt;&gt; try:\n        ...     slaf_array = SLAFArray(\"nonexistent/path\")\n        ... except FileNotFoundError as e:\n        ...     print(f\"Error: {e}\")\n        Error: SLAF dataset not found: nonexistent/path\n    \"\"\"\n    # Convert Path objects to strings for cloud compatibility\n    if hasattr(slaf_path, \"__fspath__\"):\n        slaf_path = str(slaf_path)\n\n    self.slaf_path = slaf_path\n    self._load_metadata_enabled = load_metadata\n    self._cache_dir = cache_dir\n    # Store cached config.json path for HuggingFace datasets\n    self._cached_config_path = None\n\n    # Validate dataset exists (after setting cache_dir so it can be used)\n    if not self._path_exists(self.slaf_path):\n        raise FileNotFoundError(f\"SLAF dataset not found: {self.slaf_path}\")\n\n    # Load configuration\n    config_path = self._join_path(self.slaf_path, \"config.json\")\n    with self._open_file(config_path) as f:\n        self.config = json.load(f)\n\n    # Initialize shape\n    self.shape = tuple(self.config[\"array_shape\"])\n\n    # Setup datasets\n    self._setup_datasets()\n\n    # Initialize metadata loading\n    self._obs = None\n    self._var = None\n    self._metadata_loaded = False\n    self._metadata_loading = False\n    self._metadata_thread = None\n    self._metadata_error = None\n\n    # Initialize row mapper for cell-based queries\n    from slaf.core.query_optimizer import RowIndexMapper\n\n    self.row_mapper = RowIndexMapper(self)\n\n    # Start async metadata loading in background only if enabled\n    if self._load_metadata_enabled:\n        self._start_async_metadata_loading()\n\n    # Display helpful initialization message\n    self._display_initialization_message()\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.is_metadata_ready","title":"<code>is_metadata_ready() -&gt; bool</code>","text":"<p>Check if metadata is ready for use</p> Source code in <code>slaf/core/slaf.py</code> <pre><code>def is_metadata_ready(self) -&gt; bool:\n    \"\"\"Check if metadata is ready for use\"\"\"\n    return self._metadata_loaded\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.is_metadata_loading","title":"<code>is_metadata_loading() -&gt; bool</code>","text":"<p>Check if metadata is currently loading</p> Source code in <code>slaf/core/slaf.py</code> <pre><code>def is_metadata_loading(self) -&gt; bool:\n    \"\"\"Check if metadata is currently loading\"\"\"\n    return self._metadata_loading\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.wait_for_metadata","title":"<code>wait_for_metadata(timeout: float = None)</code>","text":"<p>Wait for metadata to be loaded (with optional timeout)</p> Source code in <code>slaf/core/slaf.py</code> <pre><code>def wait_for_metadata(self, timeout: float = None):\n    \"\"\"Wait for metadata to be loaded (with optional timeout)\"\"\"\n    if self._metadata_loaded:\n        return\n\n    if self._metadata_loading and self._metadata_thread:\n        if timeout:\n            self._metadata_thread.join(timeout=timeout)\n        else:\n            self._metadata_thread.join()\n\n    if not self._metadata_loaded:\n        # Fallback to synchronous loading\n        self._load_metadata()\n        self._metadata_loaded = True\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.query","title":"<code>query(sql: str) -&gt; pl.DataFrame</code>","text":"<p>Execute SQL query on the SLAF dataset.</p> <p>Executes SQL queries directly on the underlying Lance tables using Polars. The query can reference tables: 'cells', 'genes', 'expression', and optionally 'layers'. This enables complex aggregations, joins, and filtering operations.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string to execute. Can reference tables: cells, genes, expression, layers.  Supports standard SQL operations including WHERE, GROUP BY, ORDER BY, etc.  The 'layers' table is only available if the dataset has layers (format version 0.4+).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing the query results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the SQL query is malformed or references non-existent tables.</p> <code>RuntimeError</code> <p>If the query execution fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic query to count cells\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; result = slaf_array.query(\"SELECT COUNT(*) as total_cells FROM cells\")\n&gt;&gt;&gt; print(f\"Total cells: {result['total_cells'][0]}\")\nTotal cells: 1000\n</code></pre> <pre><code>&gt;&gt;&gt; # Complex aggregation query\n&gt;&gt;&gt; result = slaf_array.query(\"\n...     SELECT cell_type,\n...            COUNT(*) as cell_count,\n...            AVG(total_counts) as avg_counts\n...     FROM cells\n...     WHERE total_counts &gt; 500\n...     GROUP BY cell_type\n...     ORDER BY avg_counts DESC\n... \")\n&gt;&gt;&gt; print(result)\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 cell_type  \u2506 cell_count \u2506 avg_counts \u2502\n\u2502 ---        \u2506 ---        \u2506 ---        \u2502\n\u2502 str        \u2506 i64        \u2506 f64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 T cells    \u2506 250        \u2506 1250.5     \u2502\n\u2502 B cells    \u2506 200        \u2506 1100.2     \u2502\n\u2502 Monocytes  \u2506 150        \u2506 950.8      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; # Join query across tables\n&gt;&gt;&gt; result = slaf_array.query(\"\n...     SELECT c.cell_type, g.gene_type, AVG(e.value) as avg_expression\n...     FROM cells c\n...     JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n...     JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n...     WHERE c.cell_type = 'T cells'\n...     GROUP BY c.cell_type, g.gene_type\n... \")\n&gt;&gt;&gt; print(f\"Found {len(result)} expression patterns\")\nFound 5 expression patterns\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def query(self, sql: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Execute SQL query on the SLAF dataset.\n\n    Executes SQL queries directly on the underlying Lance tables using Polars.\n    The query can reference tables: 'cells', 'genes', 'expression', and optionally 'layers'.\n    This enables complex aggregations, joins, and filtering operations.\n\n    Args:\n        sql: SQL query string to execute. Can reference tables: cells, genes, expression, layers.\n             Supports standard SQL operations including WHERE, GROUP BY, ORDER BY, etc.\n             The 'layers' table is only available if the dataset has layers (format version 0.4+).\n\n    Returns:\n        Polars DataFrame containing the query results.\n\n    Raises:\n        ValueError: If the SQL query is malformed or references non-existent tables.\n        RuntimeError: If the query execution fails.\n\n    Examples:\n        &gt;&gt;&gt; # Basic query to count cells\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; result = slaf_array.query(\"SELECT COUNT(*) as total_cells FROM cells\")\n        &gt;&gt;&gt; print(f\"Total cells: {result['total_cells'][0]}\")\n        Total cells: 1000\n\n        &gt;&gt;&gt; # Complex aggregation query\n        &gt;&gt;&gt; result = slaf_array.query(\"\n        ...     SELECT cell_type,\n        ...            COUNT(*) as cell_count,\n        ...            AVG(total_counts) as avg_counts\n        ...     FROM cells\n        ...     WHERE total_counts &gt; 500\n        ...     GROUP BY cell_type\n        ...     ORDER BY avg_counts DESC\n        ... \")\n        &gt;&gt;&gt; print(result)\n        shape: (3, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 cell_type  \u2506 cell_count \u2506 avg_counts \u2502\n        \u2502 ---        \u2506 ---        \u2506 ---        \u2502\n        \u2502 str        \u2506 i64        \u2506 f64        \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 T cells    \u2506 250        \u2506 1250.5     \u2502\n        \u2502 B cells    \u2506 200        \u2506 1100.2     \u2502\n        \u2502 Monocytes  \u2506 150        \u2506 950.8      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; # Join query across tables\n        &gt;&gt;&gt; result = slaf_array.query(\"\n        ...     SELECT c.cell_type, g.gene_type, AVG(e.value) as avg_expression\n        ...     FROM cells c\n        ...     JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n        ...     JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n        ...     WHERE c.cell_type = 'T cells'\n        ...     GROUP BY c.cell_type, g.gene_type\n        ... \")\n        &gt;&gt;&gt; print(f\"Found {len(result)} expression patterns\")\n        Found 5 expression patterns\n    \"\"\"\n    # Create Polars context with all Lance datasets\n    ctx = pl.SQLContext()\n\n    # Register Lance datasets with Polars context\n    ctx.register(\"expression\", pl.scan_pyarrow_dataset(self.expression))\n    ctx.register(\"cells\", pl.scan_pyarrow_dataset(self.cells))\n    ctx.register(\"genes\", pl.scan_pyarrow_dataset(self.genes))\n\n    # NEW: Register layers table if it exists (format version 0.4+)\n    if self.layers is not None:\n        ctx.register(\"layers\", pl.scan_pyarrow_dataset(self.layers))\n\n    # Execute the query using Polars SQL\n    result = ctx.execute(sql).collect()\n\n    return result\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.filter_cells","title":"<code>filter_cells(**filters: Any) -&gt; pl.DataFrame</code>","text":"<p>Filter cells based on metadata columns.</p> <p>Provides a convenient interface for filtering cells using metadata columns. Supports exact matches, list values, and range queries with operators. Uses in-memory polars filtering when metadata is loaded, falls back to SQL otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>**filters</code> <code>Any</code> <p>Column name and filter value pairs. Supports: - Exact matches: cell_type=\"T cells\" - List values: cell_type=[\"T cells\", \"B cells\"] - Range queries: total_counts=\"&gt;1000\", total_counts=\"&lt;=2000\" - Multiple conditions: cell_type=\"T cells\", total_counts=\"&gt;500\"</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing filtered cell metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a specified column is not found in cell metadata.</p> <code>TypeError</code> <p>If filter values are of unsupported types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Filter by cell type\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; t_cells = slaf_array.filter_cells(cell_type=\"T cells\")\n&gt;&gt;&gt; print(f\"Found {len(t_cells)} T cells\")\nFound 250 T cells\n</code></pre> <pre><code>&gt;&gt;&gt; # Filter by multiple criteria\n&gt;&gt;&gt; high_quality_t_cells = slaf_array.filter_cells(\n...     cell_type=\"T cells\",\n...     total_counts=\"&gt;1000\",\n...     batch=[\"batch1\", \"batch2\"]\n... )\n&gt;&gt;&gt; print(f\"Found {len(high_quality_t_cells)} high-quality T cells\")\nFound 180 high-quality T cells\n</code></pre> <pre><code>&gt;&gt;&gt; # Range query\n&gt;&gt;&gt; medium_counts = slaf_array.filter_cells(\n...     total_counts=\"&gt;=500\",\n...     total_counts=\"&lt;=2000\"\n... )\n&gt;&gt;&gt; print(f\"Found {len(medium_counts)} cells with medium counts\")\nFound 450 cells with medium counts\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid column\n&gt;&gt;&gt; try:\n...     result = slaf_array.filter_cells(invalid_column=\"value\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Column 'invalid_column' not found in cell metadata\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def filter_cells(self, **filters: Any) -&gt; pl.DataFrame:\n    \"\"\"\n    Filter cells based on metadata columns.\n\n    Provides a convenient interface for filtering cells using metadata columns.\n    Supports exact matches, list values, and range queries with operators.\n    Uses in-memory polars filtering when metadata is loaded, falls back to SQL otherwise.\n\n    Args:\n        **filters: Column name and filter value pairs. Supports:\n            - Exact matches: cell_type=\"T cells\"\n            - List values: cell_type=[\"T cells\", \"B cells\"]\n            - Range queries: total_counts=\"&gt;1000\", total_counts=\"&lt;=2000\"\n            - Multiple conditions: cell_type=\"T cells\", total_counts=\"&gt;500\"\n\n    Returns:\n        Polars DataFrame containing filtered cell metadata.\n\n    Raises:\n        ValueError: If a specified column is not found in cell metadata.\n        TypeError: If filter values are of unsupported types.\n\n    Examples:\n        &gt;&gt;&gt; # Filter by cell type\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; t_cells = slaf_array.filter_cells(cell_type=\"T cells\")\n        &gt;&gt;&gt; print(f\"Found {len(t_cells)} T cells\")\n        Found 250 T cells\n\n        &gt;&gt;&gt; # Filter by multiple criteria\n        &gt;&gt;&gt; high_quality_t_cells = slaf_array.filter_cells(\n        ...     cell_type=\"T cells\",\n        ...     total_counts=\"&gt;1000\",\n        ...     batch=[\"batch1\", \"batch2\"]\n        ... )\n        &gt;&gt;&gt; print(f\"Found {len(high_quality_t_cells)} high-quality T cells\")\n        Found 180 high-quality T cells\n\n        &gt;&gt;&gt; # Range query\n        &gt;&gt;&gt; medium_counts = slaf_array.filter_cells(\n        ...     total_counts=\"&gt;=500\",\n        ...     total_counts=\"&lt;=2000\"\n        ... )\n        &gt;&gt;&gt; print(f\"Found {len(medium_counts)} cells with medium counts\")\n        Found 450 cells with medium counts\n\n        &gt;&gt;&gt; # Error handling for invalid column\n        &gt;&gt;&gt; try:\n        ...     result = slaf_array.filter_cells(invalid_column=\"value\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Column 'invalid_column' not found in cell metadata\n    \"\"\"\n    return self._filter(\"cells\", **filters)\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.filter_genes","title":"<code>filter_genes(**filters: Any) -&gt; pl.DataFrame</code>","text":"<p>Filter genes based on metadata columns.</p> <p>Provides a convenient interface for filtering genes using metadata columns. Supports exact matches, list values, and range queries with operators. Uses in-memory polars filtering when metadata is loaded, falls back to SQL otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>**filters</code> <code>Any</code> <p>Column name and filter value pairs. Supports: - Exact matches: gene_type=\"protein_coding\" - List values: gene_type=[\"protein_coding\", \"lncRNA\"] - Range queries: expression_mean=\"&gt;5.0\", expression_mean=\"&lt;=10.0\" - Multiple conditions: gene_type=\"protein_coding\", chromosome=\"chr1\"</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing filtered gene metadata.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a specified column is not found in gene metadata.</p> <code>TypeError</code> <p>If filter values are of unsupported types.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Filter by gene type\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; protein_coding = slaf_array.filter_genes(gene_type=\"protein_coding\")\n&gt;&gt;&gt; print(f\"Found {len(protein_coding)} protein-coding genes\")\nFound 15000 protein-coding genes\n</code></pre> <pre><code>&gt;&gt;&gt; # Filter by multiple criteria\n&gt;&gt;&gt; high_expr_proteins = slaf_array.filter_genes(\n...     gene_type=\"protein_coding\",\n...     expression_mean=\"&gt;5.0\",\n...     chromosome=[\"chr1\", \"chr2\"]\n... )\n&gt;&gt;&gt; print(f\"Found {len(high_expr_proteins)} high-expression protein genes\")\nFound 2500 high-expression protein genes\n</code></pre> <pre><code>&gt;&gt;&gt; # Range query for expression\n&gt;&gt;&gt; medium_expr = slaf_array.filter_genes(\n...     expression_mean=\"&gt;=2.0\",\n...     expression_mean=\"&lt;=8.0\"\n... )\n&gt;&gt;&gt; print(f\"Found {len(medium_expr)} genes with medium expression\")\nFound 8000 genes with medium expression\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid column\n&gt;&gt;&gt; try:\n...     result = slaf_array.filter_genes(invalid_column=\"value\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Column 'invalid_column' not found in gene metadata\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def filter_genes(self, **filters: Any) -&gt; pl.DataFrame:\n    \"\"\"\n    Filter genes based on metadata columns.\n\n    Provides a convenient interface for filtering genes using metadata columns.\n    Supports exact matches, list values, and range queries with operators.\n    Uses in-memory polars filtering when metadata is loaded, falls back to SQL otherwise.\n\n    Args:\n        **filters: Column name and filter value pairs. Supports:\n            - Exact matches: gene_type=\"protein_coding\"\n            - List values: gene_type=[\"protein_coding\", \"lncRNA\"]\n            - Range queries: expression_mean=\"&gt;5.0\", expression_mean=\"&lt;=10.0\"\n            - Multiple conditions: gene_type=\"protein_coding\", chromosome=\"chr1\"\n\n    Returns:\n        Polars DataFrame containing filtered gene metadata.\n\n    Raises:\n        ValueError: If a specified column is not found in gene metadata.\n        TypeError: If filter values are of unsupported types.\n\n    Examples:\n        &gt;&gt;&gt; # Filter by gene type\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; protein_coding = slaf_array.filter_genes(gene_type=\"protein_coding\")\n        &gt;&gt;&gt; print(f\"Found {len(protein_coding)} protein-coding genes\")\n        Found 15000 protein-coding genes\n\n        &gt;&gt;&gt; # Filter by multiple criteria\n        &gt;&gt;&gt; high_expr_proteins = slaf_array.filter_genes(\n        ...     gene_type=\"protein_coding\",\n        ...     expression_mean=\"&gt;5.0\",\n        ...     chromosome=[\"chr1\", \"chr2\"]\n        ... )\n        &gt;&gt;&gt; print(f\"Found {len(high_expr_proteins)} high-expression protein genes\")\n        Found 2500 high-expression protein genes\n\n        &gt;&gt;&gt; # Range query for expression\n        &gt;&gt;&gt; medium_expr = slaf_array.filter_genes(\n        ...     expression_mean=\"&gt;=2.0\",\n        ...     expression_mean=\"&lt;=8.0\"\n        ... )\n        &gt;&gt;&gt; print(f\"Found {len(medium_expr)} genes with medium expression\")\n        Found 8000 genes with medium expression\n\n        &gt;&gt;&gt; # Error handling for invalid column\n        &gt;&gt;&gt; try:\n        ...     result = slaf_array.filter_genes(invalid_column=\"value\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Column 'invalid_column' not found in gene metadata\n    \"\"\"\n    return self._filter(\"genes\", **filters)\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.get_cell_expression","title":"<code>get_cell_expression(cell_ids: str | list[str]) -&gt; pl.DataFrame</code>","text":"<p>Get expression data for specific cells using Lance take() and Polars.</p> <p>Retrieves expression data for specified cells using efficient Lance row access and Polars for in-memory operations. This method provides significant performance improvements over SQL-based queries.</p> <p>Parameters:</p> Name Type Description Default <code>cell_ids</code> <code>str | list[str]</code> <p>Single cell ID (string) or list of cell IDs to retrieve.      Can be string identifiers or integer IDs.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing expression data for the specified cells.</p> <code>DataFrame</code> <p>Columns include cell_id, gene_id, and expression values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any cell ID is not found in the dataset.</p> <code>RuntimeError</code> <p>If the query execution fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get expression for a single cell\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; expression = slaf_array.get_cell_expression(\"cell_001\")\n&gt;&gt;&gt; print(f\"Expression data shape: {expression.shape}\")\nExpression data shape: (15000, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; # Get expression for multiple cells\n&gt;&gt;&gt; expression = slaf_array.get_cell_expression([\"cell_001\", \"cell_002\", \"cell_003\"])\n&gt;&gt;&gt; print(f\"Expression data shape: {expression.shape}\")\nExpression data shape: (45000, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid cell ID\n&gt;&gt;&gt; try:\n...     expression = slaf_array.get_cell_expression(\"invalid_cell\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: cell ID 'invalid_cell' not found\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def get_cell_expression(self, cell_ids: str | list[str]) -&gt; pl.DataFrame:\n    \"\"\"\n    Get expression data for specific cells using Lance take() and Polars.\n\n    Retrieves expression data for specified cells using efficient Lance row access\n    and Polars for in-memory operations. This method provides significant\n    performance improvements over SQL-based queries.\n\n    Args:\n        cell_ids: Single cell ID (string) or list of cell IDs to retrieve.\n                 Can be string identifiers or integer IDs.\n\n    Returns:\n        Polars DataFrame containing expression data for the specified cells.\n        Columns include cell_id, gene_id, and expression values.\n\n    Raises:\n        ValueError: If any cell ID is not found in the dataset.\n        RuntimeError: If the query execution fails.\n\n    Examples:\n        &gt;&gt;&gt; # Get expression for a single cell\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; expression = slaf_array.get_cell_expression(\"cell_001\")\n        &gt;&gt;&gt; print(f\"Expression data shape: {expression.shape}\")\n        Expression data shape: (15000, 3)\n\n        &gt;&gt;&gt; # Get expression for multiple cells\n        &gt;&gt;&gt; expression = slaf_array.get_cell_expression([\"cell_001\", \"cell_002\", \"cell_003\"])\n        &gt;&gt;&gt; print(f\"Expression data shape: {expression.shape}\")\n        Expression data shape: (45000, 3)\n\n        &gt;&gt;&gt; # Error handling for invalid cell ID\n        &gt;&gt;&gt; try:\n        ...     expression = slaf_array.get_cell_expression(\"invalid_cell\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: cell ID 'invalid_cell' not found\n    \"\"\"\n    # Convert to integer IDs\n    integer_ids = self._normalize_entity_ids(cell_ids, \"cell\")\n\n    if not integer_ids:\n        return pl.DataFrame({\"cell_id\": [], \"gene_id\": [], \"value\": []})\n\n    # Get row indices using RowIndexMapper\n    row_indices = self.row_mapper.get_cell_row_ranges(integer_ids)\n\n    # Load data with Lance take()\n    expression_data = self.expression.take(row_indices)\n\n    # Convert PyArrow Table to Polars DataFrame and join with metadata\n    expression_df = pl.from_arrow(expression_data)\n    return self._join_with_metadata(expression_df)\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.get_gene_expression","title":"<code>get_gene_expression(gene_ids: str | list[str]) -&gt; pl.DataFrame</code>","text":"<p>Get gene expression data for specified genes.</p> <p>This method uses QueryOptimizer to generate optimal SQL queries based on the gene ID distribution (consecutive ranges use BETWEEN, scattered values use IN).</p> <p>Parameters:</p> Name Type Description Default <code>gene_ids</code> <code>str | list[str]</code> <p>Gene ID(s) to query. Can be a single gene ID string or list of gene IDs.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing expression data for the specified genes.</p> <code>DataFrame</code> <p>Columns include cell_id, gene_id, and expression values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any gene ID is not found in the dataset.</p> <code>RuntimeError</code> <p>If the query execution fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get expression for a single gene\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; expression = slaf_array.get_gene_expression(\"GENE1\")\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def get_gene_expression(self, gene_ids: str | list[str]) -&gt; pl.DataFrame:\n    \"\"\"\n    Get gene expression data for specified genes.\n\n    This method uses QueryOptimizer to generate optimal SQL queries based on the\n    gene ID distribution (consecutive ranges use BETWEEN, scattered values use IN).\n\n    Args:\n        gene_ids: Gene ID(s) to query. Can be a single gene ID string or list of gene IDs.\n\n    Returns:\n        Polars DataFrame containing expression data for the specified genes.\n        Columns include cell_id, gene_id, and expression values.\n\n    Raises:\n        ValueError: If any gene ID is not found in the dataset.\n        RuntimeError: If the query execution fails.\n\n    Examples:\n        &gt;&gt;&gt; # Get expression for a single gene\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; expression = slaf_array.get_gene_expression(\"GENE1\")\n    \"\"\"\n\n    # Convert to list if single gene ID\n    if isinstance(gene_ids, str):\n        gene_ids = [gene_ids]\n\n    # Convert string gene IDs to integer IDs\n    integer_ids = self._normalize_entity_ids(gene_ids, \"gene\")\n\n    if not integer_ids:\n        return pl.DataFrame({\"cell_id\": [], \"gene_id\": [], \"value\": []})\n\n    # Use QueryOptimizer for optimal performance\n    from slaf.core.query_optimizer import QueryOptimizer\n\n    # Build optimized SQL query using QueryOptimizer\n    sql_query = QueryOptimizer.build_optimized_query(\n        entity_ids=integer_ids,\n        entity_type=\"gene\",\n        use_adaptive_batching=True,\n        max_batch_size=100,\n    )\n\n    # Fix the table name for Polars scan (use 'self' instead of 'expression')\n    sql_query = sql_query.replace(\"expression\", \"self\")\n\n    # Execute the optimized query\n    ldf = pl.scan_pyarrow_dataset(self.expression)\n    expression_df = ldf.sql(sql_query).collect()\n\n    # Join with metadata using Polars\n    return self._join_with_metadata(expression_df)\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.get_submatrix","title":"<code>get_submatrix(cell_selector: Any | None = None, gene_selector: Any | None = None, table_name: str = 'expression', layer_name: str | None = None) -&gt; pl.DataFrame</code>","text":"<p>Get expression or layer data using cell/gene selectors with Lance take() and Polars.</p> <p>Retrieves a subset of data based on cell and gene selectors. The selectors can be slices, lists, boolean masks, or None for all cells/genes. This method provides a flexible interface for subsetting data with significant performance improvements over SQL-based queries.</p> <p>Parameters:</p> Name Type Description Default <code>cell_selector</code> <code>Any | None</code> <p>Cell selector for subsetting. Can be: - None: Include all cells - slice: e.g., slice(0, 100) for first 100 cells - list: e.g., [0, 5, 10] for specific cell indices - boolean mask: e.g., [True, False, True, ...] for boolean selection</p> <code>None</code> <code>gene_selector</code> <code>Any | None</code> <p>Gene selector for subsetting. Can be: - None: Include all genes - slice: e.g., slice(0, 5000) for first 5000 genes - list: e.g., [0, 100, 200] for specific gene indices - boolean mask: e.g., [True, False, True, ...] for boolean selection</p> <code>None</code> <code>table_name</code> <code>str</code> <p>Table to query (\"expression\" or \"layers\"). Default: \"expression\"</p> <code>'expression'</code> <code>layer_name</code> <code>str | None</code> <p>Layer name for layers table (required when table_name=\"layers\").        Default: None</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame containing data for the selected subset.</p> <code>DataFrame</code> <p>Columns include cell_id, gene_id, and value (or layer column name).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If selectors are invalid or out of bounds.</p> <code>RuntimeError</code> <p>If the query execution fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get first 100 cells and first 5000 genes from expression\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; submatrix = slaf_array.get_submatrix(\n...     cell_selector=slice(0, 100),\n...     gene_selector=slice(0, 5000)\n... )\n&gt;&gt;&gt; print(f\"Submatrix shape: {submatrix.shape}\")\nSubmatrix shape: (500000, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; # Get data from a layer\n&gt;&gt;&gt; submatrix = slaf_array.get_submatrix(\n...     cell_selector=slice(0, 100),\n...     table_name=\"layers\",\n...     layer_name=\"spliced\"\n... )\n&gt;&gt;&gt; print(f\"Layer submatrix shape: {submatrix.shape}\")\nLayer submatrix shape: (500000, 3)\n</code></pre> Source code in <code>slaf/core/slaf.py</code> <pre><code>def get_submatrix(\n    self,\n    cell_selector: Any | None = None,\n    gene_selector: Any | None = None,\n    table_name: str = \"expression\",\n    layer_name: str | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Get expression or layer data using cell/gene selectors with Lance take() and Polars.\n\n    Retrieves a subset of data based on cell and gene selectors.\n    The selectors can be slices, lists, boolean masks, or None for all cells/genes.\n    This method provides a flexible interface for subsetting data with\n    significant performance improvements over SQL-based queries.\n\n    Args:\n        cell_selector: Cell selector for subsetting. Can be:\n            - None: Include all cells\n            - slice: e.g., slice(0, 100) for first 100 cells\n            - list: e.g., [0, 5, 10] for specific cell indices\n            - boolean mask: e.g., [True, False, True, ...] for boolean selection\n        gene_selector: Gene selector for subsetting. Can be:\n            - None: Include all genes\n            - slice: e.g., slice(0, 5000) for first 5000 genes\n            - list: e.g., [0, 100, 200] for specific gene indices\n            - boolean mask: e.g., [True, False, True, ...] for boolean selection\n        table_name: Table to query (\"expression\" or \"layers\"). Default: \"expression\"\n        layer_name: Layer name for layers table (required when table_name=\"layers\").\n                   Default: None\n\n    Returns:\n        Polars DataFrame containing data for the selected subset.\n        Columns include cell_id, gene_id, and value (or layer column name).\n\n    Raises:\n        ValueError: If selectors are invalid or out of bounds.\n        RuntimeError: If the query execution fails.\n\n    Examples:\n        &gt;&gt;&gt; # Get first 100 cells and first 5000 genes from expression\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; submatrix = slaf_array.get_submatrix(\n        ...     cell_selector=slice(0, 100),\n        ...     gene_selector=slice(0, 5000)\n        ... )\n        &gt;&gt;&gt; print(f\"Submatrix shape: {submatrix.shape}\")\n        Submatrix shape: (500000, 3)\n\n        &gt;&gt;&gt; # Get data from a layer\n        &gt;&gt;&gt; submatrix = slaf_array.get_submatrix(\n        ...     cell_selector=slice(0, 100),\n        ...     table_name=\"layers\",\n        ...     layer_name=\"spliced\"\n        ... )\n        &gt;&gt;&gt; print(f\"Layer submatrix shape: {submatrix.shape}\")\n        Layer submatrix shape: (500000, 3)\n    \"\"\"\n    # For layers table, use SQL query (RowIndexMapper is expression-specific)\n    if table_name == \"layers\":\n        if layer_name is None:\n            raise ValueError(\"layer_name must be provided when table_name='layers'\")\n        if self.layers is None:\n            raise ValueError(\"Layers table not available in this dataset\")\n\n        # Use QueryOptimizer to build SQL query\n        from slaf.core.query_optimizer import QueryOptimizer\n\n        sql_query = QueryOptimizer.build_submatrix_query(\n            cell_selector=cell_selector,\n            gene_selector=gene_selector,\n            cell_count=self.shape[0],\n            gene_count=self.shape[1],\n            table_name=table_name,\n            layer_name=layer_name,\n        )\n\n        # Execute query\n        result_df = self.query(sql_query)\n\n        # Join with metadata\n        return self._join_with_metadata(result_df)\n\n    # For expression table, use existing optimized path with RowIndexMapper\n    # Get row indices for cells using RowIndexMapper\n    cell_indices = self.row_mapper.get_cell_row_ranges_by_selector(cell_selector)\n\n    # Load data with Lance take()\n    expression_data = self.expression.take(cell_indices)\n    expression_df = pl.from_arrow(expression_data)\n\n    # Filter by genes if specified\n    if gene_selector is not None:\n        # Convert gene selector to integer IDs\n        if isinstance(gene_selector, int):\n            if gene_selector &lt; 0:\n                gene_selector = self.shape[1] + gene_selector\n            if 0 &lt;= gene_selector &lt; self.shape[1]:\n                gene_integer_id = self.var[\"gene_integer_id\"][gene_selector]\n                expression_df = expression_df.filter(\n                    pl.col(\"gene_integer_id\") == gene_integer_id  # type: ignore[arg-type]\n                )\n            else:\n                raise ValueError(f\"Gene index {gene_selector} out of bounds\")\n        elif isinstance(gene_selector, slice):\n            start = gene_selector.start or 0\n            stop = gene_selector.stop or self.shape[1]\n            step = gene_selector.step or 1\n\n            # Handle negative indices\n            if start &lt; 0:\n                start = self.shape[1] + start\n            if stop &lt; 0:\n                stop = self.shape[1] + stop\n\n            # Clamp bounds\n            start = max(0, min(start, self.shape[1]))\n            stop = max(0, min(stop, self.shape[1]))\n\n            gene_integer_ids = self.var[\"gene_integer_id\"][\n                start:stop:step\n            ].to_list()\n            expression_df = expression_df.filter(\n                pl.col(\"gene_integer_id\").is_in(gene_integer_ids)  # type: ignore[arg-type]\n            )\n        elif isinstance(gene_selector, list):\n            gene_integer_ids = []\n            for idx in gene_selector:\n                if isinstance(idx, int):\n                    if idx &lt; 0:\n                        idx = self.shape[1] + idx\n                    if 0 &lt;= idx &lt; self.shape[1]:\n                        gene_integer_id = self.var[\"gene_integer_id\"][idx]\n                        gene_integer_ids.append(gene_integer_id)\n                    else:\n                        raise ValueError(f\"Gene index {idx} out of bounds\")\n                else:\n                    raise ValueError(f\"Invalid gene index type: {type(idx)}\")\n            expression_df = expression_df.filter(\n                pl.col(\"gene_integer_id\").is_in(gene_integer_ids)  # type: ignore[arg-type]\n            )\n        elif isinstance(gene_selector, np.ndarray) and gene_selector.dtype == bool:\n            if len(gene_selector) != self.shape[1]:\n                raise ValueError(\n                    f\"Boolean mask length {len(gene_selector)} doesn't match gene count {self.shape[1]}\"\n                )\n            gene_integer_ids = self.var[\"gene_integer_id\"][gene_selector].to_list()\n            expression_df = expression_df.filter(\n                pl.col(\"gene_integer_id\").is_in(gene_integer_ids)  # type: ignore[arg-type]\n            )\n        else:\n            raise ValueError(\n                f\"Unsupported gene selector type: {type(gene_selector)}\"\n            )\n\n    # Join with metadata using Polars\n    return self._join_with_metadata(expression_df)\n</code></pre>"},{"location":"api/core/#slaf.core.slaf.SLAFArray.info","title":"<code>info()</code>","text":"<p>Print information about the SLAF dataset</p> Source code in <code>slaf/core/slaf.py</code> <pre><code>def info(self):\n    \"\"\"Print information about the SLAF dataset\"\"\"\n    # Build output string for both logging and printing\n    output_lines = []\n\n    output_lines.append(\"SLAF Dataset\")\n    output_lines.append(f\"  Shape: {self.shape[0]} cells \u00d7 {self.shape[1]} genes\")\n    output_lines.append(\n        f\"  Format version: {self.config.get('format_version', 'unknown')}\"\n    )\n\n    # Cell metadata columns\n    cell_cols = self.obs.columns\n    output_lines.append(f\"  Cell metadata columns: {len(cell_cols)}\")\n    if cell_cols:\n        output_lines.append(\n            f\"    {', '.join(cell_cols[:5])}{'...' if len(cell_cols) &gt; 5 else ''}\"\n        )\n\n    # Gene metadata columns\n    gene_cols = self.var.columns\n    output_lines.append(f\"  Gene metadata columns: {len(gene_cols)}\")\n    if gene_cols:\n        output_lines.append(\n            f\"    {', '.join(gene_cols[:5])}{'...' if len(gene_cols) &gt; 5 else ''}\"\n        )\n\n    # Record counts\n    output_lines.append(\"  Record counts:\")\n    output_lines.append(f\"    Cells: {len(self.obs):,}\")\n    output_lines.append(f\"    Genes: {len(self.var):,}\")\n\n    # Expression metadata - use pre-computed if available, otherwise query\n    format_version = self.config.get(\"format_version\", \"0.1\")\n    if format_version &gt;= \"0.2\" and \"metadata\" in self.config:\n        # Use pre-computed metadata from config\n        metadata = self.config[\"metadata\"]\n        expression_count = metadata[\"expression_count\"]\n        sparsity = metadata[\"sparsity\"]\n        density = metadata[\"density\"]\n\n        output_lines.append(f\"    Expression records: {expression_count:,}\")\n        output_lines.append(f\"    Sparsity: {sparsity:.1%}\")\n        output_lines.append(f\"    Density: {density:.1%}\")\n\n        # Show expression statistics if available\n        if \"expression_stats\" in metadata:\n            stats = metadata[\"expression_stats\"]\n            output_lines.append(\"  Expression statistics:\")\n            output_lines.append(f\"    Min value: {stats['min_value']:.3f}\")\n            output_lines.append(f\"    Max value: {stats['max_value']:.3f}\")\n            output_lines.append(f\"    Mean value: {stats['mean_value']:.3f}\")\n            output_lines.append(f\"    Std value: {stats['std_value']:.3f}\")\n    else:\n        # Backward compatibility: query expression count for older format versions\n        output_lines.append(\"    Expression records: computing...\")\n        expression_count = self.query(\"SELECT COUNT(*) as count FROM expression\")\n        output_lines.append(\n            f\"    Expression records: {expression_count.item(0, 0):,}\"\n        )\n\n    # Optimization info\n    optimizations = self.config.get(\"optimizations\", {})\n    if optimizations:\n        output_lines.append(\"  Optimizations:\")\n        for opt, value in optimizations.items():\n            output_lines.append(f\"    {opt}: {value}\")\n\n    # Print to stdout for backward compatibility\n    output = \"\\n\".join(output_lines)\n    print(output)\n\n    # Also log for consistency with other methods\n    logger.info(\"SLAF Dataset info displayed\")\n</code></pre>"},{"location":"api/data/","title":"Data API","text":"<p>Data conversion utilities for SLAF format.</p>"},{"location":"api/data/#slafconverter","title":"SLAFConverter","text":""},{"location":"api/data/#slaf.data.converter.SLAFConverter","title":"<code>slaf.data.converter.SLAFConverter</code>","text":"<p>Convert single-cell data formats to SLAF format with optimized storage.</p> <p>SLAFConverter provides efficient conversion from various single-cell data formats (primarily AnnData/h5ad) to the SLAF format. It optimizes storage by using integer keys, COO-style expression tables, and efficient metadata handling. Chunked conversion is now the default for optimal memory efficiency.</p> Key Features <ul> <li>AnnData/h5ad file conversion</li> <li>Integer key optimization for memory efficiency</li> <li>COO-style sparse matrix storage</li> <li>Automatic metadata type inference</li> <li>Lance format for high-performance storage</li> <li>Chunked processing by default for memory efficiency</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic conversion from h5ad file (chunked is now the default)\n&gt;&gt;&gt; converter = SLAFConverter()\n&gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output.slaf\")\nConverting data.h5ad to SLAF format...\nOptimizations: int_keys=True\nLoaded: 1000 cells \u00d7 20000 genes\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Conversion with custom optimization settings\n&gt;&gt;&gt; converter = SLAFConverter(use_integer_keys=False)\n&gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output_string_keys.slaf\")\nConverting data.h5ad to SLAF format...\nOptimizations: int_keys=False\nLoaded: 1000 cells \u00d7 20000 genes\nConversion complete! Saved to output_string_keys.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Convert existing AnnData object (chunked is now the default)\n&gt;&gt;&gt; import scanpy as sc\n&gt;&gt;&gt; adata = sc.read_h5ad(\"data.h5ad\")\n&gt;&gt;&gt; converter = SLAFConverter()\n&gt;&gt;&gt; converter.convert_anndata(adata, \"output_from_object.slaf\")\nConverting AnnData object to SLAF format...\nOptimizations: int_keys=True\nLoaded: 1000 cells \u00d7 20000 genes\nConversion complete! Saved to output_from_object.slaf\n</code></pre> Source code in <code>slaf/data/converter.py</code> <pre><code>class SLAFConverter:\n    \"\"\"\n    Convert single-cell data formats to SLAF format with optimized storage.\n\n    SLAFConverter provides efficient conversion from various single-cell data formats\n    (primarily AnnData/h5ad) to the SLAF format. It optimizes storage by using\n    integer keys, COO-style expression tables, and efficient metadata handling.\n    Chunked conversion is now the default for optimal memory efficiency.\n\n    Key Features:\n        - AnnData/h5ad file conversion\n        - Integer key optimization for memory efficiency\n        - COO-style sparse matrix storage\n        - Automatic metadata type inference\n        - Lance format for high-performance storage\n        - Chunked processing by default for memory efficiency\n\n    Examples:\n        &gt;&gt;&gt; # Basic conversion from h5ad file (chunked is now the default)\n        &gt;&gt;&gt; converter = SLAFConverter()\n        &gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output.slaf\")\n        Converting data.h5ad to SLAF format...\n        Optimizations: int_keys=True\n        Loaded: 1000 cells \u00d7 20000 genes\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Conversion with custom optimization settings\n        &gt;&gt;&gt; converter = SLAFConverter(use_integer_keys=False)\n        &gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output_string_keys.slaf\")\n        Converting data.h5ad to SLAF format...\n        Optimizations: int_keys=False\n        Loaded: 1000 cells \u00d7 20000 genes\n        Conversion complete! Saved to output_string_keys.slaf\n\n        &gt;&gt;&gt; # Convert existing AnnData object (chunked is now the default)\n        &gt;&gt;&gt; import scanpy as sc\n        &gt;&gt;&gt; adata = sc.read_h5ad(\"data.h5ad\")\n        &gt;&gt;&gt; converter = SLAFConverter()\n        &gt;&gt;&gt; converter.convert_anndata(adata, \"output_from_object.slaf\")\n        Converting AnnData object to SLAF format...\n        Optimizations: int_keys=True\n        Loaded: 1000 cells \u00d7 20000 genes\n        Conversion complete! Saved to output_from_object.slaf\n    \"\"\"\n\n    def __init__(\n        self,\n        use_integer_keys: bool = True,\n        chunked: bool = True,  # Changed from False to True - make chunked the default\n        chunk_size: int = 5000,  # Smaller chunks to avoid memory alignment issues in Lance v2.1\n        sort_metadata: bool = False,\n        create_indices: bool = False,  # Disable indices by default for small datasets\n        optimize_storage: bool = True,  # Only store integer IDs in expression table\n        use_optimized_dtypes: bool = True,  # Use uint16/uint32 for better compression\n        enable_v2_manifest: bool = True,  # Enable v2 manifest paths for better performance\n        compact_after_write: bool = False,  # Compact dataset after writing for optimal storage (disabled by default to avoid manifest corruption)\n        tiledb_collection_name: str = \"RNA\",  # Collection name for TileDB format\n        enable_checkpointing: bool = True,  # Enable checkpointing for long-running conversions\n        max_rows_per_file: int = 100_000_000,  # Max rows per Lance fragment (default: 100M to stay under HuggingFace's 10k file limit)\n    ):\n        \"\"\"\n        Initialize converter with optimization options.\n\n        Args:\n            use_integer_keys: Use integer keys instead of strings in sparse data.\n                             This saves significant memory and improves query performance.\n                             Set to False only if you need to preserve original string IDs.\n            chunked: Use chunked processing for memory efficiency (default: True).\n                    Chunked processing is now the default for optimal memory efficiency.\n                    Set to False only for small datasets or debugging purposes.\n            chunk_size: Size of each chunk when chunked=True (default: 5000).\n            create_indices: Whether to create indices for query performance.\n                          Default: False for small datasets to reduce storage overhead.\n                          Set to True for large datasets where query performance is important.\n            optimize_storage: Only store integer IDs in expression table to reduce storage size.\n                           String IDs are available in metadata tables for mapping.\n            use_optimized_dtypes: Use optimized data types (uint16/uint32) for better compression.\n                                This can significantly reduce storage size for large datasets.\n            enable_v2_manifest: Enable v2 manifest paths for better query performance.\n                              This is recommended for large datasets.\n            compact_after_write: Compact the dataset after writing to optimize storage.\n                               This creates a new version but significantly reduces file size.\n            tiledb_collection_name: Name of the measurement collection for TileDB format.\n                                  Default: \"RNA\". Only used when converting from TileDB format.\n            enable_checkpointing: Enable checkpointing for long-running conversions.\n                                This allows resuming from the last completed chunk if the\n                                conversion is interrupted. Default: True.\n            max_rows_per_file: Maximum number of rows per Lance fragment file.\n                             Larger values create fewer fragments but larger files.\n                             Default: 100M to stay under HuggingFace's 10k file limit.\n                             For very large datasets, increase this value to reduce fragment count.\n\n        Examples:\n            &gt;&gt;&gt; # Default optimization (recommended)\n            &gt;&gt;&gt; converter = SLAFConverter()\n            &gt;&gt;&gt; print(f\"Using chunked processing: {converter.chunked}\")\n            Using chunked processing: True\n\n            &gt;&gt;&gt; # Non-chunked processing for small datasets\n            &gt;&gt;&gt; converter = SLAFConverter(chunked=False)\n            &gt;&gt;&gt; print(f\"Using chunked processing: {converter.chunked}\")\n            Using chunked processing: False\n\n            &gt;&gt;&gt; # Custom chunk size for large datasets\n            &gt;&gt;&gt; converter = SLAFConverter(chunk_size=10000)\n            &gt;&gt;&gt; print(f\"Chunk size: {converter.chunk_size}\")\n            Chunk size: 5000\n        \"\"\"\n        self.use_integer_keys = use_integer_keys\n        self.chunked = chunked\n        self.chunk_size = chunk_size\n        self.sort_metadata = sort_metadata\n        self.create_indices = create_indices\n        self.optimize_storage = optimize_storage\n        self.use_optimized_dtypes = use_optimized_dtypes\n        self.enable_v2_manifest = enable_v2_manifest\n        self.compact_after_write = compact_after_write\n        self.tiledb_collection_name = tiledb_collection_name\n        self.enable_checkpointing = enable_checkpointing\n        self.max_rows_per_file = max_rows_per_file\n\n    def _is_cloud_path(self, path: str) -&gt; bool:\n        \"\"\"Check if path is a cloud storage path.\"\"\"\n        return path.startswith((\"s3://\", \"gs://\", \"azure://\", \"r2://\"))\n\n    def _path_exists(self, path: str) -&gt; bool:\n        \"\"\"Check if path exists, works with both local and cloud paths.\"\"\"\n        if self._is_cloud_path(path):\n            if not SMART_OPEN_AVAILABLE:\n                logger.warning(\n                    \"smart-open not available, cannot check cloud path existence\"\n                )\n                return False\n            try:\n                # For Lance datasets, check if the directory exists by trying to list contents\n                # Lance datasets are directories, not files\n                if path.endswith(\".lance\"):\n                    # Try to access the Lance dataset directory\n                    import lance\n\n                    lance.dataset(path)  # This will fail if the dataset doesn't exist\n                    return True\n                else:\n                    # For regular files, try to access the file directly\n                    with smart_open(path, \"r\") as f:\n                        f.read(1)  # Try to read 1 byte\n                    return True\n            except Exception:\n                return False\n        else:\n            return os.path.exists(path)\n\n    def _ensure_directory_exists(self, path: str):\n        \"\"\"Ensure directory exists, works with both local and cloud paths.\"\"\"\n        if not self._is_cloud_path(path):\n            os.makedirs(path, exist_ok=True)\n        # For cloud paths, directories are created implicitly\n\n    def _open_file(self, path: str, mode: str = \"r\"):\n        \"\"\"Open file with cloud storage compatibility.\"\"\"\n        if self._is_cloud_path(path):\n            if not SMART_OPEN_AVAILABLE:\n                raise ImportError(\"smart-open required for cloud storage operations\")\n            return smart_open(path, mode)\n        else:\n            return open(path, mode)\n\n    def _save_checkpoint(self, output_path: str, checkpoint_data: dict):\n        \"\"\"Save checkpoint data to config.json\"\"\"\n        if not self.enable_checkpointing:\n            return\n\n        config_path = f\"{output_path}/config.json\"\n\n        # Load existing config if it exists, otherwise create minimal config\n        if self._path_exists(config_path):\n            with self._open_file(config_path) as f:\n                config = json.load(f)\n        else:\n            # Create minimal config for checkpointing\n            config = {\n                \"format_version\": \"0.3\",\n                \"array_shape\": [0, 0],  # Will be updated when conversion completes\n                \"n_cells\": 0,\n                \"n_genes\": 0,\n                \"tables\": {\n                    \"expression\": \"expression.lance\",\n                    \"cells\": \"cells.lance\",\n                    \"genes\": \"genes.lance\",\n                },\n                \"optimizations\": {\n                    \"use_integer_keys\": self.use_integer_keys,\n                    \"optimize_storage\": self.optimize_storage,\n                },\n                \"metadata\": {\n                    \"conversion_in_progress\": True,\n                    \"sparsity\": 0.0,\n                    \"density\": 0.0,\n                    \"expression_count\": 0,\n                },\n            }\n\n        # Update checkpoint data\n        config[\"checkpoint\"] = checkpoint_data\n\n        # Save updated config\n        with self._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n        logger.info(f\"Checkpoint saved: {checkpoint_data}\")\n\n    def _load_checkpoint(self, output_path: str) -&gt; dict | None:\n        \"\"\"Load checkpoint data from config.json\"\"\"\n        if not self.enable_checkpointing:\n            return None\n\n        config_path = f\"{output_path}/config.json\"\n\n        if not self._path_exists(config_path):\n            return None\n\n        try:\n            with self._open_file(config_path) as f:\n                config = json.load(f)\n                return config.get(\"checkpoint\")\n        except Exception as e:\n            logger.warning(f\"Could not load checkpoint: {e}\")\n            return None\n\n    def _load_checkpoint_smart(self, output_path: str) -&gt; dict | None:\n        \"\"\"Load checkpoint with fragment-based resume logic to avoid duplication\"\"\"\n        checkpoint = self._load_checkpoint(output_path)\n        if not checkpoint:\n            return None\n\n        # Get actual progress from Lance dataset using fragment count\n        expression_path = f\"{output_path}/expression.lance\"\n        logger.info(f\"\ud83d\udd0d Checking fragment-based resume for: {expression_path}\")\n\n        # First, check if the path exists\n        path_exists = self._path_exists(expression_path)\n        logger.info(f\"\ud83d\udd0d Path exists check result: {path_exists}\")\n\n        if path_exists:\n            logger.info(\"\u2705 Expression dataset exists, loading...\")\n            try:\n                expression_dataset = lance.dataset(expression_path)\n                fragment_count = len(expression_dataset.get_fragments())\n                logger.info(f\"\ud83d\udd0d Found {fragment_count} fragments in dataset\")\n                logger.info(\n                    f\"\ud83d\udd0d Original checkpoint: last_completed_chunk={checkpoint.get('last_completed_chunk')}\"\n                )\n\n                # Calculate exact resume chunk using fragment count\n                last_checkpoint_boundary = checkpoint.get(\"last_completed_chunk\", 0)\n                checkpoint_interval = (\n                    10  # Should match the interval used in checkpointing\n                )\n                chunks_since_checkpoint = fragment_count % checkpoint_interval\n\n                # Resume from the next chunk after what was actually written\n                # The fragment count represents the actual progress, so we don't need total_chunks\n                # Just ensure we don't exceed the total chunks for the current file\n                actual_resume_chunk = (\n                    last_checkpoint_boundary + chunks_since_checkpoint + 1\n                )\n\n                # Update checkpoint with actual resume position\n                checkpoint[\"last_completed_chunk\"] = actual_resume_chunk - 1\n                checkpoint[\"fragment_count\"] = fragment_count\n                checkpoint[\"chunks_since_checkpoint\"] = chunks_since_checkpoint\n\n                logger.info(\n                    f\"Fragment-based resume: {fragment_count} fragments written, \"\n                    f\"chunks since checkpoint: {chunks_since_checkpoint}, \"\n                    f\"resuming from chunk {actual_resume_chunk}\"\n                )\n                logger.info(\n                    f\"\ud83d\udd27 Updated checkpoint: last_completed_chunk={checkpoint['last_completed_chunk']}\"\n                )\n\n            except Exception as e:\n                logger.warning(f\"Could not determine fragment count: {e}\")\n                # Fall back to regular checkpoint logic\n                pass\n\n        return checkpoint\n\n    def _clear_checkpoint(self, output_path: str):\n        \"\"\"Clear checkpoint data from config.json\"\"\"\n        if not self.enable_checkpointing:\n            return\n\n        config_path = f\"{output_path}/config.json\"\n\n        if not self._path_exists(config_path):\n            return\n\n        try:\n            with self._open_file(config_path) as f:\n                config = json.load(f)\n\n            # Remove checkpoint data\n            if \"checkpoint\" in config:\n                del config[\"checkpoint\"]\n\n            # Save updated config\n            with self._open_file(config_path, \"w\") as f:\n                json.dump(config, f, indent=2)\n\n            logger.info(\"Checkpoint cleared - conversion completed successfully\")\n        except Exception as e:\n            logger.warning(f\"Could not clear checkpoint: {e}\")\n\n    def convert(\n        self,\n        input_path: str | list[str],\n        output_path: str,\n        input_format: str = \"auto\",\n        skip_validation: bool = False,\n    ):\n        \"\"\"\n        Convert single-cell data to SLAF format with optimized storage.\n\n        SLAFConverter provides efficient conversion from various single-cell data formats\n        to the SLAF format. It optimizes storage by using integer keys, COO-style\n        expression tables, and efficient metadata handling.\n\n        Supported Input Formats:\n            - **h5ad**: AnnData files (.h5ad) - the standard single-cell format\n            - **10x MTX**: 10x Genomics MTX directories containing matrix.mtx,\n              barcodes.tsv, and genes.tsv files\n            - **10x H5**: 10x Genomics H5 files (.h5) - Cell Ranger output format\n            - **tiledb**: TileDB SOMA format (.tiledb) - high-performance single-cell format\n\n        The converter automatically detects the input format based on file extension\n        and directory structure. For optimal performance, you can also specify the\n        format explicitly.\n\n        Args:\n            input_path: Path to the input file or directory to convert.\n                       - For h5ad: path to .h5ad file\n                       - For MTX: path to directory containing matrix.mtx, barcodes.tsv, genes.tsv\n                       - For H5: path to .h5 file\n            output_path: Path where the SLAF dataset will be saved.\n                        Should be a directory path, not a file path.\n            input_format: Format of input data. Options:\n                         - \"auto\" (default): Auto-detect format\n                         - \"h5ad\": AnnData format\n                         - \"10x_mtx\": 10x MTX directory format\n                         - \"10x_h5\": 10x H5 file format\n                         - \"tiledb\": TileDB SOMA format\n\n        Raises:\n            FileNotFoundError: If the input file doesn't exist.\n            ValueError: If the input file is corrupted, invalid, or format cannot be detected.\n            RuntimeError: If the conversion process fails.\n\n        Examples:\n            &gt;&gt;&gt; # Auto-detect format (recommended)\n            &gt;&gt;&gt; converter = SLAFConverter()\n            &gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output.slaf\")\n            Converting data.h5ad to SLAF format...\n            Optimizations: int_keys=True\n            Loaded: 1000 cells \u00d7 20000 genes\n            Conversion complete! Saved to output.slaf\n\n            &gt;&gt;&gt; # Convert 10x MTX directory\n            &gt;&gt;&gt; converter.convert(\"filtered_feature_bc_matrix/\", \"output.slaf\")\n            Converting 10x MTX directory filtered_feature_bc_matrix/ to SLAF format...\n            Loaded: 2700 cells \u00d7 32738 genes\n            Conversion complete! Saved to output.slaf\n\n            &gt;&gt;&gt; # Convert 10x H5 file\n            &gt;&gt;&gt; converter.convert(\"data.h5\", \"output.slaf\")\n            Converting 10x H5 file data.h5 to SLAF format...\n            Loaded: 2700 cells \u00d7 32738 genes\n            Conversion complete! Saved to output.slaf\n\n            &gt;&gt;&gt; # Convert TileDB SOMA file\n            &gt;&gt;&gt; converter.convert(\"data.tiledb\", \"output.slaf\")\n            Converting TileDB SOMA file data.tiledb to SLAF format...\n            Loaded: 50000 cells \u00d7 20000 genes\n            Conversion complete! Saved to output.slaf\n\n            &gt;&gt;&gt; # Explicit format specification\n            &gt;&gt;&gt; converter.convert(\"data.h5\", \"output.slaf\", input_format=\"10x_h5\")\n            Converting 10x H5 file data.h5 to SLAF format...\n            Loaded: 2700 cells \u00d7 32738 genes\n            Conversion complete! Saved to output.slaf\n\n            &gt;&gt;&gt; # Convert with chunked processing for large datasets\n            &gt;&gt;&gt; converter = SLAFConverter(chunked=True, chunk_size=5000)\n            &gt;&gt;&gt; converter.convert(\"large_data.h5ad\", \"output.slaf\")\n            Converting large_data.h5ad to SLAF format...\n            Optimizations: int_keys=True, chunked=True\n            Processing in chunks of 5000 cells...\n            Conversion complete! Saved to output.slaf\n\n            &gt;&gt;&gt; # Error handling for unsupported format\n            &gt;&gt;&gt; try:\n            ...     converter.convert(\"unknown_file.txt\", \"output.slaf\")\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Cannot detect format for: unknown_file.txt\n        \"\"\"\n        # Handle both single paths and lists of files\n        if isinstance(input_path, list):\n            # Direct list of files - use multi-file conversion\n            input_files = input_path\n            # Detect format from first file\n            if input_format == \"auto\":\n                from .utils import detect_format\n\n                input_format = detect_format(input_files[0])\n        else:\n            # Single path - discover files\n            input_files, detected_format = discover_input_files(input_path)\n            # Use detected format if auto, otherwise use specified format\n            if input_format == \"auto\":\n                input_format = detected_format\n\n        # Validate multi-file compatibility if multiple files (unless skipped)\n        if not skip_validation:\n            validate_input_files(input_files, input_format)\n\n        # Handle multiple files vs single file\n        if len(input_files) &gt; 1:\n            self._convert_multiple_files(input_files, output_path, input_format)\n        else:\n            # Single file - use existing logic\n            single_file = input_files[0]\n            if input_format == \"h5ad\":\n                if not SCANPY_AVAILABLE:\n                    raise ImportError(\n                        \"Scanpy is required for h5ad conversion. \"\n                        \"Install with: pip install scanpy\"\n                    )\n                self._convert_h5ad(single_file, output_path)\n            elif input_format == \"10x_mtx\":\n                self._convert_10x_mtx(single_file, output_path)\n            elif input_format == \"10x_h5\":\n                self._convert_10x_h5(single_file, output_path)\n            elif input_format == \"tiledb\":\n                self._convert_tiledb(single_file, output_path)\n            else:\n                raise ValueError(f\"Unsupported format: {input_format}\")\n\n    def convert_anndata(self, adata, output_path: str):\n        \"\"\"Convert AnnData object to SLAF format with COO-style expression table\"\"\"\n        if self.chunked:\n            raise ValueError(\n                \"convert_anndata() not supported in chunked mode. \"\n                \"Use convert() with file path instead.\"\n            )\n\n        logger.info(\"Converting AnnData object to SLAF format...\")\n        logger.info(f\"Optimizations: int_keys={self.use_integer_keys}\")\n        logger.info(f\"Loaded: {adata.n_obs} cells \u00d7 {adata.n_vars} genes\")\n\n        # Validate optimized data types\n        if not self._validate_optimized_dtypes_anndata(adata):\n            self.use_optimized_dtypes = False\n\n        # Convert the AnnData object\n        self._convert_anndata(adata, output_path)\n\n    def _convert_h5ad(self, h5ad_path: str, output_path: str):\n        \"\"\"Convert h5ad file to SLAF format (existing logic)\"\"\"\n        logger.info(f\"Converting {h5ad_path} to SLAF format...\")\n        logger.info(\n            f\"Optimizations: int_keys={self.use_integer_keys}, chunked={self.chunked}, sort_metadata={self.sort_metadata}\"\n        )\n\n        if self.chunked:\n            self._convert_chunked(h5ad_path, output_path)\n        else:\n            # Load h5ad using scanpy backed mode, then convert to in-memory\n            logger.info(\"Loading h5ad file in backed mode...\")\n            adata_backed = sc.read_h5ad(h5ad_path, backed=\"r\")\n            logger.info(\n                f\"Loaded: {adata_backed.n_obs} cells \u00d7 {adata_backed.n_vars} genes\"\n            )\n\n            # Convert backed data to in-memory AnnData to avoid CSRDataset issues\n            logger.info(\"Converting backed data to in-memory format...\")\n            adata = sc.AnnData(\n                X=adata_backed.X[:],  # Load the full matrix into memory\n                obs=adata_backed.obs.copy(),\n                var=adata_backed.var.copy(),\n                uns=adata_backed.uns.copy() if hasattr(adata_backed, \"uns\") else {},\n            )\n\n            # Close the backed file\n            adata_backed.file.close()\n\n            logger.info(\"Successfully converted to in-memory AnnData\")\n\n            # Convert the loaded AnnData object\n            self._convert_anndata(adata, output_path)\n\n    def _convert_10x_mtx(self, mtx_dir: str, output_path: str):\n        \"\"\"Convert 10x MTX directory to SLAF format\"\"\"\n        logger.info(f\"Converting 10x MTX directory {mtx_dir} to SLAF format...\")\n\n        if self.chunked:\n            # Use native chunked reader for 10x MTX\n            logger.info(\"Using native chunked reader for 10x MTX...\")\n            self._convert_chunked(mtx_dir, output_path)\n        else:\n            # Use scanpy to read MTX files\n            try:\n                adata = sc.read_10x_mtx(mtx_dir)\n            except Exception as e:\n                logger.error(f\"Error reading 10x MTX files: {e}\")\n                logger.error(\n                    \"Please ensure the directory contains matrix.mtx and either genes.tsv or features.tsv files\"\n                )\n                raise ValueError(\n                    f\"Failed to read 10x MTX format from {mtx_dir}: {e}\"\n                ) from e\n\n            logger.info(f\"Loaded: {adata.n_obs} cells \u00d7 {adata.n_vars} genes\")\n\n            # Convert using existing AnnData conversion logic\n            self._convert_anndata(adata, output_path)\n\n    def _convert_10x_h5(self, h5_path: str, output_path: str):\n        \"\"\"Convert 10x H5 file to SLAF format\"\"\"\n        logger.info(f\"Converting 10x H5 file {h5_path} to SLAF format...\")\n\n        if self.chunked:\n            # Use native chunked reader for 10x H5\n            logger.info(\"Using native chunked reader for 10x H5...\")\n            self._convert_chunked(h5_path, output_path)\n        else:\n            # Try to read as 10x H5 first, fall back to regular h5ad\n            try:\n                adata_backed = sc.read_10x_h5(h5_path, genome=\"X\")\n                logger.info(\"Successfully read as 10x H5 format\")\n            except Exception:\n                # Fall back to reading as regular h5ad\n                logger.info(\"Reading as regular h5ad file...\")\n                adata_backed = sc.read_h5ad(h5_path, backed=\"r\")\n\n            logger.info(\n                f\"Loaded: {adata_backed.n_obs} cells \u00d7 {adata_backed.n_vars} genes\"\n            )\n\n            # Convert backed data to in-memory AnnData to avoid CSRDataset issues\n            logger.info(\"Converting backed data to in-memory format...\")\n            adata = sc.AnnData(\n                X=adata_backed.X[:],  # Load the full matrix into memory\n                obs=adata_backed.obs.copy(),\n                var=adata_backed.var.copy(),\n                uns=adata_backed.uns.copy() if hasattr(adata_backed, \"uns\") else {},\n            )\n\n            # Close the backed file if it exists\n            if hasattr(adata_backed, \"file\") and adata_backed.file is not None:\n                adata_backed.file.close()\n\n            logger.info(\"Successfully converted to in-memory AnnData\")\n\n            # Convert using existing AnnData conversion logic\n            self._convert_anndata(adata, output_path)\n\n    def _convert_tiledb(self, tiledb_path: str, output_path: str):\n        \"\"\"Convert TileDB SOMA file to SLAF format\"\"\"\n        logger.info(f\"Converting TileDB SOMA file {tiledb_path} to SLAF format...\")\n\n        if self.chunked:\n            # Use native chunked reader for TileDB\n            logger.info(\"Using native chunked reader for TileDB...\")\n            self._convert_chunked(tiledb_path, output_path)\n        else:\n            # For non-chunked mode, we need to load the entire dataset\n            # This is not recommended for large datasets\n            logger.warning(\"Non-chunked mode not recommended for TileDB format\")\n            raise NotImplementedError(\n                \"Non-chunked conversion not supported for TileDB format. \"\n                \"Use chunked=True (default) for TileDB conversion.\"\n            )\n\n    def _convert_multiple_files(\n        self, input_files: list[str], output_path: str, input_format: str\n    ):\n        \"\"\"\n        Convert multiple files to a single SLAF dataset with auto-incrementing IDs and checkpointing.\n\n        This method processes files sequentially and adds fragments to the same\n        SLAF dataset, avoiding the overhead of creating separate SLAF files.\n\n        Args:\n            input_files: List of input file paths\n            output_path: Output SLAF directory path\n            input_format: Format of input files\n        \"\"\"\n        logger.info(\n            f\"Converting {len(input_files)} {input_format} files to SLAF format...\"\n        )\n\n        # Check for existing checkpoint with smart resume logic\n        checkpoint = self._load_checkpoint_smart(output_path)\n        if checkpoint:\n            logger.info(f\"Found checkpoint: {checkpoint}\")\n            if checkpoint.get(\"status\") == \"completed\":\n                logger.info(\n                    \"Multi-file conversion already completed according to checkpoint\"\n                )\n                return\n            elif checkpoint.get(\"status\") == \"in_progress\":\n                logger.info(\"Resuming multi-file conversion from checkpoint...\")\n            else:\n                logger.info(\"Starting fresh multi-file conversion...\")\n\n        # Create output directory (only for local paths)\n        self._ensure_directory_exists(output_path)\n\n        # Check layer consistency across all files\n        layer_names = self._check_layer_consistency(input_files, input_format)\n        if layer_names is None:\n            logger.warning(\n                \"Layers are inconsistent across files. Skipping layers.lance creation.\"\n            )\n            layer_names = []\n\n        # Track source file information\n        source_file_info = []\n        global_cell_offset = 0\n        total_genes = 0\n        total_cells = 0\n\n        # Determine starting file and chunk from checkpoint\n        start_file_idx = 0\n        start_chunk_idx = 0\n        if checkpoint and checkpoint.get(\"status\") == \"in_progress\":\n            # Handle both file-level and chunk-level checkpoints\n            last_completed_chunk = checkpoint.get(\"last_completed_chunk\", -1)\n            if last_completed_chunk == -1:\n                # File-level checkpoint: file was fully completed, start next file\n                start_file_idx = checkpoint.get(\"last_completed_file\", -1) + 1\n                start_chunk_idx = 0\n            else:\n                # Chunk-level checkpoint: file is partially completed, resume same file\n                start_file_idx = checkpoint.get(\"last_completed_file\", -1)\n                start_chunk_idx = last_completed_chunk + 1\n            global_cell_offset = checkpoint.get(\"global_cell_offset\", 0)\n\n            # Enhanced logging for resume tracking\n            logger.info(\"=\" * 60)\n            logger.info(\"\ud83d\udd04 CHECKPOINT RESUME DETECTED\")\n            logger.info(\"=\" * 60)\n            logger.info(\n                f\"\ud83d\udcc1 Resuming from FILE {start_file_idx + 1} of {len(input_files)}\"\n            )\n            logger.info(f\"\ud83d\udcca Resuming from CHUNK {start_chunk_idx} within that file\")\n            logger.info(\n                f\"\ud83d\udcc8 Last completed file: {checkpoint.get('last_completed_file', -1) + 1}\"\n            )\n            logger.info(\n                f\"\ud83d\udcc8 Last completed chunk: {checkpoint.get('last_completed_chunk', -1)}\"\n            )\n            logger.info(f\"\ud83d\udd22 Global cell offset: {global_cell_offset:,}\")\n            if \"fragment_count\" in checkpoint:\n                logger.info(\n                    f\"\ud83e\udde9 Fragment count: {checkpoint.get('fragment_count', 0):,}\"\n                )\n            if \"chunks_since_checkpoint\" in checkpoint:\n                logger.info(\n                    f\"\u23ed\ufe0f  Chunks since checkpoint: {checkpoint.get('chunks_since_checkpoint', 0)}\"\n                )\n            logger.info(\"=\" * 60)\n\n        # Process each file and add fragments to the same SLAF dataset\n        for i, file_path in enumerate(input_files):\n            # Skip files if resuming from checkpoint\n            if i &lt; start_file_idx:\n                logger.info(\n                    f\"\u23ed\ufe0f  Skipping file {i + 1}/{len(input_files)}: {os.path.basename(file_path)} (already processed)\"\n                )\n                continue\n\n            # Enhanced file processing logging\n            if i == start_file_idx:\n                logger.info(\"=\" * 60)\n                logger.info(\n                    f\"\ud83d\udd04 RESUMING FILE {i + 1}/{len(input_files)}: {os.path.basename(file_path)}\"\n                )\n                logger.info(\n                    f\"\ud83d\udcca Starting from chunk {start_chunk_idx} within this file\"\n                )\n                logger.info(\"=\" * 60)\n            else:\n                logger.info(\n                    f\"\ud83d\udcc1 Processing file {i + 1}/{len(input_files)}: {os.path.basename(file_path)}\"\n                )\n\n            try:\n                # Use chunked reader to process file directly\n                with create_chunked_reader(\n                    file_path,\n                    chunk_size=self.chunk_size,\n                    collection_name=self.tiledb_collection_name,\n                ) as reader:\n                    logger.info(\n                        f\"Loaded: {reader.n_obs:,} cells \u00d7 {reader.n_vars:,} genes\"\n                    )\n\n                    # Get metadata from reader\n                    obs_df = reader.get_obs_metadata()\n                    var_df = reader.get_var_metadata()\n\n                    # Ensure cell_id and gene_id columns exist\n                    if \"cell_id\" not in obs_df.columns:\n                        obs_df[\"cell_id\"] = reader.obs_names\n                    if \"gene_id\" not in var_df.columns:\n                        var_df[\"gene_id\"] = reader.var_names\n\n                    # Add integer IDs with global offset\n                    obs_df[\"cell_integer_id\"] = range(\n                        global_cell_offset, global_cell_offset + len(obs_df)\n                    )\n                    var_df[\"gene_integer_id\"] = range(len(var_df))\n\n                    # Add source file information to cell metadata\n                    source_file = os.path.basename(file_path)\n                    obs_df[\"source_file\"] = source_file\n\n                    # Precompute cell start indices\n                    obs_df[\"cell_start_index\"] = self._compute_cell_start_indices(\n                        reader, obs_df\n                    )\n\n                    # Convert metadata to Lance tables\n                    cell_metadata_table = self._create_metadata_table(\n                        obs_df, \"cell_id\", integer_mapping=None\n                    )\n                    gene_metadata_table = self._create_metadata_table(\n                        var_df, \"gene_id\", integer_mapping=None\n                    )\n\n                    # Process expression data in chunks and write directly\n                    cells_path = f\"{output_path}/cells.lance\"\n                    genes_path = f\"{output_path}/genes.lance\"\n\n                    # Write metadata tables (overwrite for first file, append for subsequent)\n                    # Skip metadata writing for files that were already completed\n                    # Only write metadata for files that haven't been started yet, or for the current file if starting from beginning\n                    should_write_metadata = (\n                        i &gt; start_file_idx  # Future files - always write metadata\n                        or (\n                            i == start_file_idx and start_chunk_idx == 0\n                        )  # Current file only if starting from beginning\n                    )\n\n                    if i == 0:\n                        # First file - create new datasets\n                        lance.write_dataset(\n                            cell_metadata_table,\n                            cells_path,\n                            mode=\"overwrite\",\n                            enable_v2_manifest_paths=self.enable_v2_manifest,\n                            data_storage_version=\"2.1\",\n                        )\n                        lance.write_dataset(\n                            gene_metadata_table,\n                            genes_path,\n                            mode=\"overwrite\",\n                            enable_v2_manifest_paths=self.enable_v2_manifest,\n                            data_storage_version=\"2.1\",\n                        )\n                        total_genes = len(var_df)\n                    elif should_write_metadata:\n                        # Subsequent files - append to existing datasets (only if not resuming from middle)\n                        lance.write_dataset(\n                            cell_metadata_table,\n                            cells_path,\n                            mode=\"append\",\n                            enable_v2_manifest_paths=self.enable_v2_manifest,\n                            data_storage_version=\"2.1\",\n                        )\n\n                    # Process expression data in chunks with checkpointing support\n                    # If we're resuming from this file, use the checkpoint chunk index\n                    # Otherwise start from chunk 0\n                    chunk_start_idx = start_chunk_idx if i == start_file_idx else 0\n                    self._process_file_chunks_with_checkpoint(\n                        reader, output_path, i, chunk_start_idx, global_cell_offset\n                    )\n\n                    # Process layers if they exist and are consistent\n                    if layer_names:\n                        logger.info(\n                            f\"Processing {len(layer_names)} consistent layers for file {i + 1}/{len(input_files)}...\"\n                        )\n                        # Determine value type from layer data (assume float32 for layers)\n                        # Update reader's value_type to match (needed for _read_chunk_layers_wide)\n                        original_value_type = reader.value_type\n                        reader.value_type = \"float32\"\n                        try:\n                            # For multi-file, only create empty table on first file (i == 0)\n                            # For subsequent files, skip empty table creation and append directly\n                            layers_path = f\"{output_path}/layers.lance\"\n                            is_first_file = i == 0 and not self._path_exists(\n                                layers_path\n                            )\n                            self._process_expression_with_checkpoint(\n                                reader,\n                                output_path,\n                                value_type=\"float32\",\n                                checkpoint=None,  # Layers don't use checkpointing\n                                layers=True,\n                                table_name=\"layers\",\n                                cell_offset=global_cell_offset,\n                                skip_empty_table=not is_first_file,  # Skip if not first file\n                            )\n                        finally:\n                            # Restore original value_type\n                            reader.value_type = original_value_type\n\n                    # Track source file information\n                    source_file_info.append(\n                        {\n                            \"file_path\": file_path,\n                            \"file_name\": source_file,\n                            \"n_cells\": len(obs_df),\n                            \"cell_offset\": global_cell_offset,\n                        }\n                    )\n\n                    # Update global cell offset\n                    global_cell_offset += len(obs_df)\n                    total_cells += len(obs_df)\n\n                    # Save checkpoint after each file (chunk-level checkpointing is handled in _process_file_chunks_with_checkpoint)\n                    if self.enable_checkpointing:\n                        checkpoint_data = {\n                            \"status\": \"in_progress\",\n                            \"last_completed_file\": i,\n                            \"last_completed_chunk\": -1,  # -1 indicates file is fully completed\n                            \"global_cell_offset\": global_cell_offset,\n                            \"total_files\": len(input_files),\n                            \"timestamp\": pd.Timestamp.now().isoformat(),\n                        }\n                        self._save_checkpoint(output_path, checkpoint_data)\n\n            except Exception as e:\n                logger.error(f\"Failed to process file {file_path}: {e}\")\n                # Save checkpoint with error status\n                if self.enable_checkpointing:\n                    checkpoint_data = {\n                        \"status\": \"error\",\n                        \"last_completed_file\": i - 1,\n                        \"last_completed_chunk\": -1,  # -1 indicates previous file was fully completed\n                        \"global_cell_offset\": global_cell_offset,\n                        \"error\": str(e),\n                        \"timestamp\": pd.Timestamp.now().isoformat(),\n                    }\n                    self._save_checkpoint(output_path, checkpoint_data)\n                # Continue with other files\n                continue\n\n        if not source_file_info:\n            raise RuntimeError(\"No files were successfully processed\")\n\n        # Create indices if enabled\n        if self.create_indices:\n            self._create_indices(output_path)\n\n        # Compact dataset if enabled\n        if self.compact_after_write:\n            self._compact_dataset(output_path)\n\n        # Save config with multi-file information\n        self._save_multi_file_config(\n            output_path,\n            source_file_info,\n            None,  # We don't need to pass tables since they're already written\n            None,\n            None,\n            layer_names=layer_names,  # Pass layer names if consistent\n        )\n\n        # Clear checkpoint after successful completion\n        self._clear_checkpoint(output_path)\n\n        logger.info(\n            f\"Multi-file conversion complete! Processed {len(source_file_info)} files\"\n        )\n        logger.info(f\"Total cells: {total_cells}, Total genes: {total_genes}\")\n\n    def append(\n        self, input_path: str, existing_slaf_path: str, input_format: str = \"auto\"\n    ):\n        \"\"\"\n        Append new data to an existing SLAF dataset.\n\n        This method adds new data to an existing SLAF dataset by:\n        1. Validating compatibility with existing dataset\n        2. Reading existing dataset metadata\n        3. Appending new data with auto-incrementing IDs\n        4. Updating configuration with new source file info\n\n        Args:\n            input_path: Path to new input file or directory\n            existing_slaf_path: Path to existing SLAF dataset\n            input_format: Format of input data (auto-detected if not specified)\n        \"\"\"\n        logger.info(\n            f\"Appending data from {input_path} to existing SLAF dataset {existing_slaf_path}\"\n        )\n\n        # Check if existing SLAF dataset exists\n        if not self._path_exists(existing_slaf_path):\n            raise FileNotFoundError(\n                f\"Existing SLAF dataset not found: {existing_slaf_path}\"\n            )\n\n        # Discover input files\n        input_files, detected_format = discover_input_files(input_path)\n        if input_format == \"auto\":\n            input_format = detected_format\n\n        # Validate compatibility with existing dataset\n        self._validate_append_compatibility(\n            input_files, input_format, existing_slaf_path\n        )\n\n        # Read existing dataset metadata\n        existing_cells_dataset = lance.dataset(\n            os.path.join(existing_slaf_path, \"cells.lance\")\n        )\n        existing_cells_table = existing_cells_dataset.to_table()\n        current_cell_count = len(existing_cells_table)\n\n        # Track source file information\n        source_file_info = []\n        global_cell_offset = current_cell_count\n        total_new_cells = 0\n\n        # Process each new file and append to existing dataset\n        for i, file_path in enumerate(input_files):\n            logger.info(\n                f\"Processing file {i + 1}/{len(input_files)}: {os.path.basename(file_path)}\"\n            )\n\n            try:\n                # Use chunked reader to process file directly\n                with create_chunked_reader(\n                    file_path,\n                    chunk_size=self.chunk_size,\n                    collection_name=self.tiledb_collection_name,\n                ) as reader:\n                    logger.info(\n                        f\"Loaded: {reader.n_obs:,} cells \u00d7 {reader.n_vars:,} genes\"\n                    )\n\n                    # Get metadata from reader\n                    obs_df = reader.get_obs_metadata()\n                    var_df = reader.get_var_metadata()\n\n                    # Ensure cell_id and gene_id columns exist\n                    if \"cell_id\" not in obs_df.columns:\n                        obs_df[\"cell_id\"] = reader.obs_names\n                    if \"gene_id\" not in var_df.columns:\n                        var_df[\"gene_id\"] = reader.var_names\n\n                    # Add integer IDs with global offset\n                    obs_df[\"cell_integer_id\"] = range(\n                        global_cell_offset, global_cell_offset + len(obs_df)\n                    )\n\n                    # Add source file information to cell metadata\n                    source_file = os.path.basename(file_path)\n                    obs_df[\"source_file\"] = source_file\n\n                    # Check if existing dataset has source_file column\n                    existing_cells_dataset = lance.dataset(\n                        os.path.join(existing_slaf_path, \"cells.lance\")\n                    )\n                    existing_cells_table = existing_cells_dataset.to_table()\n                    existing_columns = set(existing_cells_table.column_names)\n\n                    # If existing dataset doesn't have source_file column, add it to existing data\n                    if \"source_file\" not in existing_columns:\n                        logger.info(\"Adding source_file column to existing dataset...\")\n                        # Read existing cells data\n                        existing_cells_df = existing_cells_table.to_pandas()\n                        existing_cells_df[\"source_file\"] = (\n                            \"original_data\"  # Default source for existing data\n                        )\n\n                        # Recreate the cells dataset with source_file column\n                        updated_cells_table = pa.table(existing_cells_df)\n                        # Cast large_string to string so subsequent appends match schema\n                        updated_cells_table = self._cast_table_string_columns_to_utf8(\n                            updated_cells_table, [\"cell_id\"]\n                        )\n                        lance.write_dataset(\n                            updated_cells_table,\n                            os.path.join(existing_slaf_path, \"cells.lance\"),\n                            mode=\"overwrite\",\n                            enable_v2_manifest_paths=self.enable_v2_manifest,\n                            data_storage_version=\"2.1\",\n                        )\n                        logger.info(\"\u2713 Added source_file column to existing dataset\")\n\n                    # Precompute cell start indices\n                    obs_df[\"cell_start_index\"] = self._compute_cell_start_indices(\n                        reader, obs_df\n                    )\n\n                    # Convert metadata to Lance tables\n                    cell_metadata_table = self._create_metadata_table(\n                        obs_df, \"cell_id\", integer_mapping=None\n                    )\n\n                    # Append to existing cells dataset\n                    cells_path = os.path.join(existing_slaf_path, \"cells.lance\")\n                    lance.write_dataset(\n                        cell_metadata_table,\n                        cells_path,\n                        mode=\"append\",\n                        enable_v2_manifest_paths=self.enable_v2_manifest,\n                        data_storage_version=\"2.1\",\n                    )\n\n                    # Process expression data in chunks with checkpointing support\n                    # Use the same checkpointing infrastructure as convert method\n                    self._process_file_chunks_with_checkpoint(\n                        reader, existing_slaf_path, i, 0, global_cell_offset\n                    )\n\n                    # Track source file information\n                    source_file_info.append(\n                        {\n                            \"file_path\": file_path,\n                            \"file_name\": source_file,\n                            \"n_cells\": len(obs_df),\n                            \"cell_offset\": global_cell_offset,\n                        }\n                    )\n\n                    # Update global cell offset\n                    global_cell_offset += len(obs_df)\n                    total_new_cells += len(obs_df)\n\n                    # Save checkpoint after each file (for append operations)\n                    if self.enable_checkpointing:\n                        checkpoint_data = {\n                            \"status\": \"in_progress\",\n                            \"last_completed_file\": i,\n                            \"last_completed_chunk\": -1,  # -1 indicates file is fully completed\n                            \"global_cell_offset\": global_cell_offset,\n                            \"operation\": \"append\",  # Mark this as an append operation\n                            \"timestamp\": pd.Timestamp.now().isoformat(),\n                        }\n                        self._save_checkpoint(existing_slaf_path, checkpoint_data)\n\n            except Exception as e:\n                logger.error(f\"Failed to process file {file_path}: {e}\")\n                # Save checkpoint with error status\n                if self.enable_checkpointing:\n                    checkpoint_data = {\n                        \"status\": \"error\",\n                        \"last_completed_file\": i - 1,\n                        \"last_completed_chunk\": -1,  # -1 indicates previous file was fully completed\n                        \"global_cell_offset\": global_cell_offset,\n                        \"operation\": \"append\",  # Mark this as an append operation\n                        \"error\": str(e),\n                        \"timestamp\": pd.Timestamp.now().isoformat(),\n                    }\n                    self._save_checkpoint(existing_slaf_path, checkpoint_data)\n                # Continue with other files\n                continue\n\n        if not source_file_info:\n            raise RuntimeError(\"No files were successfully processed\")\n\n        # Update configuration with new source file information\n        self._update_config_with_append(\n            existing_slaf_path, source_file_info, total_new_cells\n        )\n\n        # Clear checkpoint after successful completion\n        self._clear_checkpoint(existing_slaf_path)\n\n        logger.info(\n            f\"Append complete! Added {total_new_cells} cells from {len(source_file_info)} files\"\n        )\n        logger.info(f\"Total cells in dataset: {current_cell_count + total_new_cells}\")\n\n    def _validate_append_compatibility(\n        self, input_files: list[str], input_format: str, existing_slaf_path: str\n    ):\n        \"\"\"Validate that new files are compatible with existing SLAF dataset.\"\"\"\n        logger.info(\"Validating compatibility with existing SLAF dataset...\")\n\n        # Load existing dataset metadata\n        existing_cells_dataset = lance.dataset(f\"{existing_slaf_path}/cells.lance\")\n        existing_cells_table = existing_cells_dataset.to_table()\n        existing_genes_dataset = lance.dataset(f\"{existing_slaf_path}/genes.lance\")\n        existing_genes_table = existing_genes_dataset.to_table()\n\n        # Get existing gene set and cell metadata schema\n        existing_genes = set(existing_genes_table.column(\"gene_id\").to_numpy())\n        existing_cell_columns = set(existing_cells_table.column_names)\n\n        # Validate new files against existing dataset\n        for file_path in input_files:\n            try:\n                # Extract schema from new file\n                genes, cells, value_type = self._extract_schema_info(\n                    file_path, input_format\n                )\n\n                # Check gene compatibility\n                if genes != existing_genes:\n                    missing_genes = existing_genes - genes\n                    extra_genes = genes - existing_genes\n                    error_msg = f\"File {os.path.basename(file_path)} is incompatible with existing dataset:\"\n                    if missing_genes:\n                        error_msg += f\"\\n  Missing genes: {sorted(missing_genes)[:5]}{'...' if len(missing_genes) &gt; 5 else ''}\"\n                    if extra_genes:\n                        error_msg += f\"\\n  Extra genes: {sorted(extra_genes)[:5]}{'...' if len(extra_genes) &gt; 5 else ''}\"\n                    raise ValueError(error_msg)\n\n                # Check cell metadata schema compatibility\n                # Exclude columns that are added during SLAF conversion\n                slaF_added_columns = {\n                    \"cell_id\",\n                    \"cell_integer_id\",\n                    \"cell_start_index\",\n                    \"source_file\",\n                }\n                new_cell_columns = cells - slaF_added_columns\n                existing_cell_columns_no_slaf = (\n                    existing_cell_columns - slaF_added_columns\n                )\n\n                if new_cell_columns != existing_cell_columns_no_slaf:\n                    missing_cols = existing_cell_columns_no_slaf - new_cell_columns\n                    extra_cols = new_cell_columns - existing_cell_columns_no_slaf\n                    error_msg = f\"File {os.path.basename(file_path)} has incompatible cell metadata schema:\"\n                    if missing_cols:\n                        error_msg += f\"\\n  Missing columns: {sorted(missing_cols)}\"\n                    if extra_cols:\n                        error_msg += f\"\\n  Extra columns: {sorted(extra_cols)}\"\n                    raise ValueError(error_msg)\n\n            except Exception as e:\n                raise ValueError(\n                    f\"Validation failed for {os.path.basename(file_path)}: {e}\"\n                ) from e\n\n        logger.info(\"\u2713 All files are compatible with existing dataset\")\n\n    def _load_existing_config(self, existing_slaf_path: str) -&gt; dict:\n        \"\"\"Load existing SLAF configuration.\"\"\"\n        config_path = f\"{existing_slaf_path}/config.json\"\n        if not self._path_exists(config_path):\n            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n\n        with self._open_file(config_path) as f:\n            return json.load(f)\n\n    def _update_config_with_append(\n        self, existing_slaf_path: str, source_file_info: list, total_new_cells: int\n    ):\n        \"\"\"Update configuration with new source file information.\"\"\"\n        config_path = f\"{existing_slaf_path}/config.json\"\n        config = self._load_existing_config(existing_slaf_path)\n\n        # Update cell count\n        config[\"n_cells\"] += total_new_cells\n        config[\"array_shape\"][0] = config[\"n_cells\"]\n\n        # Update multi_file information\n        if \"multi_file\" not in config:\n            config[\"multi_file\"] = {\n                \"source_files\": [],\n                \"total_files\": 0,\n                \"total_cells_from_files\": 0,\n            }\n\n        # Add new source files\n        config[\"multi_file\"][\"source_files\"].extend(source_file_info)\n        config[\"multi_file\"][\"total_files\"] = len(config[\"multi_file\"][\"source_files\"])\n        config[\"multi_file\"][\"total_cells_from_files\"] = sum(\n            info[\"n_cells\"] for info in config[\"multi_file\"][\"source_files\"]\n        )\n\n        # Update created_at timestamp\n        config[\"last_updated\"] = pd.Timestamp.now().isoformat()\n\n        # Save updated config\n        with self._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n        logger.info(\n            f\"Updated configuration with {len(source_file_info)} new source files\"\n        )\n\n    def _extract_schema_info(\n        self, file_path: str, format_type: str\n    ) -&gt; tuple[set[str], set[str], str]:\n        \"\"\"Extract schema information from a file for compatibility checking.\"\"\"\n        if format_type == \"h5ad\":\n            return self._extract_h5ad_schema(file_path)\n        elif format_type == \"10x_mtx\":\n            return self._extract_10x_mtx_schema(file_path)\n        elif format_type == \"10x_h5\":\n            return self._extract_10x_h5_schema(file_path)\n        elif format_type == \"tiledb\":\n            return self._extract_tiledb_schema(file_path)\n        else:\n            raise ValueError(f\"Unsupported format: {format_type}\")\n\n    def _extract_h5ad_schema(self, file_path: str) -&gt; tuple[set[str], set[str], str]:\n        \"\"\"Extract schema information from h5ad file.\"\"\"\n        import scanpy as sc\n\n        # Read in backed mode for efficiency\n        adata = sc.read_h5ad(file_path, backed=\"r\")\n\n        # Get gene IDs\n        gene_ids = set(adata.var_names)\n\n        # Get cell metadata columns\n        cell_columns = set(adata.obs.columns)\n\n        # Determine value type from expression data\n        if hasattr(adata.X, \"data\"):\n            sample_data = adata.X.data[:1000]  # Sample first 1000 values\n        else:\n            # For backed mode, try to get a sample\n            try:\n                sample_data = (\n                    adata.X[:100, :100].data\n                    if hasattr(adata.X, \"data\")\n                    else adata.X[:100, :100].toarray().flatten()\n                )\n            except Exception:\n                sample_data = np.array([0])  # Fallback\n\n        # Determine value type\n        if np.issubdtype(sample_data.dtype, np.integer):\n            value_type = \"uint16\"\n        else:\n            value_type = \"float32\"\n\n        adata.file.close()\n        return gene_ids, cell_columns, value_type\n\n    def _extract_10x_mtx_schema(self, file_path: str) -&gt; tuple[set[str], set[str], str]:\n        \"\"\"Extract schema information from 10x MTX directory.\"\"\"\n        import scanpy as sc\n\n        # Read MTX files\n        adata = sc.read_10x_mtx(file_path)\n\n        # Get gene IDs\n        gene_ids = set(adata.var_names)\n\n        # Get cell metadata columns\n        cell_columns = set(adata.obs.columns)\n\n        # 10x MTX typically has integer counts\n        value_type = \"uint16\"\n\n        return gene_ids, cell_columns, value_type\n\n    def _extract_10x_h5_schema(self, file_path: str) -&gt; tuple[set[str], set[str], str]:\n        \"\"\"Extract schema information from 10x H5 file.\"\"\"\n        import scanpy as sc\n\n        try:\n            # Try to read as 10x H5 first\n            adata = sc.read_10x_h5(file_path, genome=\"X\")\n        except Exception:\n            # Fall back to regular h5ad\n            adata = sc.read_h5ad(file_path, backed=\"r\")\n\n        # Get gene IDs\n        gene_ids = set(adata.var_names)\n\n        # Get cell metadata columns\n        cell_columns = set(adata.obs.columns)\n\n        # 10x H5 typically has integer counts\n        value_type = \"uint16\"\n\n        if hasattr(adata, \"file\"):\n            adata.file.close()\n\n        return gene_ids, cell_columns, value_type\n\n    def _extract_tiledb_schema(self, file_path: str) -&gt; tuple[set[str], set[str], str]:\n        \"\"\"Extract schema information from TileDB SOMA file.\"\"\"\n        import tiledbsoma as soma\n\n        # Open TileDB SOMA experiment\n        with soma.open(file_path) as exp:\n            # Get measurement collection\n            ms = exp.ms[self.tiledb_collection_name]\n\n            # Get gene IDs from var\n            var_df = ms.var.read().concat().to_pandas()\n            gene_ids = set(var_df.index)\n\n            # Get cell metadata columns from obs\n            obs_df = ms.obs.read().concat().to_pandas()\n            cell_columns = set(obs_df.columns)\n\n            # TileDB SOMA typically has float32 values\n            value_type = \"float32\"\n\n            return gene_ids, cell_columns, value_type\n\n    def _convert_anndata(self, adata, output_path: str):\n        \"\"\"Internal method to convert AnnData object to SLAF format\"\"\"\n        # Create output directory (only for local paths)\n        self._ensure_directory_exists(output_path)\n\n        # Validate optimized data types and determine value type\n        validation_result, value_type = self._validate_optimized_dtypes_anndata(adata)\n        if not validation_result:\n            self.use_optimized_dtypes = False\n\n        # Create integer key mappings if needed\n        cell_id_mapping = None\n        gene_id_mapping = None\n\n        if self.use_integer_keys:\n            logger.info(\"Creating integer key mappings...\")\n            cell_id_mapping = self._create_id_mapping(adata.obs.index, \"cell\")\n            gene_id_mapping = self._create_id_mapping(adata.var.index, \"gene\")\n\n        # Convert expression data to COO format\n        logger.info(\"Converting expression data to COO format...\")\n        expression_table = self._sparse_to_coo_table(\n            sparse_matrix=adata.X,\n            cell_ids=adata.obs.index,\n            gene_ids=adata.var.index,\n            value_type=value_type,\n        )\n\n        # Convert metadata\n        logger.info(\"Converting metadata...\")\n\n        # Note: Sorting is disabled to maintain consistency between metadata and expression data ordering\n        # TODO: Implement proper sorting that affects both metadata and expression data\n        obs_df = adata.obs.copy()\n        var_df = adata.var.copy()\n\n        # Precompute cell start indices for fast cell-based queries\n        logger.info(\"Precomputing cell start indices...\")\n        obs_df[\"cell_start_index\"] = self._compute_cell_start_indices_anndata(\n            adata, obs_df\n        )\n\n        cell_metadata_table = self._create_metadata_table(\n            df=obs_df, entity_id_col=\"cell_id\", integer_mapping=cell_id_mapping\n        )\n        gene_metadata_table = self._create_metadata_table(\n            df=var_df, entity_id_col=\"gene_id\", integer_mapping=gene_id_mapping\n        )\n\n        # Write all Lance tables\n        logger.info(\"Writing Lance tables...\")\n        table_configs = [\n            (\"expression\", expression_table),\n            (\"cells\", cell_metadata_table),\n            (\"genes\", gene_metadata_table),\n        ]\n\n        self._write_lance_tables(output_path, table_configs)\n\n        # Convert layers if they exist\n        layer_names = []\n        if hasattr(adata, \"layers\") and adata.layers and len(adata.layers) &gt; 0:\n            logger.info(f\"Converting {len(adata.layers)} layers...\")\n            layer_names = self._convert_layers(\n                adata.layers,\n                output_path,\n                adata.obs.index,\n                adata.var.index,\n                value_type,\n            )\n\n        # Convert obsm if it exists\n        obsm_keys = []\n        if hasattr(adata, \"obsm\") and adata.obsm and len(adata.obsm) &gt; 0:\n            logger.info(f\"Converting {len(adata.obsm)} obsm embeddings...\")\n            obsm_keys = self._convert_obsm(\n                adata.obsm,\n                output_path,\n                adata.obs.index,\n                cell_id_mapping,\n            )\n\n        # Convert varm if it exists\n        varm_keys = []\n        if hasattr(adata, \"varm\") and adata.varm and len(adata.varm) &gt; 0:\n            logger.info(f\"Converting {len(adata.varm)} varm embeddings...\")\n            varm_keys = self._convert_varm(\n                adata.varm,\n                output_path,\n                adata.var.index,\n                gene_id_mapping,\n            )\n\n        # Convert uns if it exists\n        if hasattr(adata, \"uns\") and adata.uns and len(adata.uns) &gt; 0:\n            logger.info(\"Converting uns metadata...\")\n            self._convert_uns(adata.uns, output_path)\n\n        # Compact dataset for optimal storage (only if enabled)\n        if self.compact_after_write:\n            self._compact_dataset(output_path)\n\n        # Save config (with layers, obsm, varm metadata if they were converted)\n        self._save_config(\n            output_path,\n            adata.shape,\n            layer_names=layer_names,\n            obsm_keys=obsm_keys,\n            varm_keys=varm_keys,\n        )\n        logger.info(f\"Conversion complete! Saved to {output_path}\")\n\n    def _convert_chunked(self, h5ad_path: str, output_path: str):\n        \"\"\"Convert h5ad file using chunked processing with checkpointing support\"\"\"\n        logger.info(f\"Processing in chunks of {self.chunk_size} cells...\")\n\n        # Check for existing checkpoint with smart resume logic\n        checkpoint = self._load_checkpoint_smart(output_path)\n        if checkpoint:\n            logger.info(f\"Found checkpoint: {checkpoint}\")\n            if checkpoint.get(\"status\") == \"completed\":\n                logger.info(\"Conversion already completed according to checkpoint\")\n                return\n            elif checkpoint.get(\"status\") == \"in_progress\":\n                logger.info(\"Resuming from checkpoint...\")\n                # Resume logic will be handled in _process_expression\n            else:\n                logger.info(\"Starting fresh conversion...\")\n\n        # First, create a temporary reader to determine the value type\n        with create_chunked_reader(\n            h5ad_path,\n            chunk_size=self.chunk_size,\n            collection_name=self.tiledb_collection_name,\n        ) as temp_reader:\n            # Validate optimized data types and determine value type\n            validation_result, value_type = self._validate_optimized_dtypes(temp_reader)\n            if not validation_result:\n                self.use_optimized_dtypes = False\n\n        # Now create the reader with the correct value type\n        with create_chunked_reader(\n            h5ad_path,\n            chunk_size=self.chunk_size,\n            value_type=value_type,\n            collection_name=self.tiledb_collection_name,\n        ) as reader:\n            logger.info(f\"Loaded: {reader.n_obs:,} cells \u00d7 {reader.n_vars:,} genes\")\n\n            # Create output directory (only for local paths)\n            self._ensure_directory_exists(output_path)\n\n            # Write metadata tables efficiently (without loading everything into memory)\n            # Only write metadata if not resuming from checkpoint\n            if not checkpoint or checkpoint.get(\"status\") != \"in_progress\":\n                self._write_metadata_efficiently(reader, output_path)\n\n            # Check if layers exist before processing\n            layer_names = []\n            if hasattr(reader, \"adata\") and reader.adata is not None:\n                if (\n                    hasattr(reader.adata, \"layers\")\n                    and reader.adata.layers\n                    and len(reader.adata.layers) &gt; 0\n                ):\n                    layer_names = list(reader.adata.layers.keys())\n                    logger.info(f\"Detected {len(layer_names)} layers: {layer_names}\")\n\n            # Process expression data with checkpointing\n            self._process_expression_with_checkpoint(\n                reader,\n                output_path,\n                value_type,\n                checkpoint,\n                layers=False,\n                table_name=\"expression\",\n            )\n\n            # Convert layers if they exist (chunked processing)\n            if layer_names:\n                logger.info(f\"Converting {len(layer_names)} layers in chunked mode...\")\n                # For layers, use float32 as default (layers typically contain float data)\n                # Update reader's value_type to match (needed for _read_chunk_layers_wide)\n                original_value_type = reader.value_type\n                reader.value_type = \"float32\"\n                try:\n                    self._process_expression_with_checkpoint(\n                        reader,\n                        output_path,\n                        value_type=\"float32\",\n                        checkpoint=None,  # Layers don't use checkpointing (processed after expression)\n                        layers=True,\n                        table_name=\"layers\",\n                    )\n                finally:\n                    # Restore original value_type\n                    reader.value_type = original_value_type\n\n            # Create indices (if enabled)\n            if self.create_indices:\n                self._create_indices(output_path)\n\n            # Compact dataset for optimal storage (only if enabled)\n            if self.compact_after_write:\n                self._compact_dataset(output_path)\n\n            # Save config and clear checkpoint (with layers metadata if layers were converted)\n            self._save_config(\n                output_path, (reader.n_obs, reader.n_vars), layer_names=layer_names\n            )\n            self._clear_checkpoint(output_path)\n            logger.info(f\"Conversion complete! Saved to {output_path}\")\n\n    def _write_metadata_efficiently(self, reader, output_path: str):\n        \"\"\"Write metadata tables efficiently while preserving all columns\"\"\"\n        logger.info(\"Writing metadata tables...\")\n\n        # Get full metadata from reader (this loads all columns)\n        obs_df = reader.get_obs_metadata()\n        var_df = reader.get_var_metadata()\n\n        # Ensure cell_id and gene_id columns exist with actual names\n        if \"cell_id\" not in obs_df.columns:\n            obs_df[\"cell_id\"] = reader.obs_names\n        if \"gene_id\" not in var_df.columns:\n            var_df[\"gene_id\"] = reader.var_names\n\n        # Note: Sorting is disabled in chunked mode to maintain consistency\n        # between metadata and expression data ordering\n        # TODO: Implement proper sorting that affects both metadata and expression data\n\n        # Add integer IDs if enabled\n        if self.use_integer_keys:\n            obs_df[\"cell_integer_id\"] = range(len(obs_df))\n            var_df[\"gene_integer_id\"] = range(len(var_df))\n\n        # Precompute cell start indices for fast cell-based queries\n        logger.info(\"Precomputing cell start indices...\")\n        obs_df[\"cell_start_index\"] = self._compute_cell_start_indices(reader, obs_df)\n\n        # Convert to Lance tables\n        cell_metadata_table = self._create_metadata_table(\n            obs_df,\n            \"cell_id\",\n            integer_mapping=None,  # Already added above\n        )\n        gene_metadata_table = self._create_metadata_table(\n            var_df,\n            \"gene_id\",\n            integer_mapping=None,  # Already added above\n        )\n\n        # Write metadata tables\n        lance.write_dataset(\n            cell_metadata_table,\n            f\"{output_path}/cells.lance\",\n            mode=\"overwrite\",\n            enable_v2_manifest_paths=self.enable_v2_manifest,\n            data_storage_version=\"2.1\",\n        )\n        lance.write_dataset(\n            gene_metadata_table,\n            f\"{output_path}/genes.lance\",\n            mode=\"overwrite\",\n            enable_v2_manifest_paths=self.enable_v2_manifest,\n            data_storage_version=\"2.1\",\n        )\n\n        logger.info(\"Metadata tables written!\")\n\n    def _process_expression(self, reader, output_path: str, value_type=\"uint16\"):\n        \"\"\"Process expression data in single-threaded mode with large chunks\"\"\"\n        logger.info(\"Processing expression data in single-threaded mode...\")\n\n        # Calculate total chunks\n        total_chunks = (reader.n_obs + self.chunk_size - 1) // self.chunk_size\n        logger.info(\n            f\"Processing {total_chunks} chunks with chunk size {self.chunk_size:,}...\"\n        )\n\n        # Memory monitoring\n        process = None\n        initial_memory = None\n        try:\n            import psutil\n\n            process = psutil.Process()\n            initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n            logger.info(f\"Initial memory usage: {initial_memory:.1f} MB\")\n        except ImportError:\n            logger.info(\"Install psutil for memory monitoring: pip install psutil\")\n\n        # Create Lance dataset with schema\n        expression_path = f\"{output_path}/expression.lance\"\n        schema = self._get_expression_schema(value_type)\n\n        # Create empty dataset first\n        logger.info(\"Creating initial Lance dataset...\")\n        schema = self._get_expression_schema(value_type)\n\n        # Create empty table with correct schema based on settings\n        if value_type == \"uint16\":\n            value_pa_type = pa.uint16()\n        elif value_type == \"float32\":\n            value_pa_type = pa.float32()\n        else:\n            raise ValueError(f\"Unsupported value type: {value_type}\")\n\n        if self.optimize_storage:\n            if self.use_optimized_dtypes:\n                empty_table = pa.table(\n                    {\n                        \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                        \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                        \"value\": pa.array([], type=value_pa_type),\n                    }\n                )\n            else:\n                empty_table = pa.table(\n                    {\n                        \"cell_integer_id\": pa.array([], type=pa.int32()),\n                        \"gene_integer_id\": pa.array([], type=pa.int32()),\n                        \"value\": pa.array([], type=value_pa_type),\n                    }\n                )\n        else:\n            if self.use_optimized_dtypes:\n                empty_table = pa.table(\n                    {\n                        \"cell_id\": pa.array([], type=pa.string()),\n                        \"gene_id\": pa.array([], type=pa.string()),\n                        \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                        \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                        \"value\": pa.array([], type=value_pa_type),\n                    }\n                )\n            else:\n                empty_table = pa.table(\n                    {\n                        \"cell_id\": pa.array([], type=pa.string()),\n                        \"gene_id\": pa.array([], type=pa.string()),\n                        \"cell_integer_id\": pa.array([], type=pa.int32()),\n                        \"gene_integer_id\": pa.array([], type=pa.int32()),\n                        \"value\": pa.array([], type=value_pa_type),\n                    }\n                )\n\n        # Create initial Lance dataset (using max_rows_per_file for large fragments)\n        lance.write_dataset(\n            empty_table,\n            expression_path,\n            mode=\"overwrite\",\n            schema=schema,\n            max_rows_per_file=self.max_rows_per_file,\n            enable_v2_manifest_paths=self.enable_v2_manifest,\n            data_storage_version=\"2.1\",\n        )\n\n        # Process chunks sequentially\n        logger.info(\"Processing chunks sequentially...\")\n        import time\n\n        from tqdm import tqdm\n\n        processing_start_time = time.time()\n\n        for _chunk_idx, (chunk_table, _obs_slice) in enumerate(\n            tqdm(\n                reader.iter_chunks(chunk_size=self.chunk_size),\n                total=total_chunks,\n                desc=\"Processing chunks\",\n                unit=\"chunk\",\n            )\n        ):\n            # Process chunk (data type conversion and string ID addition if needed)\n            if not self.use_optimized_dtypes:\n                # Convert from optimized dtypes to standard dtypes\n                cell_integer_ids = (\n                    chunk_table.column(\"cell_integer_id\").to_numpy().astype(np.int32)\n                )\n                gene_integer_ids = (\n                    chunk_table.column(\"gene_integer_id\").to_numpy().astype(np.int32)\n                )\n                values = chunk_table.column(\"value\").to_numpy().astype(np.float32)\n\n                chunk_table = pa.table(\n                    {\n                        \"cell_integer_id\": pa.array(cell_integer_ids),\n                        \"gene_integer_id\": pa.array(gene_integer_ids),\n                        \"value\": pa.array(values),\n                    }\n                )\n\n            if not self.optimize_storage:\n                # Get cell and gene names\n                cell_names = reader.obs_names\n                gene_names = reader.var_names\n\n                # Create string ID arrays (use pa.string() for lance compatibility)\n                cell_integer_ids = chunk_table.column(\"cell_integer_id\").to_numpy()\n                gene_integer_ids = chunk_table.column(\"gene_integer_id\").to_numpy()\n\n                cell_ids = cell_names[cell_integer_ids].astype(str)\n                gene_ids = gene_names[gene_integer_ids].astype(str)\n\n                # Create new table with string IDs (explicit utf8 string for lance)\n                chunk_table = pa.table(\n                    {\n                        \"cell_id\": pa.array(cell_ids, type=pa.string()),\n                        \"gene_id\": pa.array(gene_ids, type=pa.string()),\n                        \"cell_integer_id\": chunk_table.column(\"cell_integer_id\"),\n                        \"gene_integer_id\": chunk_table.column(\"gene_integer_id\"),\n                        \"value\": chunk_table.column(\"value\"),\n                    }\n                )\n\n            # Write chunk to Lance dataset (using max_rows_per_file for large fragments)\n            lance.write_dataset(\n                chunk_table,\n                expression_path,\n                mode=\"append\",\n                max_rows_per_file=self.max_rows_per_file,\n                enable_v2_manifest_paths=self.enable_v2_manifest,\n                data_storage_version=\"2.1\",\n            )\n\n        # Final memory report\n        if process is not None and initial_memory is not None:\n            try:\n                final_memory = process.memory_info().rss / 1024 / 1024  # MB\n                memory_increase = final_memory - initial_memory\n                logger.info(\n                    f\"Final memory usage: {final_memory:.1f} MB (change: {memory_increase:+.1f} MB)\"\n                )\n            except Exception:\n                pass\n\n        # Calculate and log overall statistics\n        total_processing_time = time.time() - processing_start_time\n\n        logger.info(\n            f\"Expression data processing complete! \"\n            f\"Processed {total_chunks} chunks in {total_processing_time:.1f}s \"\n            f\"({total_processing_time / total_chunks:.2f}s per chunk average)\"\n        )\n\n    def _process_expression_with_checkpoint(\n        self,\n        reader,\n        output_path: str,\n        value_type=\"uint16\",\n        checkpoint=None,\n        layers: bool = False,\n        table_name: str = \"expression\",\n        cell_offset: int = 0,\n        skip_empty_table: bool = False,\n    ):\n        \"\"\"\n        Process expression or layer data with checkpointing support.\n\n        Args:\n            reader: Chunked reader instance\n            output_path: Output SLAF directory path\n            value_type: Value type for data (\"uint16\" or \"float32\")\n            checkpoint: Optional checkpoint data for resuming\n            layers: If True, processes all layers in wide format. If False, processes expression (X).\n            table_name: Name of the table to write to (\"expression\" or \"layers\")\n            cell_offset: Global cell offset to add to cell_integer_id (for multi-file conversion)\n            skip_empty_table: If True, skip creating empty table (for multi-file append mode)\n        \"\"\"\n        if layers:\n            logger.info(\"Processing layers data with checkpointing...\")\n        else:\n            logger.info(\"Processing expression data with checkpointing...\")\n\n        # Calculate total chunks\n        total_chunks = (reader.n_obs + self.chunk_size - 1) // self.chunk_size\n        logger.info(\n            f\"Processing {total_chunks} chunks with chunk size {self.chunk_size:,}...\"\n        )\n\n        # Determine starting chunk from checkpoint\n        start_chunk = 0\n        if checkpoint and checkpoint.get(\"status\") == \"in_progress\":\n            start_chunk = checkpoint.get(\"last_completed_chunk\", 0) + 1\n            logger.info(\n                f\"Resuming from chunk {start_chunk} (last completed: {checkpoint.get('last_completed_chunk', -1)})\"\n            )\n\n        # Memory monitoring\n        process = None\n        initial_memory = None\n        try:\n            import psutil\n\n            process = psutil.Process()\n            initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n            logger.info(f\"Initial memory usage: {initial_memory:.1f} MB\")\n        except ImportError:\n            logger.info(\"Install psutil for memory monitoring: pip install psutil\")\n\n        # Create Lance dataset with schema\n        expression_path = f\"{output_path}/expression.lance\"\n        if layers:\n            table_path = f\"{output_path}/{table_name}.lance\"\n            # For layers, we need to build schema dynamically from available layers\n            if hasattr(reader, \"adata\") and reader.adata is not None:\n                if hasattr(reader.adata, \"layers\") and reader.adata.layers:\n                    layer_names = list(reader.adata.layers.keys())\n                    # Build schema for layers table (wide format)\n                    value_pa_type = (\n                        pa.uint16() if value_type == \"uint16\" else pa.float32()\n                    )\n                    schema_fields = [\n                        pa.field(\"cell_integer_id\", pa.uint32()),\n                        pa.field(\"gene_integer_id\", pa.uint16()),\n                    ]\n                    for layer_name in layer_names:\n                        schema_fields.append(\n                            pa.field(layer_name, value_pa_type, nullable=True)\n                        )\n                    schema = pa.schema(schema_fields)\n                else:\n                    # No layers, use expression schema\n                    schema = self._get_expression_schema(value_type)\n            else:\n                schema = self._get_expression_schema(value_type)\n        else:\n            table_path = expression_path\n            schema = self._get_expression_schema(value_type)\n\n        # Create empty dataset first (only if not resuming and not skipping)\n        if start_chunk == 0 and not skip_empty_table:\n            if layers:\n                logger.info(\"Creating initial layers Lance dataset...\")\n            else:\n                logger.info(\"Creating initial Lance dataset...\")\n            # Schema already set above\n\n            # Create empty table with correct schema based on settings\n            if layers:\n                # For layers, create empty table with all layer columns\n                if hasattr(reader, \"adata\") and reader.adata is not None:\n                    if hasattr(reader.adata, \"layers\") and reader.adata.layers:\n                        layer_names = list(reader.adata.layers.keys())\n                        empty_dict = {\n                            \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                            \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                        }\n                        value_pa_type = (\n                            pa.uint16() if value_type == \"uint16\" else pa.float32()\n                        )\n                        for layer_name in layer_names:\n                            empty_dict[layer_name] = pa.array([], type=value_pa_type)\n                        empty_table = pa.table(empty_dict)\n                    else:\n                        # No layers, create minimal table\n                        empty_table = pa.table(\n                            {\n                                \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                                \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                            }\n                        )\n                else:\n                    # Can't determine layers, create minimal table\n                    empty_table = pa.table(\n                        {\n                            \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                            \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                        }\n                    )\n            else:\n                # Expression table\n                if value_type == \"uint16\":\n                    value_pa_type = pa.uint16()\n                elif value_type == \"float32\":\n                    value_pa_type = pa.float32()\n                else:\n                    raise ValueError(f\"Unsupported value type: {value_type}\")\n\n                if self.optimize_storage:\n                    if self.use_optimized_dtypes:\n                        empty_table = pa.table(\n                            {\n                                \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                                \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                                \"value\": pa.array([], type=value_pa_type),\n                            }\n                        )\n                    else:\n                        empty_table = pa.table(\n                            {\n                                \"cell_integer_id\": pa.array([], type=pa.int32()),\n                                \"gene_integer_id\": pa.array([], type=pa.int32()),\n                                \"value\": pa.array([], type=value_pa_type),\n                            }\n                        )\n                else:\n                    if self.use_optimized_dtypes:\n                        empty_table = pa.table(\n                            {\n                                \"cell_id\": pa.array([], type=pa.string()),\n                                \"gene_id\": pa.array([], type=pa.string()),\n                                \"cell_integer_id\": pa.array([], type=pa.uint32()),\n                                \"gene_integer_id\": pa.array([], type=pa.uint16()),\n                                \"value\": pa.array([], type=value_pa_type),\n                            }\n                        )\n                    else:\n                        empty_table = pa.table(\n                            {\n                                \"cell_id\": pa.array([], type=pa.string()),\n                                \"gene_id\": pa.array([], type=pa.string()),\n                                \"cell_integer_id\": pa.array([], type=pa.int32()),\n                                \"gene_integer_id\": pa.array([], type=pa.int32()),\n                                \"value\": pa.array([], type=value_pa_type),\n                            }\n                        )\n\n            # Create initial Lance dataset (using max_rows_per_file for large fragments)\n            lance.write_dataset(\n                empty_table,\n                table_path,\n                mode=\"overwrite\",\n                schema=schema,\n                max_rows_per_file=self.max_rows_per_file,\n                enable_v2_manifest_paths=self.enable_v2_manifest,\n                data_storage_version=\"2.1\",\n            )\n\n        # Process chunks sequentially with checkpointing\n        logger.info(\"Processing chunks sequentially with checkpointing...\")\n        import time\n\n        from tqdm import tqdm\n\n        processing_start_time = time.time()\n\n        # Create iterator for chunks\n        chunk_iterator = reader.iter_chunks(chunk_size=self.chunk_size)\n\n        # Skip chunks if resuming\n        for _ in range(start_chunk):\n            try:\n                next(chunk_iterator)\n            except StopIteration:\n                logger.warning(\n                    f\"Tried to skip {start_chunk} chunks but iterator ended early\"\n                )\n                break\n\n        # Process remaining chunks\n        for chunk_idx in tqdm(\n            range(start_chunk, total_chunks),\n            desc=\"Processing chunks\",\n            unit=\"chunk\",\n            initial=start_chunk,\n            total=total_chunks,\n        ):\n            try:\n                # Get next chunk - use layers flag if provided\n                if layers:\n                    # For layers, we need to read the wide format chunk\n                    # Get the slice from the iterator first\n                    _, obs_slice = next(chunk_iterator)\n                    # Read layers chunk in wide format with cell offset\n                    chunk_table = reader._read_chunk_as_arrow(\n                        obs_slice.start,\n                        obs_slice.stop,\n                        layers=True,\n                        cell_offset=cell_offset,\n                    )\n                else:\n                    chunk_table, _obs_slice = next(chunk_iterator)\n\n                # Process chunk (data type conversion and string ID addition if needed)\n                # For layers, skip dtype conversion as they're already in wide format\n                if not layers and not self.use_optimized_dtypes:\n                    # Convert from optimized dtypes to standard dtypes (expression only)\n                    cell_integer_ids = (\n                        chunk_table.column(\"cell_integer_id\")\n                        .to_numpy()\n                        .astype(np.int32)\n                    )\n                    gene_integer_ids = (\n                        chunk_table.column(\"gene_integer_id\")\n                        .to_numpy()\n                        .astype(np.int32)\n                    )\n                    values = chunk_table.column(\"value\").to_numpy().astype(np.float32)\n\n                    chunk_table = pa.table(\n                        {\n                            \"cell_integer_id\": pa.array(cell_integer_ids),\n                            \"gene_integer_id\": pa.array(gene_integer_ids),\n                            \"value\": pa.array(values),\n                        }\n                    )\n\n                # For layers, skip string ID addition as they use wide format\n                if not layers and not self.optimize_storage:\n                    # Get cell and gene names\n                    cell_names = reader.obs_names\n                    gene_names = reader.var_names\n\n                    # Create string ID arrays (use pa.string() for lance compatibility)\n                    cell_integer_ids = chunk_table.column(\"cell_integer_id\").to_numpy()\n                    gene_integer_ids = chunk_table.column(\"gene_integer_id\").to_numpy()\n\n                    cell_ids = cell_names[cell_integer_ids].astype(str)\n                    gene_ids = gene_names[gene_integer_ids].astype(str)\n\n                    # Create new table with string IDs (explicit utf8 string for lance)\n                    chunk_table = pa.table(\n                        {\n                            \"cell_id\": pa.array(cell_ids, type=pa.string()),\n                            \"gene_id\": pa.array(gene_ids, type=pa.string()),\n                            \"cell_integer_id\": chunk_table.column(\"cell_integer_id\"),\n                            \"gene_integer_id\": chunk_table.column(\"gene_integer_id\"),\n                            \"value\": chunk_table.column(\"value\"),\n                        }\n                    )\n\n                # Save checkpoint BEFORE writing chunk to avoid duplication\n                if self.enable_checkpointing:\n                    should_save_checkpoint = (\n                        chunk_idx % 10 == 0  # Every 10 chunks\n                        or chunk_idx == total_chunks - 1  # Last chunk\n                    )\n\n                    if should_save_checkpoint:\n                        checkpoint_data = {\n                            \"status\": \"in_progress\",\n                            \"last_completed_chunk\": chunk_idx,\n                            \"total_chunks\": total_chunks,\n                            \"chunk_size\": self.chunk_size,\n                            \"timestamp\": pd.Timestamp.now().isoformat(),\n                        }\n                        self._save_checkpoint(output_path, checkpoint_data)\n\n                # Write chunk to Lance dataset (using max_rows_per_file for large fragments)\n                lance.write_dataset(\n                    chunk_table,\n                    table_path,\n                    mode=\"append\",\n                    max_rows_per_file=self.max_rows_per_file,\n                    enable_v2_manifest_paths=self.enable_v2_manifest,\n                    data_storage_version=\"2.1\",\n                )\n\n            except StopIteration:\n                logger.warning(f\"Chunk iterator ended at chunk {chunk_idx}\")\n                break\n            except Exception as e:\n                logger.error(f\"Error processing chunk {chunk_idx}: {e}\")\n                # Save checkpoint with error status\n                if self.enable_checkpointing:\n                    checkpoint_data = {\n                        \"status\": \"error\",\n                        \"last_completed_chunk\": chunk_idx - 1,\n                        \"total_chunks\": total_chunks,\n                        \"error\": str(e),\n                        \"timestamp\": pd.Timestamp.now().isoformat(),\n                    }\n                    self._save_checkpoint(output_path, checkpoint_data)\n                raise\n\n        # Final memory report\n        if process is not None and initial_memory is not None:\n            try:\n                final_memory = process.memory_info().rss / 1024 / 1024  # MB\n                memory_increase = final_memory - initial_memory\n                logger.info(\n                    f\"Final memory usage: {final_memory:.1f} MB (change: {memory_increase:+.1f} MB)\"\n                )\n            except Exception:\n                pass\n\n        # Calculate and log overall statistics\n        total_processing_time = time.time() - processing_start_time\n\n        logger.info(\n            f\"Expression data processing complete! \"\n            f\"Processed {total_chunks} chunks in {total_processing_time:.1f}s \"\n            f\"({total_processing_time / total_chunks:.2f}s per chunk average)\"\n        )\n\n    def _process_file_chunks_with_checkpoint(\n        self,\n        reader,\n        output_path: str,\n        file_idx: int,\n        start_chunk_idx: int,\n        global_cell_offset: int,\n    ):\n        \"\"\"Process chunks for a single file with checkpointing support\"\"\"\n        expression_path = f\"{output_path}/expression.lance\"\n\n        # Calculate total chunks for this file\n        total_chunks = (reader.n_obs + self.chunk_size - 1) // self.chunk_size\n\n        logger.info(\n            f\"\ud83d\udcca Processing file {file_idx + 1} chunks: {start_chunk_idx}/{total_chunks}\"\n        )\n        if start_chunk_idx &gt; 0:\n            logger.info(\n                f\"\ud83d\udd04 Resuming from chunk {start_chunk_idx} (chunks 0-{start_chunk_idx - 1} already processed)\"\n            )\n\n        # Create iterator for chunks\n        chunk_iterator = reader.iter_chunks(chunk_size=self.chunk_size)\n\n        # Skip chunks if resuming from checkpoint\n        if start_chunk_idx &gt; 0:\n            logger.info(f\"\u23ed\ufe0f  Skipping {start_chunk_idx} already-processed chunks...\")\n            for _ in range(start_chunk_idx):\n                try:\n                    next(chunk_iterator)\n                except StopIteration:\n                    logger.warning(\n                        f\"Tried to skip {start_chunk_idx} chunks but iterator ended early\"\n                    )\n                    break\n\n        # Process remaining chunks\n        for chunk_idx in range(start_chunk_idx, total_chunks):\n            try:\n                # Get next chunk\n                chunk_table, _obs_slice = next(chunk_iterator)\n\n                # Log progress every 10 chunks or on first/last chunk\n                if (\n                    chunk_idx % 10 == 0\n                    or chunk_idx == start_chunk_idx\n                    or chunk_idx == total_chunks - 1\n                ):\n                    logger.info(\n                        f\"\ud83d\udcc8 Processing chunk {chunk_idx + 1}/{total_chunks} (file {file_idx + 1})\"\n                    )\n\n                # Adjust cell integer IDs with global offset\n                cell_integer_ids = (\n                    chunk_table.column(\"cell_integer_id\").to_numpy()\n                    + global_cell_offset\n                )\n\n                # Create adjusted chunk table\n                if self.optimize_storage:\n                    # Only store integer IDs for maximum storage efficiency\n                    adjusted_chunk = pa.table(\n                        {\n                            \"cell_integer_id\": pa.array(cell_integer_ids),\n                            \"gene_integer_id\": chunk_table.column(\"gene_integer_id\"),\n                            \"value\": chunk_table.column(\"value\"),\n                        }\n                    )\n                else:\n                    # Store both string and integer IDs for compatibility\n                    adjusted_chunk = pa.table(\n                        {\n                            \"cell_id\": chunk_table.column(\"cell_id\"),\n                            \"gene_id\": chunk_table.column(\"gene_id\"),\n                            \"cell_integer_id\": pa.array(cell_integer_ids),\n                            \"gene_integer_id\": chunk_table.column(\"gene_integer_id\"),\n                            \"value\": chunk_table.column(\"value\"),\n                        }\n                    )\n\n                # Save checkpoint BEFORE writing chunk to avoid duplication\n                # This ensures we know exactly what was written if a failure occurs\n                if self.enable_checkpointing:\n                    should_save_checkpoint = (\n                        chunk_idx % 10 == 0  # Every 10 chunks\n                        or chunk_idx == total_chunks - 1  # Last chunk of file\n                    )\n\n                    if should_save_checkpoint:\n                        checkpoint_data = {\n                            \"status\": \"in_progress\",\n                            \"last_completed_file\": file_idx,  # Current file being processed\n                            \"last_completed_chunk\": chunk_idx,  # Current chunk being processed\n                            \"global_cell_offset\": global_cell_offset,\n                            \"timestamp\": pd.Timestamp.now().isoformat(),\n                        }\n                        self._save_checkpoint(output_path, checkpoint_data)\n\n                # Write chunk directly to expression dataset\n                # Check if this is the first chunk of the first file in a new dataset\n                is_first_chunk_of_new_dataset = file_idx == 0 and chunk_idx == 0\n\n                # Check if the expression dataset already exists (for append operations)\n                expression_dataset_exists = self._path_exists(expression_path)\n                if is_first_chunk_of_new_dataset and not expression_dataset_exists:\n                    # First chunk of first file in a new dataset - create new dataset\n                    lance.write_dataset(\n                        adjusted_chunk,\n                        expression_path,\n                        mode=\"overwrite\",\n                        max_rows_per_file=self.max_rows_per_file,\n                        enable_v2_manifest_paths=self.enable_v2_manifest,\n                        data_storage_version=\"2.1\",\n                    )\n                else:\n                    # Append to existing dataset (either subsequent chunks or append operations)\n                    lance.write_dataset(\n                        adjusted_chunk,\n                        expression_path,\n                        mode=\"append\",\n                        max_rows_per_file=self.max_rows_per_file,\n                        enable_v2_manifest_paths=self.enable_v2_manifest,\n                        data_storage_version=\"2.1\",\n                    )\n\n            except StopIteration:\n                logger.warning(f\"Chunk iterator ended at chunk {chunk_idx}\")\n                break\n            except Exception as e:\n                logger.error(\n                    f\"Error processing chunk {chunk_idx} in file {file_idx + 1}: {e}\"\n                )\n                # Save checkpoint with error status\n                if self.enable_checkpointing:\n                    checkpoint_data = {\n                        \"status\": \"error\",\n                        \"last_completed_file\": file_idx,\n                        \"last_completed_chunk\": chunk_idx - 1,\n                        \"global_cell_offset\": global_cell_offset,\n                        \"error\": str(e),\n                        \"timestamp\": pd.Timestamp.now().isoformat(),\n                    }\n                    self._save_checkpoint(output_path, checkpoint_data)\n                raise\n\n    def _validate_optimized_dtypes(self, reader):\n        \"\"\"Validate that data fits in optimized data types and determine appropriate value type\"\"\"\n        if not self.use_optimized_dtypes:\n            return True, \"float32\"\n\n        logger.info(\"Validating data fits in optimized data types...\")\n\n        # Check if gene count fits in uint16\n        if reader.n_vars &gt; 65535:\n            logger.info(f\"Warning: {reader.n_vars:,} genes exceeds uint16 limit\")\n            logger.info(\"Falling back to standard data types\")\n            return False, \"float32\"\n\n        # Check if cell count fits in uint32 (0-4,294,967,295)\n        if reader.n_obs &gt; 4294967295:\n            logger.info(\n                f\"Warning: {reader.n_obs:,} cells exceeds uint32 limit (4,294,967,295)\"\n            )\n            logger.info(\"Falling back to standard data types\")\n            return False, \"float32\"\n\n        # Sample original data from the file to determine data type\n        logger.info(\"Sampling original expression values to determine data type...\")\n\n        # For chunked readers, we need to check the original data type from the file\n        # Handle TileDB readers specially\n        if hasattr(reader, \"_experiment\") and reader._experiment is not None:\n            # TileDB reader - check original data directly\n            try:\n                X = reader._experiment.ms[reader.collection_name].X[\"data\"]\n                # Sample some original data\n                sample_size = min(10000, X.shape[0])\n                sample_data = X.read((slice(0, sample_size),)).tables().concat()\n                sample_values = sample_data.column(\"soma_data\").to_numpy()\n\n                # Check if original data is integer or float\n                is_integer = np.issubdtype(sample_values.dtype, np.integer)\n                max_value = np.max(sample_values)\n                min_value = np.min(sample_values)\n\n                if is_integer and max_value &lt;= 65535 and min_value &gt;= 0:\n                    logger.info(\n                        f\"Original integer expression values fit in uint16 range: [{min_value}, {max_value}]\"\n                    )\n                    logger.info(\"Using uint16 for integer count data\")\n                    return True, \"uint16\"\n                elif not is_integer:\n                    # Check if float data contains only integer values\n                    rounded_data = np.round(sample_values)\n                    is_integer_values = np.allclose(\n                        sample_values, rounded_data, rtol=1e-10\n                    )\n\n                    if is_integer_values and max_value &lt;= 65535 and min_value &gt;= 0:\n                        logger.info(\n                            f\"Float data contains only integer values: [{min_value}, {max_value}]\"\n                        )\n                        logger.info(\"Converting to uint16 for count data\")\n                        return True, \"uint16\"\n                    else:\n                        logger.info(\n                            f\"Float data contains fractional values: [{min_value}, {max_value}]\"\n                        )\n                        logger.info(\"Keeping as float32 for normalized/float data\")\n                        return False, \"float32\"\n                else:\n                    logger.info(\n                        f\"Warning: Original integer values range [{min_value}, {max_value}] exceeds uint16 range [0, 65535]\"\n                    )\n                    logger.info(\"Falling back to float32\")\n                    return False, \"float32\"\n            except Exception as e:\n                logger.warning(f\"Error checking TileDB original data: {e}\")\n                # Fall through to fallback logic\n        elif hasattr(reader, \"file\") and reader.file is not None:\n            # Check the original data type from the h5ad file\n            X_group = reader.file[\"X\"]\n            if \"data\" in X_group:\n                # Sample some original data\n                data = X_group[\"data\"]\n                sample_size = min(10000, len(data))\n                # Use sequential sampling instead of random to avoid indexing issues\n                sample_data = data[:sample_size]\n\n                # Check if original data is integer or float\n                is_integer = np.issubdtype(sample_data.dtype, np.integer)\n                max_value = np.max(sample_data)\n                min_value = np.min(sample_data)\n\n                if is_integer and max_value &lt;= 65535 and min_value &gt;= 0:\n                    logger.info(\n                        f\"Original integer expression values fit in uint16 range: [{min_value}, {max_value}]\"\n                    )\n                    logger.info(\"Using uint16 for integer count data\")\n                    return True, \"uint16\"\n                elif not is_integer:\n                    # Check if float data contains only integer values\n                    # Round to nearest integer and check if it's the same\n                    rounded_data = np.round(sample_data)\n                    is_integer_values = np.allclose(\n                        sample_data, rounded_data, rtol=1e-10\n                    )\n\n                    if is_integer_values and max_value &lt;= 65535 and min_value &gt;= 0:\n                        logger.info(\n                            f\"Float data contains only integer values: [{min_value}, {max_value}]\"\n                        )\n                        logger.info(\"Converting to uint16 for count data\")\n                        return True, \"uint16\"\n                    else:\n                        logger.info(\n                            f\"Float data contains fractional values: [{min_value}, {max_value}]\"\n                        )\n                        logger.info(\"Keeping as float32 for normalized/float data\")\n                        return False, \"float32\"\n                else:\n                    logger.info(\n                        f\"Warning: Original integer values range [{min_value}, {max_value}] exceeds uint16 range [0, 65535]\"\n                    )\n                    logger.info(\"Falling back to float32\")\n                    return False, \"float32\"\n            else:\n                # Fallback to checking processed data\n                sample_size = min(100000, reader.n_obs)\n                sample_chunks = list(reader.iter_chunks(chunk_size=sample_size))\n\n                if sample_chunks:\n                    sample_chunk = sample_chunks[0][0]\n\n                    # Handle different data types from chunked reader\n                    if hasattr(sample_chunk, \"column\"):  # PyArrow Table\n                        # Get the value column from the PyArrow table\n                        value_column = sample_chunk.column(\"value\")\n                        sample_data = value_column.to_numpy()\n                    elif sparse.issparse(sample_chunk):\n                        sample_data = sample_chunk.data\n                    else:\n                        sample_data = sample_chunk.flatten()\n\n                    # Check if data is integer or float\n                    is_integer = np.issubdtype(sample_data.dtype, np.integer)\n                    max_value = np.max(sample_data)\n                    min_value = np.min(sample_data)\n\n                    if is_integer and max_value &lt;= 65535 and min_value &gt;= 0:\n                        logger.info(\n                            f\"Integer expression values fit in uint16 range: [{min_value}, {max_value}]\"\n                        )\n                        logger.info(\"Using uint16 for integer count data\")\n                        return True, \"uint16\"\n                    elif not is_integer:\n                        # Check if float data contains only integer values\n                        # Round to nearest integer and check if it's the same\n                        rounded_data = np.round(sample_data)\n                        is_integer_values = np.allclose(\n                            sample_data, rounded_data, rtol=1e-10\n                        )\n\n                        if is_integer_values and max_value &lt;= 65535 and min_value &gt;= 0:\n                            logger.info(\n                                f\"Float data contains only integer values: [{min_value}, {max_value}]\"\n                            )\n                            logger.info(\"Converting to uint16 for count data\")\n                            return True, \"uint16\"\n                        else:\n                            logger.info(\n                                f\"Float data contains fractional values: [{min_value}, {max_value}]\"\n                            )\n                            logger.info(\"Keeping as float32 for normalized/float data\")\n                            return False, \"float32\"\n                    else:\n                        logger.info(\n                            f\"Warning: Integer values range [{min_value}, {max_value}] exceeds uint16 range [0, 65535]\"\n                        )\n                        logger.info(\"Falling back to float32\")\n                        return False, \"float32\"\n\n        logger.info(\"Data validation passed - using optimized data types\")\n        return True, \"uint16\"  # Default to uint16 for integer data\n\n    def _validate_optimized_dtypes_anndata(self, adata):\n        \"\"\"Validate that AnnData object's expression data fits in optimized data types and determine appropriate value type\"\"\"\n        if not self.use_optimized_dtypes:\n            return True, \"float32\"\n\n        logger.info(\n            \"Validating AnnData object's expression data fits in optimized data types...\"\n        )\n\n        # Check if gene count fits in uint16 (0-65535)\n        if adata.n_vars &gt; 65535:\n            logger.info(f\"Warning: {adata.n_vars:,} genes exceeds uint16 limit\")\n            logger.info(\"Falling back to standard data types\")\n            return False, \"float32\"\n\n        # Check if cell count fits in uint32 (0-4,294,967,295)\n        if adata.n_obs &gt; 4294967295:\n            logger.warning(\n                f\"{adata.n_obs:,} cells exceeds uint32 limit (4,294,967,295)\"\n            )\n            logger.info(\"Falling back to standard data types\")\n            return False, \"float32\"\n\n        # Sample some values to check data type and range\n        logger.info(\"Sampling expression values to determine data type...\")\n        # Handle backed mode where X might not have .data attribute\n        if hasattr(adata.X, \"data\"):\n            sample_data = adata.X.data[:100000]\n        else:\n            # For backed mode, convert to COO and get data\n            if hasattr(adata.X, \"tocoo\"):\n                coo = adata.X.tocoo()\n            else:\n                try:\n                    coo = sparse.coo_matrix(adata.X)\n                except ValueError:\n                    # If dtype is not supported, try to convert to a supported type\n                    logger.warning(\n                        \"adata.X has unsupported dtype, attempting conversion\"\n                    )\n                    try:\n                        coo = sparse.coo_matrix(adata.X.astype(np.float32))\n                    except Exception:\n                        logger.warning(\n                            \"Could not convert adata.X, using fallback method\"\n                        )\n                        # For backed mode, we might need to load a small sample\n                        sample_size = min(1000, adata.n_obs)\n                        sample_adata = adata[:sample_size, :]\n                        if sparse.issparse(sample_adata.X):\n                            coo = sample_adata.X.tocoo()\n                        else:\n                            coo = sparse.coo_matrix(sample_adata.X)\n            sample_data = coo.data[:100000]\n\n        # Check if data is integer or float\n        is_integer = np.issubdtype(sample_data.dtype, np.integer)\n        max_value = np.max(sample_data)\n        min_value = np.min(sample_data)\n\n        if is_integer and max_value &lt;= 65535 and min_value &gt;= 0:\n            logger.info(\n                f\"Integer expression values fit in uint16 range: [{min_value}, {max_value}]\"\n            )\n            logger.info(\"Using uint16 for integer count data\")\n            return True, \"uint16\"\n        elif not is_integer:\n            logger.info(f\"Float expression values detected: [{min_value}, {max_value}]\")\n            logger.info(\"Using float32 for normalized/float data\")\n            return True, \"float32\"\n        else:\n            logger.info(\n                f\"Warning: Integer values range [{min_value}, {max_value}] exceeds uint16 range\"\n            )\n            logger.info(\"Falling back to float32\")\n            return False, \"float32\"\n\n        logger.info(\n            \"AnnData object's expression data validation passed - using optimized data types\"\n        )\n        return True, \"uint16\"  # Default to uint16 for integer data\n\n    def _compact_dataset(self, output_path: str):\n        \"\"\"Compact the dataset to optimize storage after writing\"\"\"\n        logger.info(\"Compacting dataset for optimal storage...\")\n\n        try:\n            # Compact expression table\n            expression_path = f\"{output_path}/expression.lance\"\n            if self._path_exists(expression_path):\n                logger.info(\"  Compacting expression table...\")\n                dataset = lance.dataset(expression_path)\n                dataset.optimize.compact_files(\n                    target_rows_per_fragment=1024 * 1024\n                )  # 1M rows per fragment\n                logger.info(\"  Expression table compacted!\")\n            else:\n                logger.warning(\"  Expression table not found, skipping compaction\")\n\n            # Compact metadata tables\n            for table_name in [\"cells\", \"genes\"]:\n                table_path = f\"{output_path}/{table_name}.lance\"\n                if self._path_exists(table_path):\n                    logger.info(f\"  Compacting {table_name} table...\")\n                    dataset = lance.dataset(table_path)\n                    dataset.optimize.compact_files(\n                        target_rows_per_fragment=100000\n                    )  # 100K rows per fragment for metadata\n                    logger.info(f\"  {table_name} table compacted!\")\n                else:\n                    logger.warning(\n                        f\"  {table_name} table not found, skipping compaction\"\n                    )\n\n            logger.info(\"Dataset compaction complete!\")\n        except Exception as e:\n            logger.error(f\"Error during dataset compaction: {e}\")\n            logger.error(\n                \"Dataset may be in an inconsistent state. Consider recreating without compaction.\"\n            )\n            raise\n\n    def _get_expression_schema(self, value_type=\"uint16\"):\n        \"\"\"Get the schema for expression table\"\"\"\n        # Determine the appropriate value type\n        if value_type == \"uint16\":\n            value_pa_type = pa.uint16()\n        elif value_type == \"float32\":\n            value_pa_type = pa.float32()\n        else:\n            raise ValueError(f\"Unsupported value type: {value_type}\")\n\n        if self.optimize_storage:\n            # Only store integer IDs for maximum storage efficiency\n            if self.use_optimized_dtypes:\n                # Use optimized data types for better compression\n                return pa.schema(\n                    [\n                        (\"cell_integer_id\", pa.uint32()),\n                        (\"gene_integer_id\", pa.uint16()),\n                        (\"value\", value_pa_type),\n                    ]\n                )\n            else:\n                # Use standard data types\n                return pa.schema(\n                    [\n                        (\"cell_integer_id\", pa.int32()),\n                        (\"gene_integer_id\", pa.int32()),\n                        (\"value\", value_pa_type),\n                    ]\n                )\n        else:\n            # Store both string and integer IDs for compatibility\n            if self.use_optimized_dtypes:\n                return pa.schema(\n                    [\n                        (\"cell_id\", pa.string()),\n                        (\"gene_id\", pa.string()),\n                        (\"cell_integer_id\", pa.uint32()),\n                        (\"gene_integer_id\", pa.uint16()),\n                        (\"value\", value_pa_type),\n                    ]\n                )\n            else:\n                return pa.schema(\n                    [\n                        (\"cell_id\", pa.string()),\n                        (\"gene_id\", pa.string()),\n                        (\"cell_integer_id\", pa.int32()),\n                        (\"gene_integer_id\", pa.int32()),\n                        (\"value\", value_pa_type),\n                    ]\n                )\n\n    def _create_id_mapping(self, entity_ids, entity_type: str) -&gt; list[dict[str, Any]]:\n        \"\"\"Create mapping from original entity IDs to integer indices\"\"\"\n        # Direct assignment using pandas operations\n        df = pd.DataFrame()\n        df[f\"{entity_type}_id\"] = pd.Series(entity_ids).astype(str)\n        df[\"integer_id\"] = range(len(entity_ids))\n        return df.to_dict(orient=\"records\")\n\n    def _sparse_to_coo_table(\n        self,\n        sparse_matrix,\n        cell_ids,\n        gene_ids,\n        value_type=\"uint16\",\n    ):\n        \"\"\"Convert scipy sparse matrix to COO format PyArrow table with integer IDs\"\"\"\n        # Handle backed mode where tocoo() might not be available\n        if hasattr(sparse_matrix, \"tocoo\"):\n            coo_matrix = sparse_matrix.tocoo()\n        else:\n            # For backed mode, convert to COO using scipy\n            try:\n                coo_matrix = sparse.coo_matrix(sparse_matrix)\n            except (ValueError, AttributeError):\n                # If dtype is not supported or object doesn't have expected methods\n                logger.warning(\n                    \"sparse_matrix has unsupported dtype or missing methods, using fallback\"\n                )\n                # For CSRDataset objects, we need to handle them differently\n                # Let's try to get the data in a way that works for backed mode\n                try:\n                    # Try to get the data directly from the backed object\n                    if hasattr(sparse_matrix, \"data\"):\n                        data = sparse_matrix.data\n                        if hasattr(sparse_matrix, \"indices\") and hasattr(\n                            sparse_matrix, \"indptr\"\n                        ):\n                            # It's a CSR matrix\n                            indices = sparse_matrix.indices\n                            indptr = sparse_matrix.indptr\n                            # Convert to COO\n                            row_indices = []\n                            col_indices = []\n                            for i in range(len(indptr) - 1):\n                                start = indptr[i]\n                                end = indptr[i + 1]\n                                row_indices.extend([i] * (end - start))\n                                col_indices.extend(indices[start:end])\n                            coo_matrix = sparse.coo_matrix(\n                                (data, (row_indices, col_indices)),\n                                shape=sparse_matrix.shape,\n                            )\n                        else:\n                            # Fallback: try to convert to numpy array first\n                            try:\n                                dense_array = np.array(sparse_matrix)\n                                coo_matrix = sparse.coo_matrix(dense_array)\n                            except Exception:\n                                # Last resort: create empty COO matrix\n                                logger.warning(\n                                    \"Could not convert sparse_matrix, creating empty COO matrix\"\n                                )\n                                coo_matrix = sparse.coo_matrix(sparse_matrix.shape)\n                    else:\n                        # Try to convert to numpy array first\n                        try:\n                            dense_array = np.array(sparse_matrix)\n                            coo_matrix = sparse.coo_matrix(dense_array)\n                        except Exception:\n                            # Last resort: create empty COO matrix\n                            logger.warning(\n                                \"Could not convert sparse_matrix, creating empty COO matrix\"\n                            )\n                            coo_matrix = sparse.coo_matrix(sparse_matrix.shape)\n                except Exception as e:\n                    logger.warning(f\"All conversion methods failed: {e}\")\n                    # Last resort: create empty COO matrix\n                    coo_matrix = sparse.coo_matrix(sparse_matrix.shape)\n\n        logger.info(f\"Processing {len(coo_matrix.data):,} non-zero elements...\")\n\n        # Create integer ID arrays for efficient range queries\n        if self.use_optimized_dtypes:\n            cell_integer_id_array = coo_matrix.row.astype(np.uint32)\n            gene_integer_id_array = coo_matrix.col.astype(np.uint16)\n            # Convert values based on the determined type\n            if value_type == \"uint16\":\n                value_array = coo_matrix.data.astype(np.uint16)\n                value_dtype = np.uint16\n                value_pa_type = pa.uint16()\n            elif value_type == \"float32\":\n                value_array = coo_matrix.data.astype(np.float32)\n                value_dtype = np.float32\n                value_pa_type = pa.float32()\n            else:\n                raise ValueError(f\"Unsupported value type: {value_type}\")\n        else:\n            cell_integer_id_array = coo_matrix.row.astype(np.int32)\n            gene_integer_id_array = coo_matrix.col.astype(np.int32)\n            # Expression values - use the determined type\n            if value_type == \"uint16\":\n                value_array = coo_matrix.data.astype(np.uint16)\n                value_dtype = np.uint16\n                value_pa_type = pa.uint16()\n            elif value_type == \"float32\":\n                value_array = coo_matrix.data.astype(np.float32)\n                value_dtype = np.float32\n                value_pa_type = pa.float32()\n            else:\n                raise ValueError(f\"Unsupported value type: {value_type}\")\n\n        # Create string ID arrays\n        cell_id_array = np.array(cell_ids)[coo_matrix.row].astype(str)\n        gene_id_array = np.array(gene_ids)[coo_matrix.col].astype(str)\n\n        # Check for nulls in string arrays\n        if bool(np.any(pd.isnull(cell_id_array))) or bool(\n            np.any(pd.isnull(gene_id_array))\n        ):\n            raise ValueError(\"Null values found in cell_id or gene_id arrays!\")\n\n        # Create table based on storage optimization\n        if self.optimize_storage:\n            # Only store integer IDs for maximum storage efficiency\n            if self.use_optimized_dtypes:\n                table = pa.table(\n                    {\n                        \"cell_integer_id\": pa.array(\n                            cell_integer_id_array, type=pa.uint32()\n                        ),\n                        \"gene_integer_id\": pa.array(\n                            gene_integer_id_array, type=pa.uint16()\n                        ),\n                        \"value\": pa.array(\n                            value_array.astype(value_dtype), type=value_pa_type\n                        ),\n                    }\n                )\n            else:\n                table = pa.table(\n                    {\n                        \"cell_integer_id\": pa.array(\n                            cell_integer_id_array, type=pa.int32()\n                        ),\n                        \"gene_integer_id\": pa.array(\n                            gene_integer_id_array, type=pa.int32()\n                        ),\n                        \"value\": pa.array(\n                            value_array.astype(value_dtype), type=value_pa_type\n                        ),\n                    }\n                )\n        else:\n            # Store both string and integer IDs for compatibility\n            if self.use_optimized_dtypes:\n                table = pa.table(\n                    {\n                        \"cell_id\": pa.array(cell_id_array, type=pa.string()),\n                        \"gene_id\": pa.array(gene_id_array, type=pa.string()),\n                        \"cell_integer_id\": pa.array(\n                            cell_integer_id_array, type=pa.uint32()\n                        ),\n                        \"gene_integer_id\": pa.array(\n                            gene_integer_id_array, type=pa.uint16()\n                        ),\n                        \"value\": pa.array(\n                            value_array.astype(value_dtype), type=value_pa_type\n                        ),\n                    }\n                )\n            else:\n                table = pa.table(\n                    {\n                        \"cell_id\": pa.array(cell_id_array, type=pa.string()),\n                        \"gene_id\": pa.array(gene_id_array, type=pa.string()),\n                        \"cell_integer_id\": pa.array(\n                            cell_integer_id_array, type=pa.int32()\n                        ),\n                        \"gene_integer_id\": pa.array(\n                            gene_integer_id_array, type=pa.int32()\n                        ),\n                        \"value\": pa.array(\n                            value_array.astype(value_dtype), type=value_pa_type\n                        ),\n                    }\n                )\n\n        # Validate schema\n        if self.optimize_storage:\n            if self.use_optimized_dtypes:\n                expected_types = {\n                    \"cell_integer_id\": pa.uint32(),\n                    \"gene_integer_id\": pa.uint16(),\n                    \"value\": value_pa_type,\n                }\n            else:\n                expected_types = {\n                    \"cell_integer_id\": pa.int32(),\n                    \"gene_integer_id\": pa.int32(),\n                    \"value\": value_pa_type,\n                }\n        else:\n            if self.use_optimized_dtypes:\n                expected_types = {\n                    \"cell_id\": pa.string(),\n                    \"gene_id\": pa.string(),\n                    \"cell_integer_id\": pa.uint32(),\n                    \"gene_integer_id\": pa.uint16(),\n                    \"value\": value_pa_type,\n                }\n            else:\n                expected_types = {\n                    \"cell_id\": pa.string(),\n                    \"gene_id\": pa.string(),\n                    \"cell_integer_id\": pa.int32(),\n                    \"gene_integer_id\": pa.int32(),\n                    \"value\": value_pa_type,\n                }\n\n        # Validate schema\n        for col, expected_type in expected_types.items():\n            assert table.schema.field(col).type == expected_type, (\n                f\"{col} is not {expected_type} type!\"\n            )\n            assert table.column(col).null_count == 0, f\"Nulls found in {col} column!\"\n\n        return table\n\n    def _convert_layers(\n        self,\n        layers: dict[str, Any],\n        output_path: str,\n        cell_ids: Any,\n        gene_ids: Any,\n        value_type: str = \"uint16\",\n    ) -&gt; list[str]:\n        \"\"\"\n        Convert AnnData layers to layers.lance table in wide format.\n\n        Args:\n            layers: Dictionary of layer names to sparse matrices\n            output_path: Output SLAF directory path\n            cell_ids: Cell IDs (index or array)\n            gene_ids: Gene IDs (index or array)\n            value_type: Value type for layer data (\"uint16\" or \"float32\")\n\n        Returns:\n            List of layer names that were successfully converted\n        \"\"\"\n        logger.info(\"Converting layers to wide format...\")\n\n        # Get base structure from expression table (all cell-gene pairs)\n        expression_path = f\"{output_path}/expression.lance\"\n        expression_dataset = lance.dataset(expression_path)\n\n        # Read all (cell_integer_id, gene_integer_id) pairs from expression\n        expression_df = (\n            pl.scan_pyarrow_dataset(expression_dataset)\n            .select([\"cell_integer_id\", \"gene_integer_id\"])\n            .unique()\n            .collect()\n        )\n\n        # Start with base table\n        layers_table = expression_df\n\n        layer_names = []\n        for layer_name, layer_matrix in layers.items():\n            logger.info(f\"Converting layer '{layer_name}'...\")\n\n            # Validate layer shape matches expression\n            if layer_matrix.shape != (len(cell_ids), len(gene_ids)):\n                logger.warning(\n                    f\"Layer '{layer_name}' shape {layer_matrix.shape} doesn't match \"\n                    f\"expression shape ({len(cell_ids)}, {len(gene_ids)}). Skipping.\"\n                )\n                continue\n\n            # Convert to COO format using existing method\n            layer_coo_table = self._sparse_to_coo_table(\n                sparse_matrix=layer_matrix,\n                cell_ids=cell_ids,\n                gene_ids=gene_ids,\n                value_type=value_type,\n            )\n\n            # Convert to Polars DataFrame\n            layer_df_raw = pl.from_arrow(layer_coo_table)\n            assert isinstance(layer_df_raw, pl.DataFrame), (\n                \"Expected DataFrame from Arrow table\"\n            )\n            layer_df = layer_df_raw.select(\n                [\"cell_integer_id\", \"gene_integer_id\", \"value\"]\n            )\n\n            # Rename value column to layer name for wide format\n            layer_df = layer_df.rename({\"value\": layer_name})\n\n            # Left join to add layer column (nullable for sparse data)\n            layers_table = layers_table.join(\n                layer_df,\n                on=[\"cell_integer_id\", \"gene_integer_id\"],\n                how=\"left\",\n            )\n\n            layer_names.append(layer_name)\n\n        if not layer_names:\n            logger.warning(\"No valid layers to convert\")\n            return []\n\n        # Convert back to PyArrow for Lance\n        combined_layers = layers_table.to_arrow()\n\n        # Write to layers.lance\n        layers_path = f\"{output_path}/layers.lance\"\n        logger.info(f\"Writing layers table to {layers_path}...\")\n\n        # Use same settings as expression table\n        lance.write_dataset(\n            combined_layers,\n            layers_path,\n            mode=\"overwrite\",\n            max_rows_per_file=self.max_rows_per_file,\n            enable_v2_manifest_paths=self.enable_v2_manifest,\n            data_storage_version=\"2.1\",\n        )\n\n        logger.info(f\"Successfully converted {len(layer_names)} layers: {layer_names}\")\n        return layer_names\n\n    def _convert_obsm(\n        self,\n        obsm: dict[str, np.ndarray],\n        output_path: str,\n        cell_ids: Any,\n        cell_id_mapping: list[dict[str, Any]] | None = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Convert AnnData obsm to FixedSizeListArray columns in cells.lance.\n\n        Args:\n            obsm: Dictionary of obsm key names to numpy arrays (shape: n_obs \u00d7 n_dims)\n            output_path: Output SLAF directory path\n            cell_ids: Cell IDs (index or array)\n            cell_id_mapping: Optional integer ID mapping for cells\n\n        Returns:\n            List of obsm keys that were successfully converted\n        \"\"\"\n        import lance\n        import numpy as np\n        import pyarrow as pa\n\n        logger.info(\"Converting obsm embeddings to FixedSizeListArray columns...\")\n\n        # Load existing cells.lance table\n        cells_path = f\"{output_path}/cells.lance\"\n        cells_dataset = lance.dataset(cells_path)\n\n        obsm_keys = []\n        obsm_dimensions = {}\n\n        for key, embedding in obsm.items():\n            logger.info(f\"Converting obsm '{key}'...\")\n\n            # Validate embedding shape\n            if embedding.shape[0] != len(cell_ids):\n                logger.warning(\n                    f\"obsm '{key}' shape {embedding.shape[0]} doesn't match \"\n                    f\"n_obs {len(cell_ids)}. Skipping.\"\n                )\n                continue\n\n            # Convert to numpy array\n            embedding = np.asarray(embedding)\n            n_dims = embedding.shape[1] if len(embedding.shape) &gt; 1 else 1\n\n            # Get cell integer IDs in order\n            if cell_id_mapping and self.use_integer_keys:\n                # Create mapping dict for fast lookup\n                # cell_id_mapping has keys: \"cell_id\" and \"integer_id\"\n                id_map = {\n                    item[\"cell_id\"]: item[\"integer_id\"] for item in cell_id_mapping\n                }\n                integer_ids = np.array(\n                    [id_map.get(str(cid), i) for i, cid in enumerate(cell_ids)],\n                    dtype=np.uint32,\n                )\n            else:\n                integer_ids = np.arange(len(cell_ids), dtype=np.uint32)\n\n            # Create FixedSizeListArray from embedding\n            value_dtype = pa.float32() if embedding.dtype.kind == \"f\" else pa.float32()\n            flat_values = pa.array(embedding.flatten(), type=value_dtype)\n            vector_array = pa.FixedSizeListArray.from_arrays(flat_values, n_dims)\n\n            # Create table with integer IDs and vector column\n            column_table = pa.table(\n                {\n                    \"cell_integer_id\": pa.array(integer_ids, type=pa.uint32()),\n                    key: vector_array,\n                }\n            )\n\n            # Add column to cells.lance using add_columns with UDF\n            # Create factory function to avoid loop variable binding issues\n            def create_obsm_udf(obsm_key, obsm_column_table):\n                \"\"\"Factory function to create UDF with captured variables\"\"\"\n\n                def add_obsm_column_udf(batch):\n                    \"\"\"UDF to add obsm column by joining batch with embedding data\"\"\"\n                    import polars as pl\n\n                    batch_df = pl.from_arrow(batch)\n                    column_df = pl.from_arrow(obsm_column_table)\n\n                    # Join to get the embedding column\n                    result_df = batch_df.join(\n                        column_df, on=[\"cell_integer_id\"], how=\"left\"\n                    )\n                    embedding_chunked = (\n                        result_df.select([obsm_key]).to_arrow().column(0)\n                    )\n                    embedding_array = embedding_chunked.combine_chunks()\n\n                    return pa.RecordBatch.from_arrays(\n                        [embedding_array], names=[obsm_key]\n                    )\n\n                return add_obsm_column_udf\n\n            cells_dataset.add_columns(create_obsm_udf(key, column_table))\n\n            # Reload dataset to ensure consistency\n            cells_dataset = lance.dataset(cells_path)\n\n            obsm_keys.append(key)\n            obsm_dimensions[key] = n_dims\n\n        if not obsm_keys:\n            logger.warning(\"No valid obsm embeddings to convert\")\n            return []\n\n        logger.info(\n            f\"Successfully converted {len(obsm_keys)} obsm embeddings: {obsm_keys}\"\n        )\n        return obsm_keys\n\n    def _convert_varm(\n        self,\n        varm: dict[str, np.ndarray],\n        output_path: str,\n        gene_ids: Any,\n        gene_id_mapping: list[dict[str, Any]] | None = None,\n    ) -&gt; list[str]:\n        \"\"\"\n        Convert AnnData varm to FixedSizeListArray columns in genes.lance.\n\n        Args:\n            varm: Dictionary of varm key names to numpy arrays (shape: n_vars \u00d7 n_dims)\n            output_path: Output SLAF directory path\n            gene_ids: Gene IDs (index or array)\n            gene_id_mapping: Optional integer ID mapping for genes\n\n        Returns:\n            List of varm keys that were successfully converted\n        \"\"\"\n        import lance\n        import numpy as np\n        import pyarrow as pa\n\n        logger.info(\"Converting varm embeddings to FixedSizeListArray columns...\")\n\n        # Load existing genes.lance table\n        genes_path = f\"{output_path}/genes.lance\"\n        genes_dataset = lance.dataset(genes_path)\n\n        varm_keys = []\n        varm_dimensions = {}\n\n        for key, embedding in varm.items():\n            logger.info(f\"Converting varm '{key}'...\")\n\n            # Validate embedding shape\n            if embedding.shape[0] != len(gene_ids):\n                logger.warning(\n                    f\"varm '{key}' shape {embedding.shape[0]} doesn't match \"\n                    f\"n_vars {len(gene_ids)}. Skipping.\"\n                )\n                continue\n\n            # Convert to numpy array\n            embedding = np.asarray(embedding)\n            n_dims = embedding.shape[1] if len(embedding.shape) &gt; 1 else 1\n\n            # Get gene integer IDs in order\n            if gene_id_mapping and self.use_integer_keys:\n                # Create mapping dict for fast lookup\n                # gene_id_mapping has keys: \"gene_id\" and \"integer_id\"\n                id_map = {\n                    item[\"gene_id\"]: item[\"integer_id\"] for item in gene_id_mapping\n                }\n                integer_ids = np.array(\n                    [id_map.get(str(gid), i) for i, gid in enumerate(gene_ids)],\n                    dtype=np.uint16,\n                )\n            else:\n                integer_ids = np.arange(len(gene_ids), dtype=np.uint16)\n\n            # Create FixedSizeListArray from embedding\n            value_dtype = pa.float32() if embedding.dtype.kind == \"f\" else pa.float32()\n            flat_values = pa.array(embedding.flatten(), type=value_dtype)\n            vector_array = pa.FixedSizeListArray.from_arrays(flat_values, n_dims)\n\n            # Create table with integer IDs and vector column\n            column_table = pa.table(\n                {\n                    \"gene_integer_id\": pa.array(integer_ids, type=pa.uint16()),\n                    key: vector_array,\n                }\n            )\n\n            # Add column to genes.lance using add_columns with UDF\n            # Create factory function to avoid loop variable binding issues\n            def create_varm_udf(varm_key, varm_column_table):\n                \"\"\"Factory function to create UDF with captured variables\"\"\"\n\n                def add_varm_column_udf(batch):\n                    \"\"\"UDF to add varm column by joining batch with embedding data\"\"\"\n                    import polars as pl\n\n                    batch_df = pl.from_arrow(batch)\n                    column_df = pl.from_arrow(varm_column_table)\n\n                    # Join to get the embedding column\n                    result_df = batch_df.join(\n                        column_df, on=[\"gene_integer_id\"], how=\"left\"\n                    )\n                    embedding_chunked = (\n                        result_df.select([varm_key]).to_arrow().column(0)\n                    )\n                    embedding_array = embedding_chunked.combine_chunks()\n\n                    return pa.RecordBatch.from_arrays(\n                        [embedding_array], names=[varm_key]\n                    )\n\n                return add_varm_column_udf\n\n            genes_dataset.add_columns(create_varm_udf(key, column_table))\n\n            # Reload dataset to ensure consistency\n            genes_dataset = lance.dataset(genes_path)\n\n            varm_keys.append(key)\n            varm_dimensions[key] = n_dims\n\n        if not varm_keys:\n            logger.warning(\"No valid varm embeddings to convert\")\n            return []\n\n        logger.info(\n            f\"Successfully converted {len(varm_keys)} varm embeddings: {varm_keys}\"\n        )\n        return varm_keys\n\n    def _convert_uns(self, uns: dict, output_path: str):\n        \"\"\"\n        Convert AnnData uns to uns.json file.\n\n        Args:\n            uns: Dictionary of unstructured metadata\n            output_path: Output SLAF directory path\n        \"\"\"\n        import json\n\n        import numpy as np\n        import pandas as pd\n\n        logger.info(\"Converting uns metadata to uns.json...\")\n\n        # Serialize uns to JSON-compatible format\n        def json_serialize(value):\n            \"\"\"Convert value to JSON-serializable format\"\"\"\n            if isinstance(value, np.ndarray):\n                return value.tolist()\n            elif isinstance(value, np.integer | np.floating):\n                return value.item()\n            elif isinstance(value, pd.Series):\n                return value.tolist()\n            elif isinstance(value, dict):\n                return {k: json_serialize(v) for k, v in value.items()}\n            elif isinstance(value, list):\n                return [json_serialize(item) for item in value]\n            return value\n\n        uns_serialized = {k: json_serialize(v) for k, v in uns.items()}\n\n        # Write to uns.json\n        uns_path = f\"{output_path}/uns.json\"\n        with self._open_file(uns_path, \"w\") as f:\n            json.dump(uns_serialized, f, indent=2)\n\n        logger.info(\n            f\"Successfully converted uns metadata with {len(uns_serialized)} keys\"\n        )\n\n    def _check_layer_consistency(\n        self, input_files: list[str], input_format: str\n    ) -&gt; list[str] | None:\n        \"\"\"\n        Check if layers are consistent across all input files.\n\n        Args:\n            input_files: List of input file paths\n            input_format: Format of input files\n\n        Returns:\n            List of layer names if consistent, None if inconsistent\n        \"\"\"\n        if input_format not in [\"h5ad\", \"10x_h5\"]:\n            # Only h5ad and 10x_h5 formats support layers\n            return None\n\n        layer_sets = []\n        for file_path in input_files:\n            try:\n                # Use chunked reader to check layers\n                with create_chunked_reader(\n                    file_path,\n                    chunk_size=self.chunk_size,\n                    collection_name=self.tiledb_collection_name,\n                ) as reader:\n                    if hasattr(reader, \"adata\") and reader.adata is not None:\n                        if hasattr(reader.adata, \"layers\") and reader.adata.layers:\n                            layer_names = sorted(reader.adata.layers.keys())\n                            layer_sets.append(set(layer_names))\n                        else:\n                            # No layers in this file\n                            layer_sets.append(set())\n                    else:\n                        # Can't check layers for this file type\n                        return None\n            except Exception as e:\n                logger.warning(\n                    f\"Could not check layers in {file_path}: {e}. Skipping layer consistency check.\"\n                )\n                return None\n\n        if not layer_sets:\n            # No layers in any file\n            return []\n\n        # Check if all files have the same layers\n        first_set = layer_sets[0]\n        if all(layer_set == first_set for layer_set in layer_sets):\n            # All files have the same layers\n            return sorted(first_set)\n        else:\n            # Layers are inconsistent\n            logger.warning(\n                f\"Layer inconsistency detected across files. \"\n                f\"File 1 has layers: {sorted(layer_sets[0])}, \"\n                f\"but other files have different layers.\"\n            )\n            return None\n\n    def _create_metadata_table(\n        self,\n        df: pd.DataFrame,\n        entity_id_col: str,\n        integer_mapping: list[dict[str, Any]] | None = None,\n    ) -&gt; pa.Table:\n        result_df = df.copy()\n        # Assign entity ID column using index directly to avoid misalignment\n        result_df[entity_id_col] = df.index.astype(str)\n        if integer_mapping and self.use_integer_keys:\n            integer_id_col = f\"{entity_id_col.replace('_id', '')}_integer_id\"\n            result_df[integer_id_col] = range(len(df))\n        result_df = result_df.where(pd.notnull(result_df), None)\n        # Convert all categorical/object columns to string for Arrow compatibility\n        for col in result_df.columns:\n            if (\n                isinstance(result_df[col].dtype, pd.CategoricalDtype)\n                or result_df[col].dtype == object\n            ):\n                result_df[col] = result_df[col].astype(str)\n        # Ensure all ID columns are string and non-null\n        result_df[entity_id_col] = result_df[entity_id_col].astype(str)\n        if bool(result_df[entity_id_col].isnull().any()):\n            raise ValueError(f\"Null values found in {entity_id_col} column!\")\n\n        # Reset index to avoid __index_level_0__ column in Arrow table\n        result_df = result_df.reset_index(drop=True)\n\n        # Sanitize column names: replace '.' with '_' for Lance compatibility\n        # Lance doesn't allow '.' in field names\n        column_mapping = {\n            col: col.replace(\".\", \"_\") for col in result_df.columns if \".\" in col\n        }\n        if column_mapping:\n            result_df = result_df.rename(columns=column_mapping)\n            logger.debug(f\"Sanitized column names: {column_mapping}\")\n\n        table = pa.table(result_df)\n        table = self._cast_table_string_columns_to_utf8(table, [entity_id_col])\n        return table\n\n    def _cast_table_string_columns_to_utf8(\n        self, table: pa.Table, column_names: list[str]\n    ) -&gt; pa.Table:\n        \"\"\"Cast large_string columns to string (utf8) for lance schema consistency.\"\"\"\n        for name in column_names:\n            if table.schema.get_field_index(name) &lt; 0:\n                continue\n            col_idx = table.schema.get_field_index(name)\n            field = table.schema.field(col_idx)\n            if field.type == pa.large_string():\n                arr = table.column(name)\n                table = table.set_column(col_idx, name, arr.cast(pa.string()))\n        return table\n\n    def _write_lance_tables(\n        self, output_path: str, table_configs: list[tuple[str, pa.Table]]\n    ):\n        \"\"\"Write multiple Lance tables with consistent naming\"\"\"\n        for table_name, table in table_configs:\n            table_path = f\"{output_path}/{table_name}.lance\"\n\n            # Write table with basic settings (using max_rows_per_file for large fragments)\n            if table_name == \"expression\":\n                lance.write_dataset(\n                    table,\n                    table_path,\n                    max_rows_per_file=self.max_rows_per_file,\n                    enable_v2_manifest_paths=self.enable_v2_manifest,\n                    data_storage_version=\"2.1\",\n                )\n            else:\n                lance.write_dataset(\n                    table,\n                    table_path,\n                    enable_v2_manifest_paths=self.enable_v2_manifest,\n                    data_storage_version=\"2.1\",\n                )\n\n        # Create indices after all tables are written (if enabled)\n        if self.create_indices:\n            self._create_indices(output_path)\n\n    def _create_indices(self, output_path: str):\n        \"\"\"Create optimal indices for SLAF tables with column existence checks\"\"\"\n        logger.info(\"Creating indices for optimal query performance...\")\n\n        # Define desired indices for each table\n        # For small datasets, create fewer indices to reduce overhead\n        table_indices = {\n            \"cells\": [\n                \"cell_id\",\n                \"cell_integer_id\",\n                # Only create metadata indices for larger datasets\n            ],\n            \"genes\": [\"gene_id\", \"gene_integer_id\"],\n            \"expression\": [\n                \"cell_integer_id\",\n                \"gene_integer_id\",\n            ],  # Only integer indices for efficiency\n        }\n\n        # Create indices for each table\n        for table_name, desired_columns in table_indices.items():\n            table_path = f\"{output_path}/{table_name}.lance\"\n            if self._path_exists(table_path):\n                dataset = lance.dataset(table_path)\n                schema = dataset.schema\n\n                for column in desired_columns:\n                    if column in schema.names:\n                        logger.info(f\"  Creating index on {table_name}.{column}\")\n                        dataset.create_scalar_index(column, \"BTREE\")\n\n        logger.info(\"Index creation complete!\")\n\n    def _compute_cell_start_indices(self, reader, obs_df: pd.DataFrame) -&gt; list[int]:\n        \"\"\"Compute cell start indices during metadata creation\"\"\"\n        # Check for existing n_genes or gene_count column\n        if \"n_genes\" in obs_df.columns:\n            logger.info(\"Using existing n_genes column for cell start indices\")\n            gene_counts = obs_df[\"n_genes\"].to_numpy()\n        elif \"gene_count\" in obs_df.columns:\n            logger.info(\"Using existing gene_count column for cell start indices\")\n            gene_counts = obs_df[\"gene_count\"].to_numpy()\n        elif \"n_genes_by_counts\" in obs_df.columns:\n            logger.info(\n                \"Using existing n_genes_by_counts column for cell start indices\"\n            )\n            gene_counts = obs_df[\"n_genes_by_counts\"].to_numpy()\n        else:\n            # Calculate from expression data\n            logger.info(\"Calculating gene counts from expression data...\")\n\n            # Collect all chunk counts\n            all_chunk_gene_counts = []\n            for chunk_table, _obs_slice in reader.iter_chunks(\n                chunk_size=self.chunk_size\n            ):\n                # Count genes per cell in this chunk using Polars groupby\n                chunk_df = pl.from_arrow(chunk_table)\n                assert isinstance(chunk_df, pl.DataFrame)\n                if len(chunk_df) &gt; 0:\n                    chunk_gene_counts = chunk_df.group_by(\"cell_integer_id\").agg(\n                        pl.len().alias(\"count\")\n                    )\n                    all_chunk_gene_counts.append(chunk_gene_counts)\n\n            # Concatenate and aggregate all chunk counts\n            if all_chunk_gene_counts:\n                combined_gene_counts = pl.concat(all_chunk_gene_counts)\n                final_gene_counts = combined_gene_counts.group_by(\n                    \"cell_integer_id\"\n                ).agg(pl.sum(\"count\").alias(\"count\"))\n\n                # Create a complete gene counts array for all cells\n                # Initialize with zeros for all cells\n                gene_counts = np.zeros(len(obs_df), dtype=np.int64)\n\n                # Fill in the counts for cells that have expression data\n                cell_ids = final_gene_counts[\"cell_integer_id\"].to_numpy()\n                counts = final_gene_counts[\"count\"].to_numpy()\n                gene_counts[cell_ids] = counts\n\n                logger.info(f\"Gene counts: {gene_counts}\")\n            else:\n                gene_counts = np.zeros(len(obs_df), dtype=np.int64)\n\n        # Compute cumulative sum with first value as 0\n        return np.insert(np.cumsum(gene_counts)[:-1], 0, 0).tolist()\n\n    def _compute_cell_start_indices_anndata(\n        self, adata, obs_df: pd.DataFrame\n    ) -&gt; list[int]:\n        \"\"\"Compute cell start indices for AnnData object\"\"\"\n        # Check for existing n_genes or gene_count column\n        if \"n_genes\" in obs_df.columns:\n            logger.info(\"Using existing n_genes column for cell start indices\")\n            gene_counts = obs_df[\"n_genes\"].to_numpy()\n        elif \"gene_count\" in obs_df.columns:\n            logger.info(\"Using existing gene_count column for cell start indices\")\n            gene_counts = obs_df[\"gene_count\"].to_numpy()\n        elif \"n_genes_by_counts\" in obs_df.columns:\n            logger.info(\n                \"Using existing n_genes_by_counts column for cell start indices\"\n            )\n            gene_counts = obs_df[\"n_genes_by_counts\"].to_numpy()\n        else:\n            # Calculate from expression data\n            logger.info(\"Calculating gene counts from expression data...\")\n            # Convert sparse matrix to COO to count genes per cell\n            if sparse.issparse(adata.X):\n                if hasattr(adata.X, \"tocoo\"):\n                    coo = adata.X.tocoo()\n                else:\n                    # Handle backed mode where tocoo() might not be available\n                    try:\n                        coo = sparse.coo_matrix(adata.X)\n                    except ValueError:\n                        # If dtype is not supported, try to convert to a supported type\n                        logger.warning(\n                            \"adata.X has unsupported dtype, attempting conversion\"\n                        )\n                        # Try to convert to float32 first\n                        try:\n                            coo = sparse.coo_matrix(adata.X.astype(np.float32))\n                        except Exception:\n                            # If that fails, try to get the data in a different way\n                            logger.warning(\n                                \"Could not convert adata.X, using fallback method\"\n                            )\n                            # For backed mode, we might need to load a small sample\n                            sample_size = min(1000, adata.n_obs)\n                            sample_adata = adata[:sample_size, :]\n                            if sparse.issparse(sample_adata.X):\n                                coo = sample_adata.X.tocoo()\n                            else:\n                                coo = sparse.coo_matrix(sample_adata.X)\n            else:\n                try:\n                    coo = sparse.coo_matrix(adata.X)\n                except ValueError:\n                    # If dtype is not supported, try to convert to a supported type\n                    logger.warning(\n                        \"adata.X has unsupported dtype, attempting conversion\"\n                    )\n                    try:\n                        coo = sparse.coo_matrix(adata.X.astype(np.float32))\n                    except Exception:\n                        logger.warning(\n                            \"Could not convert adata.X, using fallback method\"\n                        )\n                        # For backed mode, we might need to load a small sample\n                        sample_size = min(1000, adata.n_obs)\n                        sample_adata = adata[:sample_size, :]\n                        if sparse.issparse(sample_adata.X):\n                            coo = sample_adata.X.tocoo()\n                        else:\n                            coo = sparse.coo_matrix(sample_adata.X)\n\n            # Count genes per cell using numpy bincount\n            n_cells = adata.n_obs\n            gene_counts = np.bincount(coo.row, minlength=n_cells)\n\n        # Compute cumulative sum with first value as 0\n        return np.insert(np.cumsum(gene_counts)[:-1], 0, 0).tolist()\n\n    def _compute_expression_statistics(\n        self, expression_dataset\n    ) -&gt; tuple[dict[str, float], int]:\n        \"\"\"Compute basic statistics from expression dataset using SQL\"\"\"\n        # Use Polars to compute statistics directly from Lance dataset\n\n        logger.info(\n            \"Computing expression statistics using fragment-by-fragment processing...\"\n        )\n\n        # Initialize running statistics\n        running_stats = {\n            \"min_value\": float(\"inf\"),\n            \"max_value\": float(\"-inf\"),\n            \"sum_value\": 0.0,\n            \"sum_squared\": 0.0,\n            \"count\": 0,\n        }\n\n        # Process each fragment individually to avoid memory issues\n        fragments = expression_dataset.get_fragments()\n        total_fragments = len(fragments)\n\n        logger.info(\n            f\"Processing {total_fragments} fragments for statistics computation...\"\n        )\n\n        from tqdm import tqdm\n\n        for i, fragment in enumerate(tqdm(fragments, desc=\"Computing statistics\")):\n            try:\n                # Create Polars LazyFrame from this fragment\n                ldf = pl.scan_pyarrow_dataset(fragment)\n\n                # Compute fragment-level statistics\n                fragment_stats = ldf.select(\n                    [\n                        pl.col(\"value\").min().alias(\"min_value\"),\n                        pl.col(\"value\").max().alias(\"max_value\"),\n                        pl.col(\"value\").sum().alias(\"sum_value\"),\n                        (pl.col(\"value\") ** 2).sum().alias(\"sum_squared\"),\n                        pl.col(\"value\").count().alias(\"count\"),\n                    ]\n                ).collect()\n\n                # Extract values from the result\n                row = fragment_stats.row(0)\n                frag_min, frag_max, frag_sum, frag_sum_squared, frag_count = row\n\n                # Update running statistics\n                running_stats[\"min_value\"] = min(running_stats[\"min_value\"], frag_min)\n                running_stats[\"max_value\"] = max(running_stats[\"max_value\"], frag_max)\n                running_stats[\"sum_value\"] += frag_sum\n                running_stats[\"sum_squared\"] += frag_sum_squared\n                running_stats[\"count\"] += frag_count\n\n            except Exception as e:\n                logger.warning(f\"Error processing fragment {i}: {e}\")\n                logger.warning(\"Continuing with remaining fragments...\")\n                continue\n\n        # Compute final statistics\n        if running_stats[\"count\"] == 0:\n            logger.warning(\"No valid data found for statistics computation\")\n            return {\n                \"min_value\": 0.0,\n                \"max_value\": 0.0,\n                \"mean_value\": 0.0,\n                \"std_value\": 0.0,\n            }, 0\n\n        # Calculate mean\n        mean_value = running_stats[\"sum_value\"] / running_stats[\"count\"]\n\n        # Calculate standard deviation using the formula: sqrt((sum(x\u00b2) - n*mean\u00b2) / (n-1))\n        variance = (\n            running_stats[\"sum_squared\"] - running_stats[\"count\"] * mean_value**2\n        ) / (running_stats[\"count\"] - 1)\n        std_value = variance**0.5 if variance &gt; 0 else 0.0\n\n        stats = {\n            \"min_value\": float(running_stats[\"min_value\"]),\n            \"max_value\": float(running_stats[\"max_value\"]),\n            \"mean_value\": float(mean_value),\n            \"std_value\": float(std_value),\n        }\n\n        logger.info(\n            f\"Statistics computed: min={stats['min_value']:.2f}, max={stats['max_value']:.2f}, mean={stats['mean_value']:.2f}, std={stats['std_value']:.2f}\"\n        )\n\n        return stats, int(running_stats[\"count\"])\n\n    def _save_config(\n        self,\n        output_path: str,\n        shape: tuple,\n        layer_names: list[str] | None = None,\n        obsm_keys: list[str] | None = None,\n        varm_keys: list[str] | None = None,\n    ):\n        \"\"\"Save SLAF configuration with computed metadata\"\"\"\n        n_cells = int(shape[0])\n        n_genes = int(shape[1])\n\n        # Compute additional metadata for faster info() method\n        logger.info(\"Computing dataset statistics...\")\n\n        # Reference Lance dataset\n        expression = lance.dataset(f\"{output_path}/expression.lance\")\n\n        # Compute basic statistics and count from expression data\n        expression_stats, expression_count = self._compute_expression_statistics(\n            expression\n        )\n\n        total_possible_elements = n_cells * n_genes\n        sparsity = 1 - (expression_count / total_possible_elements)\n\n        # Load existing config to preserve checkpoint data\n        config_path = f\"{output_path}/config.json\"\n        existing_config = {}\n        if self._path_exists(config_path):\n            try:\n                with self._open_file(config_path) as f:\n                    existing_config = json.load(f)\n            except Exception:\n                pass  # If we can't load existing config, start fresh\n\n        # Build tables dict\n        tables = {\n            \"expression\": \"expression.lance\",\n            \"cells\": \"cells.lance\",\n            \"genes\": \"genes.lance\",\n        }\n\n        # Add layers table if layers were converted\n        if layer_names and len(layer_names) &gt; 0:\n            tables[\"layers\"] = \"layers.lance\"\n\n        config = {\n            \"format_version\": \"0.4\",  # Bumped to 0.4 for layers support\n            \"array_shape\": [n_cells, n_genes],\n            \"n_cells\": n_cells,\n            \"n_genes\": n_genes,\n            \"tables\": tables,\n            \"optimizations\": {\n                \"use_integer_keys\": self.use_integer_keys,\n                \"optimize_storage\": self.optimize_storage,\n            },\n            \"metadata\": {\n                \"expression_count\": int(expression_count),\n                \"sparsity\": float(sparsity),\n                \"density\": float(1 - sparsity),\n                \"total_possible_elements\": int(total_possible_elements),\n                \"expression_stats\": expression_stats,\n            },\n            \"created_at\": pd.Timestamp.now().isoformat(),\n        }\n\n        # Add layers metadata if layers were converted\n        if layer_names and len(layer_names) &gt; 0:\n            # All converted layers are immutable by default\n            config[\"layers\"] = {\n                \"available\": layer_names,\n                \"immutable\": layer_names,  # Converted layers cannot be deleted\n                \"mutable\": [],  # No mutable layers yet\n            }\n\n        # Add obsm metadata if obsm was converted\n        if obsm_keys and len(obsm_keys) &gt; 0:\n            # Get dimensions from cells.lance schema\n            cells_dataset = lance.dataset(f\"{output_path}/cells.lance\")\n            obsm_dimensions = {}\n            schema = cells_dataset.schema\n            for field in schema:\n                if field.name in obsm_keys and isinstance(\n                    field.type, pa.FixedSizeListType\n                ):\n                    obsm_dimensions[field.name] = field.type.list_size\n\n            # All converted obsm keys are immutable by default\n            config[\"obsm\"] = {\n                \"available\": obsm_keys,\n                \"immutable\": obsm_keys,  # Converted obsm cannot be deleted\n                \"mutable\": [],  # No mutable obsm yet\n                \"dimensions\": obsm_dimensions,\n            }\n\n        # Add varm metadata if varm was converted\n        if varm_keys and len(varm_keys) &gt; 0:\n            # Get dimensions from genes.lance schema\n            genes_dataset = lance.dataset(f\"{output_path}/genes.lance\")\n            varm_dimensions = {}\n            schema = genes_dataset.schema\n            for field in schema:\n                if field.name in varm_keys and isinstance(\n                    field.type, pa.FixedSizeListType\n                ):\n                    varm_dimensions[field.name] = field.type.list_size\n\n            # All converted varm keys are immutable by default\n            config[\"varm\"] = {\n                \"available\": varm_keys,\n                \"immutable\": varm_keys,  # Converted varm cannot be deleted\n                \"mutable\": [],  # No mutable varm yet\n                \"dimensions\": varm_dimensions,\n            }\n\n        # Preserve checkpoint data if it exists\n        if \"checkpoint\" in existing_config:\n            config[\"checkpoint\"] = existing_config[\"checkpoint\"]\n\n        config_path = f\"{output_path}/config.json\"\n        with self._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n    def _save_multi_file_config(\n        self,\n        output_path: str,\n        source_file_info: list,\n        combined_expression: pa.Table | None = None,\n        combined_cells: pa.Table | None = None,\n        combined_genes: pa.Table | None = None,\n        layer_names: list[str] | None = None,\n    ):\n        \"\"\"Save SLAF configuration for multi-file conversion with source file tracking\"\"\"\n\n        # If tables are not provided, load them from the Lance datasets\n        if combined_cells is None or combined_genes is None:\n            cells_dataset = lance.dataset(f\"{output_path}/cells.lance\")\n            genes_dataset = lance.dataset(f\"{output_path}/genes.lance\")\n            n_cells = len(cells_dataset.to_table())\n            n_genes = len(genes_dataset.to_table())\n        else:\n            n_cells = len(combined_cells) if combined_cells is not None else 0\n            n_genes = len(combined_genes) if combined_genes is not None else 0\n\n        # Compute additional metadata for faster info() method\n        logger.info(\"Computing multi-file dataset statistics...\")\n\n        # Reference Lance dataset\n        expression = lance.dataset(f\"{output_path}/expression.lance\")\n\n        # Compute basic statistics and count from expression data\n        expression_stats, expression_count = self._compute_expression_statistics(\n            expression\n        )\n\n        total_possible_elements = n_cells * n_genes\n        sparsity = (\n            1 - (expression_count / total_possible_elements)\n            if total_possible_elements &gt; 0\n            else 1.0\n        )\n\n        # Calculate total cells and genes from source files\n        total_cells_from_files = sum(info[\"n_cells\"] for info in source_file_info)\n\n        config = {\n            \"format_version\": \"0.4\",  # Bumped to 0.4 for layers support\n            \"array_shape\": [n_cells, n_genes],\n            \"n_cells\": n_cells,\n            \"n_genes\": n_genes,\n            \"tables\": {\n                \"expression\": \"expression.lance\",\n                \"cells\": \"cells.lance\",\n                \"genes\": \"genes.lance\",\n            },\n            \"optimizations\": {\n                \"use_integer_keys\": self.use_integer_keys,\n                \"optimize_storage\": self.optimize_storage,\n            },\n            \"metadata\": {\n                \"expression_count\": int(expression_count),\n                \"sparsity\": float(sparsity),\n                \"density\": float(1 - sparsity),\n                \"total_possible_elements\": int(total_possible_elements),\n                \"expression_stats\": expression_stats,\n            },\n            \"multi_file\": {\n                \"source_files\": source_file_info,\n                \"total_files\": len(source_file_info),\n                \"total_cells_from_files\": total_cells_from_files,\n            },\n            \"created_at\": pd.Timestamp.now().isoformat(),\n        }\n\n        # Add layers metadata if layers were converted\n        if layer_names:\n            config[\"tables\"][\"layers\"] = \"layers.lance\"\n            config[\"layers\"] = {\n                \"available\": layer_names,\n                \"immutable\": layer_names,  # All converted layers are immutable\n                \"mutable\": [],\n            }\n\n        config_path = f\"{output_path}/config.json\"\n        with self._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n</code></pre>"},{"location":"api/data/#slaf.data.converter.SLAFConverter-functions","title":"Functions","text":""},{"location":"api/data/#slaf.data.converter.SLAFConverter.__init__","title":"<code>__init__(use_integer_keys: bool = True, chunked: bool = True, chunk_size: int = 5000, sort_metadata: bool = False, create_indices: bool = False, optimize_storage: bool = True, use_optimized_dtypes: bool = True, enable_v2_manifest: bool = True, compact_after_write: bool = False, tiledb_collection_name: str = 'RNA', enable_checkpointing: bool = True, max_rows_per_file: int = 100000000)</code>","text":"<p>Initialize converter with optimization options.</p> <p>Parameters:</p> Name Type Description Default <code>use_integer_keys</code> <code>bool</code> <p>Use integer keys instead of strings in sparse data.              This saves significant memory and improves query performance.              Set to False only if you need to preserve original string IDs.</p> <code>True</code> <code>chunked</code> <code>bool</code> <p>Use chunked processing for memory efficiency (default: True).     Chunked processing is now the default for optimal memory efficiency.     Set to False only for small datasets or debugging purposes.</p> <code>True</code> <code>chunk_size</code> <code>int</code> <p>Size of each chunk when chunked=True (default: 5000).</p> <code>5000</code> <code>create_indices</code> <code>bool</code> <p>Whether to create indices for query performance.           Default: False for small datasets to reduce storage overhead.           Set to True for large datasets where query performance is important.</p> <code>False</code> <code>optimize_storage</code> <code>bool</code> <p>Only store integer IDs in expression table to reduce storage size.            String IDs are available in metadata tables for mapping.</p> <code>True</code> <code>use_optimized_dtypes</code> <code>bool</code> <p>Use optimized data types (uint16/uint32) for better compression.                 This can significantly reduce storage size for large datasets.</p> <code>True</code> <code>enable_v2_manifest</code> <code>bool</code> <p>Enable v2 manifest paths for better query performance.               This is recommended for large datasets.</p> <code>True</code> <code>compact_after_write</code> <code>bool</code> <p>Compact the dataset after writing to optimize storage.                This creates a new version but significantly reduces file size.</p> <code>False</code> <code>tiledb_collection_name</code> <code>str</code> <p>Name of the measurement collection for TileDB format.                   Default: \"RNA\". Only used when converting from TileDB format.</p> <code>'RNA'</code> <code>enable_checkpointing</code> <code>bool</code> <p>Enable checkpointing for long-running conversions.                 This allows resuming from the last completed chunk if the                 conversion is interrupted. Default: True.</p> <code>True</code> <code>max_rows_per_file</code> <code>int</code> <p>Maximum number of rows per Lance fragment file.              Larger values create fewer fragments but larger files.              Default: 100M to stay under HuggingFace's 10k file limit.              For very large datasets, increase this value to reduce fragment count.</p> <code>100000000</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Default optimization (recommended)\n&gt;&gt;&gt; converter = SLAFConverter()\n&gt;&gt;&gt; print(f\"Using chunked processing: {converter.chunked}\")\nUsing chunked processing: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Non-chunked processing for small datasets\n&gt;&gt;&gt; converter = SLAFConverter(chunked=False)\n&gt;&gt;&gt; print(f\"Using chunked processing: {converter.chunked}\")\nUsing chunked processing: False\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom chunk size for large datasets\n&gt;&gt;&gt; converter = SLAFConverter(chunk_size=10000)\n&gt;&gt;&gt; print(f\"Chunk size: {converter.chunk_size}\")\nChunk size: 5000\n</code></pre> Source code in <code>slaf/data/converter.py</code> <pre><code>def __init__(\n    self,\n    use_integer_keys: bool = True,\n    chunked: bool = True,  # Changed from False to True - make chunked the default\n    chunk_size: int = 5000,  # Smaller chunks to avoid memory alignment issues in Lance v2.1\n    sort_metadata: bool = False,\n    create_indices: bool = False,  # Disable indices by default for small datasets\n    optimize_storage: bool = True,  # Only store integer IDs in expression table\n    use_optimized_dtypes: bool = True,  # Use uint16/uint32 for better compression\n    enable_v2_manifest: bool = True,  # Enable v2 manifest paths for better performance\n    compact_after_write: bool = False,  # Compact dataset after writing for optimal storage (disabled by default to avoid manifest corruption)\n    tiledb_collection_name: str = \"RNA\",  # Collection name for TileDB format\n    enable_checkpointing: bool = True,  # Enable checkpointing for long-running conversions\n    max_rows_per_file: int = 100_000_000,  # Max rows per Lance fragment (default: 100M to stay under HuggingFace's 10k file limit)\n):\n    \"\"\"\n    Initialize converter with optimization options.\n\n    Args:\n        use_integer_keys: Use integer keys instead of strings in sparse data.\n                         This saves significant memory and improves query performance.\n                         Set to False only if you need to preserve original string IDs.\n        chunked: Use chunked processing for memory efficiency (default: True).\n                Chunked processing is now the default for optimal memory efficiency.\n                Set to False only for small datasets or debugging purposes.\n        chunk_size: Size of each chunk when chunked=True (default: 5000).\n        create_indices: Whether to create indices for query performance.\n                      Default: False for small datasets to reduce storage overhead.\n                      Set to True for large datasets where query performance is important.\n        optimize_storage: Only store integer IDs in expression table to reduce storage size.\n                       String IDs are available in metadata tables for mapping.\n        use_optimized_dtypes: Use optimized data types (uint16/uint32) for better compression.\n                            This can significantly reduce storage size for large datasets.\n        enable_v2_manifest: Enable v2 manifest paths for better query performance.\n                          This is recommended for large datasets.\n        compact_after_write: Compact the dataset after writing to optimize storage.\n                           This creates a new version but significantly reduces file size.\n        tiledb_collection_name: Name of the measurement collection for TileDB format.\n                              Default: \"RNA\". Only used when converting from TileDB format.\n        enable_checkpointing: Enable checkpointing for long-running conversions.\n                            This allows resuming from the last completed chunk if the\n                            conversion is interrupted. Default: True.\n        max_rows_per_file: Maximum number of rows per Lance fragment file.\n                         Larger values create fewer fragments but larger files.\n                         Default: 100M to stay under HuggingFace's 10k file limit.\n                         For very large datasets, increase this value to reduce fragment count.\n\n    Examples:\n        &gt;&gt;&gt; # Default optimization (recommended)\n        &gt;&gt;&gt; converter = SLAFConverter()\n        &gt;&gt;&gt; print(f\"Using chunked processing: {converter.chunked}\")\n        Using chunked processing: True\n\n        &gt;&gt;&gt; # Non-chunked processing for small datasets\n        &gt;&gt;&gt; converter = SLAFConverter(chunked=False)\n        &gt;&gt;&gt; print(f\"Using chunked processing: {converter.chunked}\")\n        Using chunked processing: False\n\n        &gt;&gt;&gt; # Custom chunk size for large datasets\n        &gt;&gt;&gt; converter = SLAFConverter(chunk_size=10000)\n        &gt;&gt;&gt; print(f\"Chunk size: {converter.chunk_size}\")\n        Chunk size: 5000\n    \"\"\"\n    self.use_integer_keys = use_integer_keys\n    self.chunked = chunked\n    self.chunk_size = chunk_size\n    self.sort_metadata = sort_metadata\n    self.create_indices = create_indices\n    self.optimize_storage = optimize_storage\n    self.use_optimized_dtypes = use_optimized_dtypes\n    self.enable_v2_manifest = enable_v2_manifest\n    self.compact_after_write = compact_after_write\n    self.tiledb_collection_name = tiledb_collection_name\n    self.enable_checkpointing = enable_checkpointing\n    self.max_rows_per_file = max_rows_per_file\n</code></pre>"},{"location":"api/data/#slaf.data.converter.SLAFConverter.convert","title":"<code>convert(input_path: str | list[str], output_path: str, input_format: str = 'auto', skip_validation: bool = False)</code>","text":"<p>Convert single-cell data to SLAF format with optimized storage.</p> <p>SLAFConverter provides efficient conversion from various single-cell data formats to the SLAF format. It optimizes storage by using integer keys, COO-style expression tables, and efficient metadata handling.</p> Supported Input Formats <ul> <li>h5ad: AnnData files (.h5ad) - the standard single-cell format</li> <li>10x MTX: 10x Genomics MTX directories containing matrix.mtx,   barcodes.tsv, and genes.tsv files</li> <li>10x H5: 10x Genomics H5 files (.h5) - Cell Ranger output format</li> <li>tiledb: TileDB SOMA format (.tiledb) - high-performance single-cell format</li> </ul> <p>The converter automatically detects the input format based on file extension and directory structure. For optimal performance, you can also specify the format explicitly.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str | list[str]</code> <p>Path to the input file or directory to convert.        - For h5ad: path to .h5ad file        - For MTX: path to directory containing matrix.mtx, barcodes.tsv, genes.tsv        - For H5: path to .h5 file</p> required <code>output_path</code> <code>str</code> <p>Path where the SLAF dataset will be saved.         Should be a directory path, not a file path.</p> required <code>input_format</code> <code>str</code> <p>Format of input data. Options:          - \"auto\" (default): Auto-detect format          - \"h5ad\": AnnData format          - \"10x_mtx\": 10x MTX directory format          - \"10x_h5\": 10x H5 file format          - \"tiledb\": TileDB SOMA format</p> <code>'auto'</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the input file doesn't exist.</p> <code>ValueError</code> <p>If the input file is corrupted, invalid, or format cannot be detected.</p> <code>RuntimeError</code> <p>If the conversion process fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Auto-detect format (recommended)\n&gt;&gt;&gt; converter = SLAFConverter()\n&gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output.slaf\")\nConverting data.h5ad to SLAF format...\nOptimizations: int_keys=True\nLoaded: 1000 cells \u00d7 20000 genes\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Convert 10x MTX directory\n&gt;&gt;&gt; converter.convert(\"filtered_feature_bc_matrix/\", \"output.slaf\")\nConverting 10x MTX directory filtered_feature_bc_matrix/ to SLAF format...\nLoaded: 2700 cells \u00d7 32738 genes\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Convert 10x H5 file\n&gt;&gt;&gt; converter.convert(\"data.h5\", \"output.slaf\")\nConverting 10x H5 file data.h5 to SLAF format...\nLoaded: 2700 cells \u00d7 32738 genes\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Convert TileDB SOMA file\n&gt;&gt;&gt; converter.convert(\"data.tiledb\", \"output.slaf\")\nConverting TileDB SOMA file data.tiledb to SLAF format...\nLoaded: 50000 cells \u00d7 20000 genes\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Explicit format specification\n&gt;&gt;&gt; converter.convert(\"data.h5\", \"output.slaf\", input_format=\"10x_h5\")\nConverting 10x H5 file data.h5 to SLAF format...\nLoaded: 2700 cells \u00d7 32738 genes\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Convert with chunked processing for large datasets\n&gt;&gt;&gt; converter = SLAFConverter(chunked=True, chunk_size=5000)\n&gt;&gt;&gt; converter.convert(\"large_data.h5ad\", \"output.slaf\")\nConverting large_data.h5ad to SLAF format...\nOptimizations: int_keys=True, chunked=True\nProcessing in chunks of 5000 cells...\nConversion complete! Saved to output.slaf\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for unsupported format\n&gt;&gt;&gt; try:\n...     converter.convert(\"unknown_file.txt\", \"output.slaf\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Cannot detect format for: unknown_file.txt\n</code></pre> Source code in <code>slaf/data/converter.py</code> <pre><code>def convert(\n    self,\n    input_path: str | list[str],\n    output_path: str,\n    input_format: str = \"auto\",\n    skip_validation: bool = False,\n):\n    \"\"\"\n    Convert single-cell data to SLAF format with optimized storage.\n\n    SLAFConverter provides efficient conversion from various single-cell data formats\n    to the SLAF format. It optimizes storage by using integer keys, COO-style\n    expression tables, and efficient metadata handling.\n\n    Supported Input Formats:\n        - **h5ad**: AnnData files (.h5ad) - the standard single-cell format\n        - **10x MTX**: 10x Genomics MTX directories containing matrix.mtx,\n          barcodes.tsv, and genes.tsv files\n        - **10x H5**: 10x Genomics H5 files (.h5) - Cell Ranger output format\n        - **tiledb**: TileDB SOMA format (.tiledb) - high-performance single-cell format\n\n    The converter automatically detects the input format based on file extension\n    and directory structure. For optimal performance, you can also specify the\n    format explicitly.\n\n    Args:\n        input_path: Path to the input file or directory to convert.\n                   - For h5ad: path to .h5ad file\n                   - For MTX: path to directory containing matrix.mtx, barcodes.tsv, genes.tsv\n                   - For H5: path to .h5 file\n        output_path: Path where the SLAF dataset will be saved.\n                    Should be a directory path, not a file path.\n        input_format: Format of input data. Options:\n                     - \"auto\" (default): Auto-detect format\n                     - \"h5ad\": AnnData format\n                     - \"10x_mtx\": 10x MTX directory format\n                     - \"10x_h5\": 10x H5 file format\n                     - \"tiledb\": TileDB SOMA format\n\n    Raises:\n        FileNotFoundError: If the input file doesn't exist.\n        ValueError: If the input file is corrupted, invalid, or format cannot be detected.\n        RuntimeError: If the conversion process fails.\n\n    Examples:\n        &gt;&gt;&gt; # Auto-detect format (recommended)\n        &gt;&gt;&gt; converter = SLAFConverter()\n        &gt;&gt;&gt; converter.convert(\"data.h5ad\", \"output.slaf\")\n        Converting data.h5ad to SLAF format...\n        Optimizations: int_keys=True\n        Loaded: 1000 cells \u00d7 20000 genes\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Convert 10x MTX directory\n        &gt;&gt;&gt; converter.convert(\"filtered_feature_bc_matrix/\", \"output.slaf\")\n        Converting 10x MTX directory filtered_feature_bc_matrix/ to SLAF format...\n        Loaded: 2700 cells \u00d7 32738 genes\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Convert 10x H5 file\n        &gt;&gt;&gt; converter.convert(\"data.h5\", \"output.slaf\")\n        Converting 10x H5 file data.h5 to SLAF format...\n        Loaded: 2700 cells \u00d7 32738 genes\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Convert TileDB SOMA file\n        &gt;&gt;&gt; converter.convert(\"data.tiledb\", \"output.slaf\")\n        Converting TileDB SOMA file data.tiledb to SLAF format...\n        Loaded: 50000 cells \u00d7 20000 genes\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Explicit format specification\n        &gt;&gt;&gt; converter.convert(\"data.h5\", \"output.slaf\", input_format=\"10x_h5\")\n        Converting 10x H5 file data.h5 to SLAF format...\n        Loaded: 2700 cells \u00d7 32738 genes\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Convert with chunked processing for large datasets\n        &gt;&gt;&gt; converter = SLAFConverter(chunked=True, chunk_size=5000)\n        &gt;&gt;&gt; converter.convert(\"large_data.h5ad\", \"output.slaf\")\n        Converting large_data.h5ad to SLAF format...\n        Optimizations: int_keys=True, chunked=True\n        Processing in chunks of 5000 cells...\n        Conversion complete! Saved to output.slaf\n\n        &gt;&gt;&gt; # Error handling for unsupported format\n        &gt;&gt;&gt; try:\n        ...     converter.convert(\"unknown_file.txt\", \"output.slaf\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Cannot detect format for: unknown_file.txt\n    \"\"\"\n    # Handle both single paths and lists of files\n    if isinstance(input_path, list):\n        # Direct list of files - use multi-file conversion\n        input_files = input_path\n        # Detect format from first file\n        if input_format == \"auto\":\n            from .utils import detect_format\n\n            input_format = detect_format(input_files[0])\n    else:\n        # Single path - discover files\n        input_files, detected_format = discover_input_files(input_path)\n        # Use detected format if auto, otherwise use specified format\n        if input_format == \"auto\":\n            input_format = detected_format\n\n    # Validate multi-file compatibility if multiple files (unless skipped)\n    if not skip_validation:\n        validate_input_files(input_files, input_format)\n\n    # Handle multiple files vs single file\n    if len(input_files) &gt; 1:\n        self._convert_multiple_files(input_files, output_path, input_format)\n    else:\n        # Single file - use existing logic\n        single_file = input_files[0]\n        if input_format == \"h5ad\":\n            if not SCANPY_AVAILABLE:\n                raise ImportError(\n                    \"Scanpy is required for h5ad conversion. \"\n                    \"Install with: pip install scanpy\"\n                )\n            self._convert_h5ad(single_file, output_path)\n        elif input_format == \"10x_mtx\":\n            self._convert_10x_mtx(single_file, output_path)\n        elif input_format == \"10x_h5\":\n            self._convert_10x_h5(single_file, output_path)\n        elif input_format == \"tiledb\":\n            self._convert_tiledb(single_file, output_path)\n        else:\n            raise ValueError(f\"Unsupported format: {input_format}\")\n</code></pre>"},{"location":"api/data/#slaf.data.converter.SLAFConverter.convert_anndata","title":"<code>convert_anndata(adata, output_path: str)</code>","text":"<p>Convert AnnData object to SLAF format with COO-style expression table</p> Source code in <code>slaf/data/converter.py</code> <pre><code>def convert_anndata(self, adata, output_path: str):\n    \"\"\"Convert AnnData object to SLAF format with COO-style expression table\"\"\"\n    if self.chunked:\n        raise ValueError(\n            \"convert_anndata() not supported in chunked mode. \"\n            \"Use convert() with file path instead.\"\n        )\n\n    logger.info(\"Converting AnnData object to SLAF format...\")\n    logger.info(f\"Optimizations: int_keys={self.use_integer_keys}\")\n    logger.info(f\"Loaded: {adata.n_obs} cells \u00d7 {adata.n_vars} genes\")\n\n    # Validate optimized data types\n    if not self._validate_optimized_dtypes_anndata(adata):\n        self.use_optimized_dtypes = False\n\n    # Convert the AnnData object\n    self._convert_anndata(adata, output_path)\n</code></pre>"},{"location":"api/data/#slaf.data.converter.SLAFConverter.append","title":"<code>append(input_path: str, existing_slaf_path: str, input_format: str = 'auto')</code>","text":"<p>Append new data to an existing SLAF dataset.</p> <p>This method adds new data to an existing SLAF dataset by: 1. Validating compatibility with existing dataset 2. Reading existing dataset metadata 3. Appending new data with auto-incrementing IDs 4. Updating configuration with new source file info</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to new input file or directory</p> required <code>existing_slaf_path</code> <code>str</code> <p>Path to existing SLAF dataset</p> required <code>input_format</code> <code>str</code> <p>Format of input data (auto-detected if not specified)</p> <code>'auto'</code> Source code in <code>slaf/data/converter.py</code> <pre><code>def append(\n    self, input_path: str, existing_slaf_path: str, input_format: str = \"auto\"\n):\n    \"\"\"\n    Append new data to an existing SLAF dataset.\n\n    This method adds new data to an existing SLAF dataset by:\n    1. Validating compatibility with existing dataset\n    2. Reading existing dataset metadata\n    3. Appending new data with auto-incrementing IDs\n    4. Updating configuration with new source file info\n\n    Args:\n        input_path: Path to new input file or directory\n        existing_slaf_path: Path to existing SLAF dataset\n        input_format: Format of input data (auto-detected if not specified)\n    \"\"\"\n    logger.info(\n        f\"Appending data from {input_path} to existing SLAF dataset {existing_slaf_path}\"\n    )\n\n    # Check if existing SLAF dataset exists\n    if not self._path_exists(existing_slaf_path):\n        raise FileNotFoundError(\n            f\"Existing SLAF dataset not found: {existing_slaf_path}\"\n        )\n\n    # Discover input files\n    input_files, detected_format = discover_input_files(input_path)\n    if input_format == \"auto\":\n        input_format = detected_format\n\n    # Validate compatibility with existing dataset\n    self._validate_append_compatibility(\n        input_files, input_format, existing_slaf_path\n    )\n\n    # Read existing dataset metadata\n    existing_cells_dataset = lance.dataset(\n        os.path.join(existing_slaf_path, \"cells.lance\")\n    )\n    existing_cells_table = existing_cells_dataset.to_table()\n    current_cell_count = len(existing_cells_table)\n\n    # Track source file information\n    source_file_info = []\n    global_cell_offset = current_cell_count\n    total_new_cells = 0\n\n    # Process each new file and append to existing dataset\n    for i, file_path in enumerate(input_files):\n        logger.info(\n            f\"Processing file {i + 1}/{len(input_files)}: {os.path.basename(file_path)}\"\n        )\n\n        try:\n            # Use chunked reader to process file directly\n            with create_chunked_reader(\n                file_path,\n                chunk_size=self.chunk_size,\n                collection_name=self.tiledb_collection_name,\n            ) as reader:\n                logger.info(\n                    f\"Loaded: {reader.n_obs:,} cells \u00d7 {reader.n_vars:,} genes\"\n                )\n\n                # Get metadata from reader\n                obs_df = reader.get_obs_metadata()\n                var_df = reader.get_var_metadata()\n\n                # Ensure cell_id and gene_id columns exist\n                if \"cell_id\" not in obs_df.columns:\n                    obs_df[\"cell_id\"] = reader.obs_names\n                if \"gene_id\" not in var_df.columns:\n                    var_df[\"gene_id\"] = reader.var_names\n\n                # Add integer IDs with global offset\n                obs_df[\"cell_integer_id\"] = range(\n                    global_cell_offset, global_cell_offset + len(obs_df)\n                )\n\n                # Add source file information to cell metadata\n                source_file = os.path.basename(file_path)\n                obs_df[\"source_file\"] = source_file\n\n                # Check if existing dataset has source_file column\n                existing_cells_dataset = lance.dataset(\n                    os.path.join(existing_slaf_path, \"cells.lance\")\n                )\n                existing_cells_table = existing_cells_dataset.to_table()\n                existing_columns = set(existing_cells_table.column_names)\n\n                # If existing dataset doesn't have source_file column, add it to existing data\n                if \"source_file\" not in existing_columns:\n                    logger.info(\"Adding source_file column to existing dataset...\")\n                    # Read existing cells data\n                    existing_cells_df = existing_cells_table.to_pandas()\n                    existing_cells_df[\"source_file\"] = (\n                        \"original_data\"  # Default source for existing data\n                    )\n\n                    # Recreate the cells dataset with source_file column\n                    updated_cells_table = pa.table(existing_cells_df)\n                    # Cast large_string to string so subsequent appends match schema\n                    updated_cells_table = self._cast_table_string_columns_to_utf8(\n                        updated_cells_table, [\"cell_id\"]\n                    )\n                    lance.write_dataset(\n                        updated_cells_table,\n                        os.path.join(existing_slaf_path, \"cells.lance\"),\n                        mode=\"overwrite\",\n                        enable_v2_manifest_paths=self.enable_v2_manifest,\n                        data_storage_version=\"2.1\",\n                    )\n                    logger.info(\"\u2713 Added source_file column to existing dataset\")\n\n                # Precompute cell start indices\n                obs_df[\"cell_start_index\"] = self._compute_cell_start_indices(\n                    reader, obs_df\n                )\n\n                # Convert metadata to Lance tables\n                cell_metadata_table = self._create_metadata_table(\n                    obs_df, \"cell_id\", integer_mapping=None\n                )\n\n                # Append to existing cells dataset\n                cells_path = os.path.join(existing_slaf_path, \"cells.lance\")\n                lance.write_dataset(\n                    cell_metadata_table,\n                    cells_path,\n                    mode=\"append\",\n                    enable_v2_manifest_paths=self.enable_v2_manifest,\n                    data_storage_version=\"2.1\",\n                )\n\n                # Process expression data in chunks with checkpointing support\n                # Use the same checkpointing infrastructure as convert method\n                self._process_file_chunks_with_checkpoint(\n                    reader, existing_slaf_path, i, 0, global_cell_offset\n                )\n\n                # Track source file information\n                source_file_info.append(\n                    {\n                        \"file_path\": file_path,\n                        \"file_name\": source_file,\n                        \"n_cells\": len(obs_df),\n                        \"cell_offset\": global_cell_offset,\n                    }\n                )\n\n                # Update global cell offset\n                global_cell_offset += len(obs_df)\n                total_new_cells += len(obs_df)\n\n                # Save checkpoint after each file (for append operations)\n                if self.enable_checkpointing:\n                    checkpoint_data = {\n                        \"status\": \"in_progress\",\n                        \"last_completed_file\": i,\n                        \"last_completed_chunk\": -1,  # -1 indicates file is fully completed\n                        \"global_cell_offset\": global_cell_offset,\n                        \"operation\": \"append\",  # Mark this as an append operation\n                        \"timestamp\": pd.Timestamp.now().isoformat(),\n                    }\n                    self._save_checkpoint(existing_slaf_path, checkpoint_data)\n\n        except Exception as e:\n            logger.error(f\"Failed to process file {file_path}: {e}\")\n            # Save checkpoint with error status\n            if self.enable_checkpointing:\n                checkpoint_data = {\n                    \"status\": \"error\",\n                    \"last_completed_file\": i - 1,\n                    \"last_completed_chunk\": -1,  # -1 indicates previous file was fully completed\n                    \"global_cell_offset\": global_cell_offset,\n                    \"operation\": \"append\",  # Mark this as an append operation\n                    \"error\": str(e),\n                    \"timestamp\": pd.Timestamp.now().isoformat(),\n                }\n                self._save_checkpoint(existing_slaf_path, checkpoint_data)\n            # Continue with other files\n            continue\n\n    if not source_file_info:\n        raise RuntimeError(\"No files were successfully processed\")\n\n    # Update configuration with new source file information\n    self._update_config_with_append(\n        existing_slaf_path, source_file_info, total_new_cells\n    )\n\n    # Clear checkpoint after successful completion\n    self._clear_checkpoint(existing_slaf_path)\n\n    logger.info(\n        f\"Append complete! Added {total_new_cells} cells from {len(source_file_info)} files\"\n    )\n    logger.info(f\"Total cells in dataset: {current_cell_count + total_new_cells}\")\n</code></pre>"},{"location":"api/integrations/","title":"Integrations API","text":"<p>AnnData and Scanpy integration modules.</p>"},{"location":"api/integrations/#anndata-integration","title":"AnnData Integration","text":""},{"location":"api/integrations/#slaf.integrations.anndata","title":"<code>slaf.integrations.anndata</code>","text":""},{"location":"api/integrations/#slaf.integrations.anndata-classes","title":"Classes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix","title":"<code>LazyExpressionMatrix</code>","text":"<p>               Bases: <code>LazySparseMixin</code></p> <p>Lazy expression matrix backed by SLAF with scipy.sparse interface.</p> <p>LazyExpressionMatrix provides a scipy.sparse-compatible interface for accessing single-cell expression data stored in SLAF format. It implements lazy evaluation to avoid loading all data into memory, making it suitable for large datasets.</p> Key Features <ul> <li>scipy.sparse-compatible interface</li> <li>Lazy evaluation for memory efficiency</li> <li>Caching for repeated queries</li> <li>Support for cell and gene subsetting</li> <li>Integration with AnnData objects</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; lazy_matrix = LazyExpressionMatrix(slaf_array)\n&gt;&gt;&gt; print(f\"Matrix shape: {lazy_matrix.shape}\")\nMatrix shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # With AnnData integration\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; matrix = adata.X\n&gt;&gt;&gt; print(f\"Expression matrix shape: {matrix.shape}\")\nExpression matrix shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Subsetting operations\n&gt;&gt;&gt; subset_matrix = matrix[:100, :5000]  # First 100 cells, first 5000 genes\n&gt;&gt;&gt; print(f\"Subset shape: {subset_matrix.shape}\")\nSubset shape: (100, 5000)\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyExpressionMatrix(LazySparseMixin):\n    \"\"\"\n    Lazy expression matrix backed by SLAF with scipy.sparse interface.\n\n    LazyExpressionMatrix provides a scipy.sparse-compatible interface for accessing\n    single-cell expression data stored in SLAF format. It implements lazy evaluation\n    to avoid loading all data into memory, making it suitable for large datasets.\n\n    Key Features:\n        - scipy.sparse-compatible interface\n        - Lazy evaluation for memory efficiency\n        - Caching for repeated queries\n        - Support for cell and gene subsetting\n        - Integration with AnnData objects\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; lazy_matrix = LazyExpressionMatrix(slaf_array)\n        &gt;&gt;&gt; print(f\"Matrix shape: {lazy_matrix.shape}\")\n        Matrix shape: (1000, 20000)\n\n        &gt;&gt;&gt; # With AnnData integration\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; matrix = adata.X\n        &gt;&gt;&gt; print(f\"Expression matrix shape: {matrix.shape}\")\n        Expression matrix shape: (1000, 20000)\n\n        &gt;&gt;&gt; # Subsetting operations\n        &gt;&gt;&gt; subset_matrix = matrix[:100, :5000]  # First 100 cells, first 5000 genes\n        &gt;&gt;&gt; print(f\"Subset shape: {subset_matrix.shape}\")\n        Subset shape: (100, 5000)\n    \"\"\"\n\n    def __init__(\n        self,\n        slaf_array: SLAFArray,\n        table_name: str = \"expression\",\n        layer_name: str | None = None,\n    ):\n        \"\"\"\n        Initialize lazy expression matrix with SLAF array.\n\n        Args:\n            slaf_array: SLAFArray instance containing the single-cell data.\n                       Used for database queries and metadata access.\n            table_name: Table name to query (\"expression\" or \"layers\"). Default: \"expression\"\n            layer_name: Layer name for layers table (required when table_name=\"layers\").\n                       Default: None\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; matrix = LazyExpressionMatrix(slaf_array)\n            &gt;&gt;&gt; print(f\"Initialized with shape: {matrix.shape}\")\n            Initialized with shape: (1000, 20000)\n\n            &gt;&gt;&gt; # Check parent reference\n            &gt;&gt;&gt; print(f\"Parent adata: {matrix.parent_adata}\")\n            Parent adata: None\n\n            &gt;&gt;&gt; # Initialize for layers table\n            &gt;&gt;&gt; layer_matrix = LazyExpressionMatrix(slaf_array, table_name=\"layers\", layer_name=\"spliced\")\n            &gt;&gt;&gt; print(f\"Layer matrix shape: {layer_matrix.shape}\")\n            Layer matrix shape: (1000, 20000)\n        \"\"\"\n        super().__init__()\n        self.slaf_array = slaf_array\n        self.table_name = table_name\n        self.layer_name = layer_name\n        self.parent_adata: LazyAnnData | None = None\n        # Store slicing selectors\n        self._cell_selector: Any = None\n        self._gene_selector: Any = None\n        # Initialize shape attribute (required by LazySparseMixin)\n        self._shape = self.slaf_array.shape\n        self._cache: dict[str, Any] = {}  # Simple caching for repeated queries\n        # Validate parameters\n        if table_name == \"layers\" and layer_name is None:\n            raise ValueError(\"layer_name must be provided when table_name='layers'\")\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"\n        Shape of the expression matrix.\n\n        Returns:\n            Tuple of (n_cells, n_genes) representing the matrix dimensions.\n\n        Examples:\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; matrix = LazyExpressionMatrix(slaf_array)\n            &gt;&gt;&gt; print(f\"Matrix shape: {matrix.shape}\")\n            Matrix shape: (1000, 20000)\n        \"\"\"\n        return self._shape\n\n    @property\n    def obs_names(self) -&gt; pd.Index | None:\n        \"\"\"\n        Cell names (observations).\n\n        Returns:\n            pandas.Index of cell names if parent AnnData is available, None otherwise.\n\n        Examples:\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; matrix = adata.X\n            &gt;&gt;&gt; print(f\"Cell names: {len(matrix.obs_names)}\")\n            Cell names: 1000\n        \"\"\"\n        if hasattr(self, \"parent_adata\") and self.parent_adata is not None:\n            return self.parent_adata.obs_names\n        return None\n\n    @property\n    def var_names(self) -&gt; pd.Index | None:\n        \"\"\"\n        Gene names (variables).\n\n        Returns:\n            pandas.Index of gene names if parent AnnData is available, None otherwise.\n\n        Examples:\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; matrix = adata.X\n            &gt;&gt;&gt; print(f\"Gene names: {len(matrix.var_names)}\")\n            Gene names: 20000\n        \"\"\"\n        if hasattr(self, \"parent_adata\") and self.parent_adata is not None:\n            return self.parent_adata.var_names\n        return None\n\n    def _update_shape(self):\n        \"\"\"Update the shape attribute based on current selectors\"\"\"\n        if self._cell_selector is not None or self._gene_selector is not None:\n            # Calculate the shape based on selectors\n            cell_selector = (\n                self._cell_selector if self._cell_selector is not None else slice(None)\n            )\n            gene_selector = (\n                self._gene_selector if self._gene_selector is not None else slice(None)\n            )\n\n            # Use the same logic as _get_result_shape in LazySparseMixin\n            cell_selector = self._compose_selectors(cell_selector, None, axis=0)\n            gene_selector = self._compose_selectors(gene_selector, None, axis=1)\n\n            n_cells = self._calculate_selected_count(cell_selector, axis=0)\n            n_genes = self._calculate_selected_count(gene_selector, axis=1)\n\n            self._shape = (n_cells, n_genes)\n        else:\n            # No selectors applied, return original shape\n            self._shape = self.slaf_array.shape\n\n    def _calculate_selected_count(self, selector, axis: int) -&gt; int:\n        \"\"\"Calculate the number of selected entities for a given selector\"\"\"\n        if selector is None or (\n            isinstance(selector, slice) and selector == slice(None)\n        ):\n            return self.slaf_array.shape[axis]\n\n        if isinstance(selector, slice):\n            start = selector.start or 0\n            stop = selector.stop or self.slaf_array.shape[axis]\n            step = selector.step or 1\n\n            # Clamp bounds to actual data size\n            start = max(0, min(start, self.slaf_array.shape[axis]))\n            stop = max(0, min(stop, self.slaf_array.shape[axis]))\n\n            return len(range(start, stop, step))\n        elif isinstance(selector, list | np.ndarray):\n            if isinstance(selector, np.ndarray) and selector.dtype == bool:\n                return np.sum(selector)\n            return len(selector)\n        elif isinstance(selector, int | np.integer):\n            return 1\n        else:\n            return self.slaf_array.shape[axis]\n\n    def __getitem__(self, key) -&gt; \"LazyExpressionMatrix\":\n        \"\"\"\n        Lazy slicing - returns a new LazyExpressionMatrix with composed selectors\n        No computation happens until .compute() is called\n        \"\"\"\n        cell_selector, gene_selector = self._parse_key(key)\n\n        # Handle selectors directly as integer indices\n        # For slices, lists, ints - these are already relative to the current view\n        # We need to compose them with existing selectors\n\n        # Create a new LazyExpressionMatrix with composed selectors\n        new_matrix = LazyExpressionMatrix(\n            self.slaf_array, table_name=self.table_name, layer_name=self.layer_name\n        )\n        new_matrix.parent_adata = self.parent_adata\n\n        # Compose selectors (these are relative to current view)\n        new_matrix._cell_selector = self._compose_selectors(\n            self._cell_selector, cell_selector, axis=0\n        )\n        new_matrix._gene_selector = self._compose_selectors(\n            self._gene_selector, gene_selector, axis=1\n        )\n\n        # Update shape based on new selectors\n        new_matrix._update_shape()\n\n        return new_matrix\n\n    def _compose_selectors(self, old, new, axis):\n        \"\"\"Compose two selectors (helper method)\"\"\"\n        if old is None:\n            return new\n        if new is None or (isinstance(new, slice) and new == slice(None)):\n            return old\n\n        # If old is a slice, we need to apply new to the range defined by old\n        if isinstance(old, slice):\n            # Get the range from the old slice\n            old_start = old.start or 0\n            old_stop = old.stop or self.slaf_array.shape[axis]\n            old_step = old.step or 1\n\n            # Handle negative indices in old slice\n            if old_start &lt; 0:\n                old_start = self.slaf_array.shape[axis] + old_start\n            if old_stop &lt; 0:\n                old_stop = self.slaf_array.shape[axis] + old_stop\n\n            # Clamp old slice bounds\n            old_start = max(0, min(old_start, self.slaf_array.shape[axis]))\n            old_stop = max(0, min(old_stop, self.slaf_array.shape[axis]))\n\n            # Create the range from the old slice\n            old_range = list(range(old_start, old_stop, old_step))\n\n            # Now apply the new selector to this range\n            if isinstance(new, slice):\n                # Apply new slice to the old range\n                new_start = new.start or 0\n                new_stop = new.stop or len(old_range)\n                new_step = new.step or 1\n\n                # Handle negative indices in new slice\n                if new_start &lt; 0:\n                    new_start = len(old_range) + new_start\n                if new_stop &lt; 0:\n                    new_stop = len(old_range) + new_stop\n\n                # Clamp new slice bounds\n                new_start = max(0, min(new_start, len(old_range)))\n                new_stop = max(0, min(new_stop, len(old_range)))\n\n                # Apply the new slice to the old range\n                result_indices = old_range[new_start:new_stop:new_step]\n                return result_indices\n            elif isinstance(new, int | np.integer):\n                # Single index into the old range\n                if 0 &lt;= new &lt; len(old_range):\n                    return [old_range[new]]\n                else:\n                    return []\n            elif isinstance(new, list | np.ndarray):\n                # List of indices into the old range\n                result = []\n                for idx in new:\n                    if 0 &lt;= idx &lt; len(old_range):\n                        result.append(old_range[idx])\n                return result\n            else:\n                return new\n\n        # If old is a list of indices, apply new to those indices\n        elif isinstance(old, list | np.ndarray):\n            if isinstance(new, slice):\n                # Apply slice to the old list\n                new_start = new.start or 0\n                new_stop = new.stop or len(old)\n                new_step = new.step or 1\n\n                # Handle negative indices\n                if new_start &lt; 0:\n                    new_start = len(old) + new_start\n                if new_stop &lt; 0:\n                    new_stop = len(old) + new_stop\n\n                # Clamp bounds\n                new_start = max(0, min(new_start, len(old)))\n                new_stop = max(0, min(new_stop, len(old)))\n\n                return old[new_start:new_stop:new_step]\n            elif isinstance(new, int | np.integer):\n                # Single index into the old list\n                if 0 &lt;= new &lt; len(old):\n                    return [old[new]]\n                else:\n                    return []\n            elif isinstance(new, list | np.ndarray):\n                # List of indices into the old list\n                result = []\n                for idx in new:\n                    if 0 &lt;= idx &lt; len(old):\n                        result.append(old[idx])\n                return result\n            else:\n                return new\n\n        # For other cases, return the new selector as fallback\n        return new\n\n    def _estimate_slice_size(self, cell_selector, gene_selector) -&gt; int:\n        \"\"\"Estimate the size of the slice for strategy selection\"\"\"\n        cell_count = self._estimate_selected_count(cell_selector, axis=0)\n        gene_count = self._estimate_selected_count(gene_selector, axis=1)\n        return cell_count * gene_count\n\n    def _apply_transformations(\n        self,\n        matrix: scipy.sparse.csr_matrix,\n        cell_selector,\n        gene_selector,\n    ) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"Apply any stored transformations to the matrix\"\"\"\n        # Get transformations from the parent LazyAnnData if available\n        if self.parent_adata is not None and hasattr(\n            self.parent_adata, \"_transformations\"\n        ):\n            transformations = self.parent_adata._transformations\n        else:\n            return matrix\n\n        # Avoid copying if no transformations\n        if not transformations:\n            return matrix\n\n        # Try to apply transformations at SQL level first\n        sql_transformed = self._apply_sql_transformations(\n            cell_selector, gene_selector, transformations\n        )\n\n        if sql_transformed is not None:\n            # SQL transformations were applied, reconstruct matrix\n            return self._reconstruct_sparse_matrix(\n                sql_transformed, cell_selector, gene_selector\n            )\n\n        # Fall back to numpy transformations\n        return self._apply_numpy_transformations(\n            matrix, cell_selector, gene_selector, transformations\n        )\n\n    def _apply_sql_transformations(\n        self, cell_selector, gene_selector, transformations\n    ) -&gt; pd.DataFrame | None:\n        \"\"\"Apply transformations at SQL level when possible\"\"\"\n        # Check if we can apply all transformations in SQL\n        sql_applicable = []\n        numpy_needed = []\n\n        for transform_name, transform_data in transformations.items():\n            if transform_name == \"normalize_total\":\n                # normalize_total can be applied in SQL\n                sql_applicable.append((transform_name, transform_data))\n            elif transform_name == \"log1p\":\n                # log1p can be applied in SQL\n                sql_applicable.append((transform_name, transform_data))\n            else:\n                # Unknown transformation, needs numpy\n                numpy_needed.append((transform_name, transform_data))\n\n        # If we have numpy-only transformations, don't use SQL\n        if numpy_needed:\n            return None\n\n        # Build SQL query with transformations\n        # Convert list of tuples to dictionary for _build_transformed_query\n        sql_transformations = dict(sql_applicable)\n        query = self._build_transformed_query(\n            cell_selector, gene_selector, sql_transformations\n        )\n\n        try:\n            # Execute the transformed query\n            return self.slaf_array.query(query)\n        except Exception:\n            # If SQL transformation fails, fall back to numpy\n            return None\n\n    def _build_transformed_query(\n        self, cell_selector, gene_selector, transformations\n    ) -&gt; str:\n        \"\"\"Build SQL query with transformations applied\"\"\"\n        # Build base SQL query string\n        base_query = self._build_submatrix_sql(cell_selector, gene_selector)\n\n        # Apply transformations in order\n        transformed_query = base_query\n        for transform_name, transform_data in transformations.items():\n            if transform_name == \"normalize_total\":\n                transformed_query = self._apply_sql_normalize_total(\n                    transformed_query, transform_data\n                )\n            elif transform_name == \"log1p\":\n                transformed_query = self._apply_sql_log1p(transformed_query)\n\n        return transformed_query\n\n    def _build_submatrix_sql(self, cell_selector, gene_selector) -&gt; str:\n        \"\"\"Build SQL query string for submatrix selection\"\"\"\n        # Use the QueryOptimizer to build the SQL string\n        from slaf.core.query_optimizer import QueryOptimizer\n\n        return QueryOptimizer.build_submatrix_query(\n            cell_selector=cell_selector,\n            gene_selector=gene_selector,\n            cell_count=self.slaf_array.shape[0],  # Use original dataset dimensions\n            gene_count=self.slaf_array.shape[1],  # Use original dataset dimensions\n            table_name=self.table_name,\n            layer_name=self.layer_name,\n        )\n\n    def _apply_sql_normalize_total(self, query: str, transform_data: dict) -&gt; str:\n        \"\"\"Apply normalize_total transformation in SQL\"\"\"\n        cell_factors = transform_data[\"cell_factors\"]\n        # target_sum = transform_data[\"target_sum\"]  # Removed unused variable\n\n        # Create a CASE statement for cell factors\n        case_statements = []\n        for cell_id, factor in cell_factors.items():\n            # Convert scientific notation to regular decimal format for SQL compatibility\n            factor_str = (\n                f\"{factor:.10f}\".rstrip(\"0\").rstrip(\".\") if factor != 0 else \"0\"\n            )\n\n            # Handle both integer and string cell IDs\n            if isinstance(cell_id, str) and cell_id.startswith(\"cell_\"):\n                # Extract integer from string like \"cell_0\" -&gt; 0\n                try:\n                    cell_integer_id_int = int(cell_id.split(\"_\")[1])\n                except (ValueError, IndexError):\n                    # Fallback: skip this cell if we can't parse it\n                    continue\n            elif isinstance(cell_id, int | np.integer):\n                cell_integer_id_int = int(cell_id)\n            else:\n                # Skip if we can't handle this cell ID type\n                continue\n\n            case_statements.append(\n                f\"WHEN e.cell_integer_id = {cell_integer_id_int} THEN {factor_str}\"\n            )\n\n        factor_case = \"CASE \" + \" \".join(case_statements) + \" ELSE 1.0 END\"\n\n        # Wrap the query to apply normalization\n        return f\"\"\"\n        SELECT\n            e.cell_integer_id,\n            e.gene_integer_id,\n            e.value * {factor_case} as value\n        FROM ({query}) as base_data e\n        \"\"\"\n\n    def _apply_sql_log1p(self, query: str) -&gt; str:\n        \"\"\"Apply log1p transformation in SQL, only to nonzero values (sparse semantics)\"\"\"\n        return f\"\"\"\n        SELECT\n            e.cell_integer_id,\n            e.gene_integer_id,\n            CASE WHEN e.value != 0 THEN LN(1 + e.value) ELSE 0 END as value\n        FROM ({query}) as base_data e\n        \"\"\"\n\n    def _apply_numpy_transformations(\n        self,\n        matrix: scipy.sparse.csr_matrix,\n        cell_selector,\n        gene_selector,\n        transformations,\n    ) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"Apply transformations using numpy operations\"\"\"\n        # Apply transformations in the order they were added (preserves order)\n        result = matrix\n        for transform_name, transform_data in transformations.items():\n            if transform_name == \"normalize_total\":\n                result = self._apply_normalize_total(\n                    result, cell_selector, transform_data\n                )\n            elif transform_name == \"log1p\":\n                result = self._apply_log1p(result)\n\n        return result\n\n    def _apply_normalize_total(\n        self,\n        matrix: scipy.sparse.csr_matrix,\n        cell_selector,\n        transform_data,\n    ) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"Apply normalize_total transformation using vectorized operations\"\"\"\n        cell_factors = transform_data.get(\"cell_factors\", {})\n        obs_names_local = []  # Always a list\n        obs_names = None\n        if hasattr(self, \"parent_adata\") and self.parent_adata is not None:\n            try:\n                obs_names = self.parent_adata.obs_names\n            except (AttributeError, TypeError):\n                obs_names = None\n        if obs_names is None or not isinstance(obs_names, list | np.ndarray | pd.Index):\n            if (\n                matrix is not None\n                and hasattr(matrix, \"shape\")\n                and matrix.shape is not None\n            ):\n                obs_names_local = [f\"cell_{i}\" for i in range(matrix.shape[0])]\n            else:\n                obs_names_local = []\n        else:\n            obs_names_local = list(obs_names)\n        # Now obs_names_local is always a list\n        # Determine selected_cell_names based on cell_selector\n        if cell_selector is None or (\n            isinstance(cell_selector, slice) and cell_selector == slice(None)\n        ):\n            selected_cell_names = obs_names_local\n        elif isinstance(cell_selector, slice):\n            start = cell_selector.start or 0\n            stop = cell_selector.stop or len(obs_names_local)\n            step = cell_selector.step or 1\n            # Clamp bounds\n            start = max(0, min(start, len(obs_names_local)))\n            stop = max(0, min(stop, len(obs_names_local)))\n            selected_cell_names = obs_names_local[start:stop:step]\n        elif isinstance(cell_selector, list | np.ndarray):\n            if len(obs_names_local) &gt; 0:\n                if (\n                    isinstance(cell_selector, np.ndarray)\n                    and cell_selector.dtype == bool\n                ):\n                    selected_cell_names = [\n                        obs_names_local[i]\n                        for i, keep in enumerate(cell_selector)\n                        if keep and 0 &lt;= i &lt; len(obs_names_local)\n                    ]\n                else:\n                    selected_cell_names = [\n                        obs_names_local[i]\n                        for i in cell_selector\n                        if isinstance(i, int | np.integer)\n                        and 0 &lt;= i &lt; len(obs_names_local)\n                    ]\n            else:\n                selected_cell_names = []\n        elif isinstance(cell_selector, int | np.integer):\n            if 0 &lt;= cell_selector &lt; len(obs_names_local):\n                selected_cell_names = [obs_names_local[cell_selector]]\n            else:\n                selected_cell_names = []\n        else:\n            selected_cell_names = obs_names_local\n        # Create a vector of factors for all cells at once\n        cell_factors_vector = np.array(\n            [cell_factors.get(name, 1.0) for name in selected_cell_names]\n        )\n        # Apply vectorized scaling using CSR matrix properties\n        # Create a copy only if we need to modify the data\n        if not np.allclose(cell_factors_vector, 1.0):\n            result = matrix.copy()\n            # Apply scaling to each row using vectorized operations\n            for i in range(result.shape[0]):\n                if i &lt; len(cell_factors_vector):\n                    factor = cell_factors_vector[i]\n                    if factor != 1.0:\n                        # Scale the non-zero elements in this row\n                        start_idx = result.indptr[i]\n                        end_idx = result.indptr[i + 1]\n                        result.data[start_idx:end_idx] *= factor\n            return result\n        else:\n            # No scaling needed, return original matrix\n            return matrix\n\n    def _apply_log1p(self, matrix: scipy.sparse.csr_matrix) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"Apply log1p transformation using vectorized operations\"\"\"\n        # Create a copy only if we need to modify the data\n        result = matrix.copy()\n        # Apply log1p to all non-zero values using vectorized operation\n        result.data = np.log1p(result.data)\n        return result\n\n    def __array_function__(self, func, types, args, kwargs):\n        \"\"\"Intercept numpy functions for lazy evaluation\"\"\"\n        if func == np.mean:\n            axis = kwargs.get(\"axis\", None)\n            return self.mean(axis=axis)\n        elif func == np.sum:\n            axis = kwargs.get(\"axis\", None)\n            return self.sum(axis=axis)\n        elif func == np.var:\n            axis = kwargs.get(\"axis\", None)\n            return self.var(axis=axis)\n        elif func == np.std:\n            axis = kwargs.get(\"axis\", None)\n            return self.std(axis=axis)\n        else:\n            # Fall back to materializing the matrix\n            matrix = self.compute()\n            return func(matrix, *args[1:], **kwargs)\n\n    def mean(\n        self, axis: int | None = None, fragments: bool | None = None\n    ) -&gt; float | np.ndarray:\n        \"\"\"Compute mean along axis via SQL aggregation\"\"\"\n        return self._aggregation_with_fragments(\"mean\", fragments, axis=axis)\n\n    def sum(\n        self, axis: int | None = None, fragments: bool | None = None\n    ) -&gt; float | np.ndarray:\n        \"\"\"Compute sum along axis via SQL aggregation\"\"\"\n        return self._aggregation_with_fragments(\"sum\", fragments, axis=axis)\n\n    def var(self, axis: int | None = None) -&gt; float | np.ndarray:\n        \"\"\"Compute variance along axis via SQL aggregation\"\"\"\n        return self._sql_aggregation(\"variance\", axis)\n\n    def std(self, axis: int | None = None) -&gt; float | np.ndarray:\n        \"\"\"Compute standard deviation along axis\"\"\"\n        return self._sql_aggregation(\"stddev\", axis)\n\n    def toarray(self) -&gt; np.ndarray:\n        \"\"\"Convert to dense numpy array\"\"\"\n        matrix = self.compute()\n        return matrix.toarray()\n\n    def compute(self, fragments: bool | None = None) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"\n        Explicitly compute the matrix with all transformations applied.\n        This is where the actual SQL query and materialization happens.\n\n        Args:\n            fragments: Whether to use fragment processing (None for automatic)\n\n        Returns:\n            Sparse matrix with transformations applied\n        \"\"\"\n        # Determine processing strategy\n        if fragments is not None:\n            use_fragments = fragments\n        else:\n            # Check if dataset has multiple fragments\n            try:\n                fragments_list = self.slaf_array.expression.get_fragments()\n                use_fragments = len(fragments_list) &gt; 1\n            except Exception:\n                use_fragments = False\n\n        # Fragment processing now supports both expression and layers tables\n        if use_fragments:\n            processor = FragmentProcessor(\n                self.slaf_array,\n                cell_selector=self._cell_selector,\n                gene_selector=self._gene_selector,\n                max_workers=4,\n                enable_caching=True,\n                table_name=self.table_name,\n                layer_name=self.layer_name,\n            )\n            # Use smart strategy selection for optimal performance\n            lazy_pipeline = processor.build_lazy_pipeline_smart(\"compute_matrix\")\n            result = processor.compute(lazy_pipeline)\n            return self._convert_to_sparse_matrix(result)\n        else:\n            return self._compute_global()\n\n    def _compute_global(self) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"Compute the matrix globally (original implementation)\"\"\"\n        # Build the SQL query with transformations\n        if self.parent_adata is not None and hasattr(\n            self.parent_adata, \"_transformations\"\n        ):\n            transformations = self.parent_adata._transformations\n        else:\n            transformations = {}\n\n        # For layers table, use Lance's native filter pushdown instead of SQL to avoid Substrait issues\n        if self.table_name == \"layers\":\n            base_result = self._query_layers_with_lance()\n        else:\n            # Try SQL-level transformations first\n            if transformations:\n                sql_result = self._apply_sql_transformations(\n                    self._cell_selector, self._gene_selector, transformations\n                )\n                if sql_result is not None:\n                    return self._reconstruct_sparse_matrix(\n                        sql_result, self._cell_selector, self._gene_selector\n                    )\n\n            # Fall back to base query + numpy transformations\n            base_query = self._build_submatrix_sql(\n                self._cell_selector, self._gene_selector\n            )\n            base_result = self.slaf_array.query(base_query)\n\n        # Reconstruct base matrix\n        base_matrix = self._reconstruct_sparse_matrix(\n            base_result, self._cell_selector, self._gene_selector\n        )\n\n        # Apply transformations in numpy if needed\n        if transformations:\n            return self._apply_numpy_transformations(\n                base_matrix,\n                self._cell_selector,\n                self._gene_selector,\n                transformations,\n            )\n\n        return base_matrix\n\n    def _query_layers_with_lance(self) -&gt; pl.DataFrame:\n        \"\"\"\n        Query layers table using Lance's native filter pushdown (avoids Substrait issues).\n\n        This method uses Lance's native `to_table()` with SQL filter strings, which\n        provides efficient filter pushdown without the Substrait casting issues that\n        occur with Polars' scan_pyarrow_dataset.\n\n        According to https://lance.org/guide/read_and_write/#reading-lance-dataset,\n        Lance supports native SQL filter pushdown which is more efficient and reliable\n        than going through Polars' PyArrow dataset scanning.\n\n        Args:\n            Returns Polars DataFrame with columns: cell_integer_id, gene_integer_id, value\n        \"\"\"\n        if self.slaf_array.layers is None:\n            raise ValueError(\"Layers table not available in this dataset\")\n        if self.layer_name is None:\n            raise ValueError(\"layer_name must be provided when table_name='layers'\")\n\n        # Build SQL filter conditions for cell and gene selectors\n        cell_condition = self._selector_to_sql_condition(\n            self._cell_selector, axis=0, entity_type=\"cell\"\n        )\n        gene_condition = self._selector_to_sql_condition(\n            self._gene_selector, axis=1, entity_type=\"gene\"\n        )\n\n        # Compose filter string (combine cell and gene conditions with AND)\n        filter_parts = []\n        if cell_condition != \"TRUE\":\n            filter_parts.append(cell_condition)\n        if gene_condition != \"TRUE\":\n            filter_parts.append(gene_condition)\n        # Also filter out NULL values for the layer column (sparse representation)\n        filter_parts.append(f\"{self.layer_name} IS NOT NULL\")\n\n        # Compose final filter string\n        if filter_parts:\n            filter_str = \" AND \".join(filter_parts)\n        else:\n            filter_str = f\"{self.layer_name} IS NOT NULL\"\n\n        # Use Lance's native to_table() with filter pushdown\n        # Select only the columns we need\n        table = self.slaf_array.layers.to_table(\n            columns=[\"cell_integer_id\", \"gene_integer_id\", self.layer_name],\n            filter=filter_str,\n        )\n\n        # Convert to Polars DataFrame\n        # Type cast: to_table() with multiple columns always returns a DataFrame\n        df: pl.DataFrame = pl.from_arrow(table)  # type: ignore[assignment]\n\n        # Rename layer column to \"value\" for consistency with expression table\n        df = df.rename({self.layer_name: \"value\"})\n\n        return df\n\n    def _convert_to_sparse_matrix(\n        self, result_df: pl.DataFrame\n    ) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"\n        Convert fragment processing result to sparse matrix.\n\n        Args:\n            result_df: Polars DataFrame from fragment processing\n\n        Returns:\n            Sparse matrix representation\n        \"\"\"\n        if len(result_df) == 0:\n            # Return empty matrix with appropriate shape\n            return scipy.sparse.csr_matrix(self.shape)\n\n        # Convert to COO format for efficient sparse matrix construction\n        rows = result_df[\"cell_integer_id\"].to_numpy()\n        cols = result_df[\"gene_integer_id\"].to_numpy()\n        data = result_df[\"value\"].to_numpy()\n\n        # Remap integer IDs to local coordinates if selectors are applied\n        if self._cell_selector is not None:\n            if isinstance(self._cell_selector, slice):\n                start = self._cell_selector.start or 0\n                if start &gt; 0:\n                    rows = rows - start\n            elif isinstance(self._cell_selector, list | np.ndarray):\n                # For list/array selectors, we need to create a mapping\n                # But this is complex, so for now we'll assume the query already filtered correctly\n                # and we just need to remap if it's a slice\n                pass\n\n        if self._gene_selector is not None:\n            if isinstance(self._gene_selector, slice):\n                start = self._gene_selector.start or 0\n                if start &gt; 0:\n                    cols = cols - start\n            elif isinstance(self._gene_selector, list | np.ndarray):\n                # For list/array selectors, we need to create a mapping\n                # But this is complex, so for now we'll assume the query already filtered correctly\n                # and we just need to remap if it's a slice\n                pass\n\n        # Create sparse matrix\n        return scipy.sparse.coo_matrix((data, (rows, cols)), shape=self.shape).tocsr()\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix-attributes","title":"Attributes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Shape of the expression matrix.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Tuple of (n_cells, n_genes) representing the matrix dimensions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; matrix = LazyExpressionMatrix(slaf_array)\n&gt;&gt;&gt; print(f\"Matrix shape: {matrix.shape}\")\nMatrix shape: (1000, 20000)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.obs_names","title":"<code>obs_names: pd.Index | None</code>  <code>property</code>","text":"<p>Cell names (observations).</p> <p>Returns:</p> Type Description <code>Index | None</code> <p>pandas.Index of cell names if parent AnnData is available, None otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; matrix = adata.X\n&gt;&gt;&gt; print(f\"Cell names: {len(matrix.obs_names)}\")\nCell names: 1000\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.var_names","title":"<code>var_names: pd.Index | None</code>  <code>property</code>","text":"<p>Gene names (variables).</p> <p>Returns:</p> Type Description <code>Index | None</code> <p>pandas.Index of gene names if parent AnnData is available, None otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; matrix = adata.X\n&gt;&gt;&gt; print(f\"Gene names: {len(matrix.var_names)}\")\nGene names: 20000\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.__init__","title":"<code>__init__(slaf_array: SLAFArray, table_name: str = 'expression', layer_name: str | None = None)</code>","text":"<p>Initialize lazy expression matrix with SLAF array.</p> <p>Parameters:</p> Name Type Description Default <code>slaf_array</code> <code>SLAFArray</code> <p>SLAFArray instance containing the single-cell data.        Used for database queries and metadata access.</p> required <code>table_name</code> <code>str</code> <p>Table name to query (\"expression\" or \"layers\"). Default: \"expression\"</p> <code>'expression'</code> <code>layer_name</code> <code>str | None</code> <p>Layer name for layers table (required when table_name=\"layers\").        Default: None</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; matrix = LazyExpressionMatrix(slaf_array)\n&gt;&gt;&gt; print(f\"Initialized with shape: {matrix.shape}\")\nInitialized with shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Check parent reference\n&gt;&gt;&gt; print(f\"Parent adata: {matrix.parent_adata}\")\nParent adata: None\n</code></pre> <pre><code>&gt;&gt;&gt; # Initialize for layers table\n&gt;&gt;&gt; layer_matrix = LazyExpressionMatrix(slaf_array, table_name=\"layers\", layer_name=\"spliced\")\n&gt;&gt;&gt; print(f\"Layer matrix shape: {layer_matrix.shape}\")\nLayer matrix shape: (1000, 20000)\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(\n    self,\n    slaf_array: SLAFArray,\n    table_name: str = \"expression\",\n    layer_name: str | None = None,\n):\n    \"\"\"\n    Initialize lazy expression matrix with SLAF array.\n\n    Args:\n        slaf_array: SLAFArray instance containing the single-cell data.\n                   Used for database queries and metadata access.\n        table_name: Table name to query (\"expression\" or \"layers\"). Default: \"expression\"\n        layer_name: Layer name for layers table (required when table_name=\"layers\").\n                   Default: None\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; matrix = LazyExpressionMatrix(slaf_array)\n        &gt;&gt;&gt; print(f\"Initialized with shape: {matrix.shape}\")\n        Initialized with shape: (1000, 20000)\n\n        &gt;&gt;&gt; # Check parent reference\n        &gt;&gt;&gt; print(f\"Parent adata: {matrix.parent_adata}\")\n        Parent adata: None\n\n        &gt;&gt;&gt; # Initialize for layers table\n        &gt;&gt;&gt; layer_matrix = LazyExpressionMatrix(slaf_array, table_name=\"layers\", layer_name=\"spliced\")\n        &gt;&gt;&gt; print(f\"Layer matrix shape: {layer_matrix.shape}\")\n        Layer matrix shape: (1000, 20000)\n    \"\"\"\n    super().__init__()\n    self.slaf_array = slaf_array\n    self.table_name = table_name\n    self.layer_name = layer_name\n    self.parent_adata: LazyAnnData | None = None\n    # Store slicing selectors\n    self._cell_selector: Any = None\n    self._gene_selector: Any = None\n    # Initialize shape attribute (required by LazySparseMixin)\n    self._shape = self.slaf_array.shape\n    self._cache: dict[str, Any] = {}  # Simple caching for repeated queries\n    # Validate parameters\n    if table_name == \"layers\" and layer_name is None:\n        raise ValueError(\"layer_name must be provided when table_name='layers'\")\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.mean","title":"<code>mean(axis: int | None = None, fragments: bool | None = None) -&gt; float | np.ndarray</code>","text":"<p>Compute mean along axis via SQL aggregation</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def mean(\n    self, axis: int | None = None, fragments: bool | None = None\n) -&gt; float | np.ndarray:\n    \"\"\"Compute mean along axis via SQL aggregation\"\"\"\n    return self._aggregation_with_fragments(\"mean\", fragments, axis=axis)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.sum","title":"<code>sum(axis: int | None = None, fragments: bool | None = None) -&gt; float | np.ndarray</code>","text":"<p>Compute sum along axis via SQL aggregation</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def sum(\n    self, axis: int | None = None, fragments: bool | None = None\n) -&gt; float | np.ndarray:\n    \"\"\"Compute sum along axis via SQL aggregation\"\"\"\n    return self._aggregation_with_fragments(\"sum\", fragments, axis=axis)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.var","title":"<code>var(axis: int | None = None) -&gt; float | np.ndarray</code>","text":"<p>Compute variance along axis via SQL aggregation</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def var(self, axis: int | None = None) -&gt; float | np.ndarray:\n    \"\"\"Compute variance along axis via SQL aggregation\"\"\"\n    return self._sql_aggregation(\"variance\", axis)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.std","title":"<code>std(axis: int | None = None) -&gt; float | np.ndarray</code>","text":"<p>Compute standard deviation along axis</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def std(self, axis: int | None = None) -&gt; float | np.ndarray:\n    \"\"\"Compute standard deviation along axis\"\"\"\n    return self._sql_aggregation(\"stddev\", axis)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.toarray","title":"<code>toarray() -&gt; np.ndarray</code>","text":"<p>Convert to dense numpy array</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def toarray(self) -&gt; np.ndarray:\n    \"\"\"Convert to dense numpy array\"\"\"\n    matrix = self.compute()\n    return matrix.toarray()\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyExpressionMatrix.compute","title":"<code>compute(fragments: bool | None = None) -&gt; scipy.sparse.csr_matrix</code>","text":"<p>Explicitly compute the matrix with all transformations applied. This is where the actual SQL query and materialization happens.</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>bool | None</code> <p>Whether to use fragment processing (None for automatic)</p> <code>None</code> <p>Returns:</p> Type Description <code>csr_matrix</code> <p>Sparse matrix with transformations applied</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def compute(self, fragments: bool | None = None) -&gt; scipy.sparse.csr_matrix:\n    \"\"\"\n    Explicitly compute the matrix with all transformations applied.\n    This is where the actual SQL query and materialization happens.\n\n    Args:\n        fragments: Whether to use fragment processing (None for automatic)\n\n    Returns:\n        Sparse matrix with transformations applied\n    \"\"\"\n    # Determine processing strategy\n    if fragments is not None:\n        use_fragments = fragments\n    else:\n        # Check if dataset has multiple fragments\n        try:\n            fragments_list = self.slaf_array.expression.get_fragments()\n            use_fragments = len(fragments_list) &gt; 1\n        except Exception:\n            use_fragments = False\n\n    # Fragment processing now supports both expression and layers tables\n    if use_fragments:\n        processor = FragmentProcessor(\n            self.slaf_array,\n            cell_selector=self._cell_selector,\n            gene_selector=self._gene_selector,\n            max_workers=4,\n            enable_caching=True,\n            table_name=self.table_name,\n            layer_name=self.layer_name,\n        )\n        # Use smart strategy selection for optimal performance\n        lazy_pipeline = processor.build_lazy_pipeline_smart(\"compute_matrix\")\n        result = processor.compute(lazy_pipeline)\n        return self._convert_to_sparse_matrix(result)\n    else:\n        return self._compute_global()\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyDictionaryViewMixin","title":"<code>LazyDictionaryViewMixin</code>","text":"<p>Base mixin for dictionary-like views (layers, obs, var).</p> <p>Provides common dictionary interface methods that are identical across all view types: layers, obs columns, and var columns.</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyDictionaryViewMixin:\n    \"\"\"\n    Base mixin for dictionary-like views (layers, obs, var).\n\n    Provides common dictionary interface methods that are identical across\n    all view types: layers, obs columns, and var columns.\n    \"\"\"\n\n    def keys(self) -&gt; list[str]:\n        \"\"\"\n        Return list of keys.\n\n        This method must be implemented by subclasses.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement keys()\")\n\n    def __contains__(self, key: str) -&gt; bool:\n        \"\"\"Check if key exists\"\"\"\n        return key in self.keys()\n\n    def __len__(self) -&gt; int:\n        \"\"\"Number of keys\"\"\"\n        return len(self.keys())\n\n    def __iter__(self):\n        \"\"\"Iterate over keys\"\"\"\n        return iter(self.keys())\n\n    def _validate_name(self, key: str):\n        \"\"\"\n        Validate name (alphanumeric + underscore, non-empty).\n\n        Args:\n            key: Name to validate\n\n        Raises:\n            ValueError: If name is empty or contains invalid characters\n        \"\"\"\n        if not key:\n            raise ValueError(\"Name cannot be empty\")\n        if not key.replace(\"_\", \"\").isalnum():\n            raise ValueError(\n                f\"Name '{key}' contains invalid characters. \"\n                \"Only alphanumeric characters and underscores are allowed.\"\n            )\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyDictionaryViewMixin-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyDictionaryViewMixin.keys","title":"<code>keys() -&gt; list[str]</code>","text":"<p>Return list of keys.</p> <p>This method must be implemented by subclasses.</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def keys(self) -&gt; list[str]:\n    \"\"\"\n    Return list of keys.\n\n    This method must be implemented by subclasses.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement keys()\")\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyLayersView","title":"<code>LazyLayersView</code>","text":"<p>               Bases: <code>LazyDictionaryViewMixin</code></p> <p>Dictionary-like view of layers with lazy evaluation.</p> <p>LazyLayersView provides a dictionary-like interface for accessing AnnData layers stored in the layers.lance table. It supports reading layers as LazyExpressionMatrix objects and provides methods to list, check, and iterate over available layers.</p> Key Features <ul> <li>Dictionary-like interface: layers[\"name\"], \"name\" in layers, len(layers)</li> <li>Lazy evaluation: layers are accessed on-demand</li> <li>Config.json consistency: reads from config for fast layer discovery</li> <li>Backward compatibility: works with datasets without layers</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access a layer\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; spliced = adata.layers[\"spliced\"]\n&gt;&gt;&gt; print(f\"Layer shape: {spliced.shape}\")\nLayer shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # List available layers\n&gt;&gt;&gt; print(list(adata.layers.keys()))\n['spliced', 'unspliced']\n</code></pre> <pre><code>&gt;&gt;&gt; # Check if layer exists\n&gt;&gt;&gt; assert \"spliced\" in adata.layers\n&gt;&gt;&gt; assert \"nonexistent\" not in adata.layers\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyLayersView(LazyDictionaryViewMixin):\n    \"\"\"\n    Dictionary-like view of layers with lazy evaluation.\n\n    LazyLayersView provides a dictionary-like interface for accessing AnnData layers\n    stored in the layers.lance table. It supports reading layers as LazyExpressionMatrix\n    objects and provides methods to list, check, and iterate over available layers.\n\n    Key Features:\n        - Dictionary-like interface: layers[\"name\"], \"name\" in layers, len(layers)\n        - Lazy evaluation: layers are accessed on-demand\n        - Config.json consistency: reads from config for fast layer discovery\n        - Backward compatibility: works with datasets without layers\n\n    Examples:\n        &gt;&gt;&gt; # Access a layer\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; spliced = adata.layers[\"spliced\"]\n        &gt;&gt;&gt; print(f\"Layer shape: {spliced.shape}\")\n        Layer shape: (1000, 20000)\n\n        &gt;&gt;&gt; # List available layers\n        &gt;&gt;&gt; print(list(adata.layers.keys()))\n        ['spliced', 'unspliced']\n\n        &gt;&gt;&gt; # Check if layer exists\n        &gt;&gt;&gt; assert \"spliced\" in adata.layers\n        &gt;&gt;&gt; assert \"nonexistent\" not in adata.layers\n    \"\"\"\n\n    def __init__(self, lazy_adata: \"LazyAnnData\"):\n        \"\"\"\n        Initialize LazyLayersView with LazyAnnData.\n\n        Args:\n            lazy_adata: LazyAnnData instance containing the single-cell data.\n        \"\"\"\n        self.lazy_adata = lazy_adata\n        self._slaf_array = lazy_adata.slaf\n\n    def _get_layers_from_config(self) -&gt; list[str]:\n        \"\"\"Get layer names from config.json (fast path)\"\"\"\n        if self._slaf_array.layers is None:\n            return []\n\n        layers_config = self._slaf_array.config.get(\"layers\", {})\n        return layers_config.get(\"available\", [])\n\n    def _get_layers_from_table(self) -&gt; list[str]:\n        \"\"\"Get layer names by querying layers table (fallback)\"\"\"\n        if self._slaf_array.layers is None:\n            return []\n\n        try:\n            # Query to get distinct layer column names\n            # In wide format, we need to check which columns exist\n            schema = self._slaf_array.layers.schema\n            column_names = [field.name for field in schema]\n\n            # Filter out cell_integer_id and gene_integer_id\n            layer_names = [\n                col\n                for col in column_names\n                if col not in (\"cell_integer_id\", \"gene_integer_id\")\n            ]\n            return layer_names\n        except Exception:\n            return []\n\n    def keys(self) -&gt; list[str]:\n        \"\"\"List all available layer names\"\"\"\n        # Fast path: read from config.json\n        config_layers = self._get_layers_from_config()\n\n        if config_layers:\n            # Verify consistency with table if possible\n            table_layers = self._get_layers_from_table()\n            if table_layers and set(config_layers) != set(table_layers):\n                # Log warning but prefer config\n                import warnings\n\n                warnings.warn(\n                    f\"Layer names in config.json ({config_layers}) don't match \"\n                    f\"layers table ({table_layers}). Using config.json values.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n            return config_layers\n\n        # Fallback: query table\n        return self._get_layers_from_table()\n\n    def __getitem__(self, key: str) -&gt; LazyExpressionMatrix:\n        \"\"\"Get a layer as LazyExpressionMatrix\"\"\"\n        # Validate layer exists\n        if key not in self.keys():\n            raise KeyError(f\"Layer '{key}' not found\")\n\n        # Create LazyExpressionMatrix pointing to layers table\n        layer_matrix = LazyExpressionMatrix(\n            self._slaf_array, table_name=\"layers\", layer_name=key\n        )\n        layer_matrix.parent_adata = self.lazy_adata\n\n        # Propagate cell/gene selectors from parent LazyAnnData\n        if hasattr(self.lazy_adata, \"_cell_selector\"):\n            layer_matrix._cell_selector = self.lazy_adata._cell_selector\n        if hasattr(self.lazy_adata, \"_gene_selector\"):\n            layer_matrix._gene_selector = self.lazy_adata._gene_selector\n        layer_matrix._update_shape()\n\n        return layer_matrix\n\n    def _is_immutable(self, key: str) -&gt; bool:\n        \"\"\"Check if a layer is immutable (converted from h5ad)\"\"\"\n        layers_config = self._slaf_array.config.get(\"layers\", {})\n        immutable_layers = layers_config.get(\"immutable\", [])\n        return key in immutable_layers\n\n    def __setitem__(\n        self, key: str, value: \"LazyExpressionMatrix | scipy.sparse.spmatrix\"\n    ):\n        \"\"\"\n        Create or update a layer (lazy write - requires commit()).\n\n        Stores the assignment in a pending writes queue. The actual write to\n        layers.lance happens when commit() is called. This allows batching\n        multiple layer operations and ensures config.json consistency.\n\n        Args:\n            key: Layer name (must be alphanumeric + underscore, non-empty)\n            value: LazyExpressionMatrix or scipy sparse matrix to assign.\n                   Must have the same shape as adata.X.\n\n        Raises:\n            ValueError: If layer name is invalid, shape doesn't match X,\n                       or trying to overwrite an immutable layer.\n        \"\"\"\n        # Validate layer name\n        self._validate_name(key)\n\n        # Validate shape matches X\n        if value.shape != self.lazy_adata.shape:\n            raise ValueError(\n                f\"Layer shape {value.shape} doesn't match X shape {self.lazy_adata.shape}\"\n            )\n\n        # Check if layer already exists and is immutable\n        if key in self.keys():\n            if self._is_immutable(key):\n                raise ValueError(\n                    f\"Layer '{key}' is immutable (converted from h5ad) and cannot be overwritten\"\n                )\n\n        # Convert to materialized sparse matrix if needed\n        from_lazy = isinstance(value, LazyExpressionMatrix)\n        if from_lazy:\n            value = value.compute()  # Materialize the lazy matrix\n\n        # Ensure it's a sparse matrix\n        import scipy.sparse\n\n        if not scipy.sparse.issparse(value):\n            # Convert dense to sparse\n            value = scipy.sparse.csr_matrix(value)\n\n        # Write immediately (eager write)\n        self._write_layer_immediate(key, value, from_lazy=from_lazy)\n\n    def _write_layer_immediate(\n        self, layer_name: str, layer_matrix, from_lazy: bool = False\n    ):\n        \"\"\"\n        Write layer immediately to layers.lance (eager write).\n\n        This method handles the immediate write of a layer to the layers.lance table.\n        It's designed for small datasets (~10k cells) where the entire layer fits in memory.\n\n        Args:\n            layer_name: Name of the layer\n            layer_matrix: Sparse matrix to write\n            from_lazy: If True, the matrix came from LazyExpressionMatrix.compute(),\n                      so coo.row and coo.col are already integer IDs\n        \"\"\"\n        # Ensure layers table exists (create if needed)\n        layers_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path,\n            self._slaf_array.config.get(\"tables\", {}).get(\"layers\", \"layers.lance\"),\n        )\n\n        # Convert sparse matrix to COO\n        coo = layer_matrix.tocoo()\n\n        # Get cell and gene integer IDs\n        if from_lazy:\n            # Matrix came from LazyExpressionMatrix.compute(), so row/col are already integer IDs\n            cell_integer_ids = coo.row\n            gene_integer_ids = coo.col\n        else:\n            # Matrix came from external source, need to map row/col indices to integer IDs\n            obs_df = self.lazy_adata.slaf.obs\n            var_df = self.lazy_adata.slaf.var\n            cell_integer_ids = obs_df[\"cell_integer_id\"].to_numpy()[coo.row]\n            gene_integer_ids = var_df[\"gene_integer_id\"].to_numpy()[coo.col]\n\n        # Optimize dtype\n        values, value_pa_type = self._optimize_dtype_for_layer(coo.data)\n\n        # Create PyArrow table with the new layer column\n        layer_table = pa.table(\n            {\n                \"cell_integer_id\": pa.array(cell_integer_ids, type=pa.uint32()),\n                \"gene_integer_id\": pa.array(gene_integer_ids, type=pa.uint16()),\n                layer_name: pa.array(values, type=value_pa_type),\n            }\n        )\n\n        # Check if layers table exists\n        if self._slaf_array.layers is None:\n            # Create new layers table from expression table structure\n            self._create_layers_table_with_layer(layer_table, layer_name, layers_path)\n        else:\n            # Update existing layers table\n            self._update_layers_table_with_layer(layer_table, layer_name, layers_path)\n\n        # Update config.json atomically\n        self._update_config_layers_list([layer_name], add=True)\n\n    def _optimize_dtype_for_layer(self, data: np.ndarray) -&gt; tuple[np.ndarray, str]:\n        \"\"\"\n        Optimize dtype for layer values.\n\n        If float32 data contains only integers within uint16 range, convert to uint16.\n        Otherwise, use float32.\n\n        Args:\n            data: Array of layer values\n\n        Returns:\n            Tuple of (optimized values array, value type string)\n        \"\"\"\n        if len(data) == 0:\n            return np.array([], dtype=np.float32), \"float32\"\n\n        # Sample data to determine dtype\n        sample_size = min(10000, len(data))\n        sample_data = data[:sample_size]\n\n        # Check if data is integer or float\n        is_integer = np.issubdtype(sample_data.dtype, np.integer)\n        max_value = np.max(data)\n        min_value = np.min(data)\n\n        if is_integer and max_value &lt;= 65535 and min_value &gt;= 0:\n            return data.astype(np.uint16), \"uint16\"\n        elif not is_integer:\n            # Check if float data contains only integer values\n            rounded_data = np.round(data)\n            is_integer_values = np.allclose(data, rounded_data, rtol=1e-10)\n\n            if is_integer_values and max_value &lt;= 65535 and min_value &gt;= 0:\n                return rounded_data.astype(np.uint16), \"uint16\"\n            else:\n                return data.astype(np.float32), \"float32\"\n        else:\n            return data.astype(np.float32), \"float32\"\n\n    def _get_pyarrow_type(self, value_type: str) -&gt; pa.DataType:\n        \"\"\"\n        Get PyArrow data type for a given value type string.\n\n        Args:\n            value_type: \"uint16\" or \"float32\"\n\n        Returns:\n            PyArrow data type\n        \"\"\"\n        if value_type == \"uint16\":\n            return pa.uint16()\n        elif value_type == \"float32\":\n            return pa.float32()\n        else:\n            raise ValueError(f\"Unsupported value type: {value_type}\")\n\n    def _create_layers_table_with_layer(\n        self, layer_table: pa.Table, layer_name: str, layers_path: str\n    ):\n        \"\"\"Create new layers table with a single layer.\"\"\"\n        import lance\n\n        # Get base structure from expression table (all cell-gene pairs)\n        expression_df = (\n            pl.scan_pyarrow_dataset(self._slaf_array.expression)\n            .select([\"cell_integer_id\", \"gene_integer_id\"])\n            .unique()\n            .collect()\n        )\n\n        # Convert to PyArrow\n        base_table = expression_df.to_arrow()\n\n        # Join with new layer data\n        layer_df_raw = pl.from_arrow(layer_table)\n        base_df_raw = pl.from_arrow(base_table)\n        assert isinstance(layer_df_raw, pl.DataFrame), (\n            \"Expected DataFrame from Arrow table\"\n        )\n        assert isinstance(base_df_raw, pl.DataFrame), (\n            \"Expected DataFrame from Arrow table\"\n        )\n        layer_df = layer_df_raw\n        base_df = base_df_raw\n\n        # Left join to add layer column (nullable for sparse data)\n        combined_df = base_df.join(\n            layer_df, on=[\"cell_integer_id\", \"gene_integer_id\"], how=\"left\"\n        )\n\n        # Write new layers table\n        lance.write_dataset(\n            combined_df.to_arrow(),\n            layers_path,\n            mode=\"create\",\n            max_rows_per_file=10000000,\n        )\n\n        # Reload layers dataset\n        self._slaf_array.layers = lance.dataset(layers_path)\n\n    def _update_layers_table_with_layer(\n        self, layer_table: pa.Table, layer_name: str, layers_path: str\n    ):\n        \"\"\"Update existing layers table with a new/updated layer using add_columns() with UDF.\"\"\"\n        import lance\n\n        layers_dataset = self._slaf_array.layers\n\n        # Check if layer column already exists\n        schema = layers_dataset.schema\n        column_names = [field.name for field in schema]\n\n        # Drop the old column if it exists (using Lance native method - metadata-only, very fast)\n        if layer_name in column_names:\n            layers_dataset = layers_dataset.drop_columns([layer_name])\n            # Reload from path after drop_columns to get the updated dataset\n            layers_dataset = lance.dataset(layers_path)\n\n        # Convert layer data to polars for efficient lookup in UDF\n        layer_df = pl.from_arrow(layer_table)\n\n        # Create UDF that returns just the new column data\n        # Note: Pass function directly to add_columns(), not decorated\n        def add_layer_column_udf(batch):\n            \"\"\"\n            UDF to add layer column by joining batch with layer data.\n\n            Receives batch with existing columns (cell_integer_id, gene_integer_id, etc.),\n            returns RecordBatch with just the new layer column.\n\n            This processes the dataset in batches, joining each batch with the\n            new layer data to add the column efficiently without rewriting\n            existing data.\n            \"\"\"\n            # Convert batch to polars to access cell_integer_id and gene_integer_id\n            batch_df = pl.from_arrow(batch)\n\n            # Join with layer data (left join preserves all rows, nullable for sparse data)\n            result_df = batch_df.join(\n                layer_df, on=[\"cell_integer_id\", \"gene_integer_id\"], how=\"left\"\n            )\n\n            # Extract just the new layer column and convert to single array\n            new_layer_chunked = result_df.select([layer_name]).to_arrow().column(0)\n            new_layer_array = new_layer_chunked.combine_chunks()\n\n            # Return RecordBatch with just the new column (names must match column name)\n            return pa.RecordBatch.from_arrays(\n                [new_layer_array],\n                names=[layer_name],\n            )\n\n        # Add the column using add_columns() with UDF\n        # This processes in batches and doesn't rewrite existing data\n        layers_dataset.add_columns(add_layer_column_udf)\n\n        # Reload layers dataset from path to ensure consistency\n        self._slaf_array.layers = lance.dataset(layers_path)\n\n    def _update_config_layers_list(self, layer_names: list[str], add: bool):\n        \"\"\"\n        Update config.json to add or remove layers from available/mutable lists.\n\n        Args:\n            layer_names: List of layer names to add or remove\n            add: If True, add layers; if False, remove layers\n        \"\"\"\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n\n        # Load existing config\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            config = json.load(f)\n\n        # Ensure layers config exists\n        if \"layers\" not in config:\n            config[\"layers\"] = {\"available\": [], \"immutable\": [], \"mutable\": []}\n\n        # Ensure tables config includes layers\n        if \"tables\" not in config:\n            config[\"tables\"] = {}\n        if \"layers\" not in config[\"tables\"]:\n            config[\"tables\"][\"layers\"] = \"layers.lance\"\n\n        layers_config = config[\"layers\"]\n        available = set(layers_config.get(\"available\", []))\n        immutable = set(layers_config.get(\"immutable\", []))\n        mutable = set(layers_config.get(\"mutable\", []))\n\n        if add:\n            # Add layers to available and mutable (new layers are mutable)\n            for layer_name in layer_names:\n                available.add(layer_name)\n                mutable.add(layer_name)\n                # Remove from immutable if it was there (shouldn't happen, but be safe)\n                immutable.discard(layer_name)\n        else:\n            # Remove layers from all lists\n            for layer_name in layer_names:\n                available.discard(layer_name)\n                mutable.discard(layer_name)\n                immutable.discard(layer_name)\n\n        # Update config\n        layers_config[\"available\"] = sorted(available)\n        layers_config[\"immutable\"] = sorted(immutable)\n        layers_config[\"mutable\"] = sorted(mutable)\n\n        # Save updated config\n        # Note: this will not work for huggingface remote\n        with self._slaf_array._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n    def __delitem__(self, key: str):\n        \"\"\"\n        Delete a layer (only if mutable).\n\n        Args:\n            key: Layer name to delete\n\n        Raises:\n            KeyError: If layer doesn't exist\n            ValueError: If layer is immutable and cannot be deleted\n        \"\"\"\n        import lance\n\n        # Check if layer exists\n        if key not in self.keys():\n            raise KeyError(f\"Layer '{key}' not found\")\n\n        # Check if layer is immutable\n        if self._is_immutable(key):\n            raise ValueError(\n                f\"Layer '{key}' is immutable (converted from h5ad) and cannot be deleted\"\n            )\n\n        # Get layers path\n        layers_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path,\n            self._slaf_array.config.get(\"tables\", {}).get(\"layers\", \"layers.lance\"),\n        )\n\n        # Drop the column using Lance's drop_columns() method (metadata-only, very fast)\n        layers_dataset = self._slaf_array.layers\n        layers_dataset = layers_dataset.drop_columns([key])\n\n        # Reload layers dataset from path to ensure consistency\n        self._slaf_array.layers = lance.dataset(layers_path)\n\n        # Update config.json atomically\n        self._update_config_layers_list([key], add=False)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyLayersView-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyLayersView.__init__","title":"<code>__init__(lazy_adata: LazyAnnData)</code>","text":"<p>Initialize LazyLayersView with LazyAnnData.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(self, lazy_adata: \"LazyAnnData\"):\n    \"\"\"\n    Initialize LazyLayersView with LazyAnnData.\n\n    Args:\n        lazy_adata: LazyAnnData instance containing the single-cell data.\n    \"\"\"\n    self.lazy_adata = lazy_adata\n    self._slaf_array = lazy_adata.slaf\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyLayersView.keys","title":"<code>keys() -&gt; list[str]</code>","text":"<p>List all available layer names</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def keys(self) -&gt; list[str]:\n    \"\"\"List all available layer names\"\"\"\n    # Fast path: read from config.json\n    config_layers = self._get_layers_from_config()\n\n    if config_layers:\n        # Verify consistency with table if possible\n        table_layers = self._get_layers_from_table()\n        if table_layers and set(config_layers) != set(table_layers):\n            # Log warning but prefer config\n            import warnings\n\n            warnings.warn(\n                f\"Layer names in config.json ({config_layers}) don't match \"\n                f\"layers table ({table_layers}). Using config.json values.\",\n                UserWarning,\n                stacklevel=2,\n            )\n        return config_layers\n\n    # Fallback: query table\n    return self._get_layers_from_table()\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyMetadataViewMixin","title":"<code>LazyMetadataViewMixin</code>","text":"<p>               Bases: <code>LazySparseMixin</code>, <code>LazyDictionaryViewMixin</code></p> <p>Mixin class for metadata view operations (obs/var columns).</p> <p>This mixin provides shared functionality for LazyObsView and LazyVarView, eliminating code duplication. It handles: - Dictionary-like interface (keys, getitem, setitem, delitem) - Column management (create, update, delete) - Config.json synchronization - Selector support - Immutability tracking</p> <p>Required attributes (set by subclasses): - table_type: \"obs\" or \"var\" - table_name: \"cells\" or \"genes\" - id_column: \"cell_integer_id\" or \"gene_integer_id\" - lazy_adata: LazyAnnData instance - _slaf_array: SLAFArray instance - _shape: tuple[int, int] shape</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyMetadataViewMixin(LazySparseMixin, LazyDictionaryViewMixin):\n    \"\"\"\n    Mixin class for metadata view operations (obs/var columns).\n\n    This mixin provides shared functionality for LazyObsView and LazyVarView,\n    eliminating code duplication. It handles:\n    - Dictionary-like interface (keys, __getitem__, __setitem__, __delitem__)\n    - Column management (create, update, delete)\n    - Config.json synchronization\n    - Selector support\n    - Immutability tracking\n\n    Required attributes (set by subclasses):\n    - table_type: \"obs\" or \"var\"\n    - table_name: \"cells\" or \"genes\"\n    - id_column: \"cell_integer_id\" or \"gene_integer_id\"\n    - lazy_adata: LazyAnnData instance\n    - _slaf_array: SLAFArray instance\n    - _shape: tuple[int, int] shape\n    \"\"\"\n\n    # Type annotations for required attributes (set by subclasses)\n    table_type: str\n    table_name: str\n    id_column: str\n    lazy_adata: \"LazyAnnData\"\n    _slaf_array: SLAFArray\n    _shape: tuple[int, int]\n\n    def _get_axis(self) -&gt; int:\n        \"\"\"Get axis for this view (0 for obs/cells/obsm, 1 for var/genes/varm)\"\"\"\n        return 0 if self.table_type in (\"obs\", \"obsm\") else 1\n\n    def _get_current_selector(self) -&gt; Any:\n        \"\"\"Get current selector from parent LazyAnnData\"\"\"\n        # Map table_type to actual selector attribute names in LazyAnnData\n        # obsm uses cell selector, varm uses gene selector\n        selector_map = {\n            \"obs\": \"_cell_selector\",\n            \"var\": \"_gene_selector\",\n            \"obsm\": \"_cell_selector\",\n            \"varm\": \"_gene_selector\",\n        }\n        selector_attr = selector_map.get(self.table_type)\n        if selector_attr:\n            return getattr(self.lazy_adata, selector_attr, None)\n        return None\n\n    def _get_entity_count(self) -&gt; int:\n        \"\"\"Get count of entities considering selectors\"\"\"\n        selector = self._get_current_selector()\n        axis = self._get_axis()\n        if selector is None:\n            return self._shape[axis]\n        return self._get_selector_size(selector, axis)\n\n    def _invalidate_metadata_cache(self):\n        \"\"\"\n        Invalidate cached obs/var DataFrames when table structure changes.\n\n        When we modify cells.lance or genes.lance (add/remove columns),\n        the cached DataFrames in both LazyAnnData and SLAFArray become stale\n        and need to be cleared so they reload from the updated tables.\n        \"\"\"\n        # Invalidate LazyAnnData cache\n        if hasattr(self.lazy_adata, \"_obs\"):\n            self.lazy_adata._obs = None\n        if hasattr(self.lazy_adata, \"_var\"):\n            self.lazy_adata._var = None\n        if hasattr(self.lazy_adata, \"_cached_obs_names\"):\n            self.lazy_adata._cached_obs_names = None\n        if hasattr(self.lazy_adata, \"_cached_var_names\"):\n            self.lazy_adata._cached_var_names = None\n\n        # Invalidate SLAFArray cache (which LazyAnnData.obs/var load from)\n        # This ensures that when LazyAnnData.obs/var are accessed again,\n        # they reload from the updated Lance tables\n        if self.table_type in (\"obs\", \"obsm\"):\n            # Modified cells.lance - invalidate SLAFArray._obs\n            if hasattr(self._slaf_array, \"_obs\"):\n                self._slaf_array._obs = None\n            if hasattr(self._slaf_array, \"_obs_columns\"):\n                self._slaf_array._obs_columns = None\n            # Mark metadata as not loaded so it gets reloaded\n            if hasattr(self._slaf_array, \"_metadata_loaded\"):\n                self._slaf_array._metadata_loaded = False\n        elif self.table_type in (\"var\", \"varm\"):\n            # Modified genes.lance - invalidate SLAFArray._var\n            if hasattr(self._slaf_array, \"_var\"):\n                self._slaf_array._var = None\n            if hasattr(self._slaf_array, \"_var_columns\"):\n                self._slaf_array._var_columns = None\n            # Mark metadata as not loaded so it gets reloaded\n            if hasattr(self._slaf_array, \"_metadata_loaded\"):\n                self._slaf_array._metadata_loaded = False\n\n        # Invalidate view's cached DataFrame\n        if hasattr(self, \"_dataframe\"):\n            self._dataframe = None\n\n    def _sql_condition_to_polars(self, sql_condition: str, id_column: str) -&gt; pl.Expr:\n        \"\"\"Convert SQL WHERE condition to Polars expression\"\"\"\n        if sql_condition == \"TRUE\":\n            return pl.lit(True)\n        if sql_condition == \"FALSE\":\n            return pl.lit(False)\n\n        # Handle range: \"cell_integer_id &gt;= 0 AND cell_integer_id &lt; 100\"\n        if \"&gt;=\" in sql_condition and \"&lt;\" in sql_condition:\n            parts = sql_condition.split(\" AND \")\n            ge_part = [p for p in parts if \"&gt;=\" in p][0]\n            lt_part = [p for p in parts if \"&lt;\" in p][0]\n            try:\n                ge_value = int(ge_part.split(\"&gt;=\")[1].strip())\n                lt_value = int(lt_part.split(\"&lt;\")[1].strip())\n                return (pl.col(id_column) &gt;= ge_value) &amp; (pl.col(id_column) &lt; lt_value)\n            except ValueError:\n                # If values are not integers, return False condition\n                return pl.lit(False)\n\n        # Handle IN clause: \"cell_integer_id IN (0,1,2,3)\"\n        if \" IN \" in sql_condition:\n            values_str = sql_condition.split(\" IN \")[1].strip(\"()\")\n            # Filter out non-numeric values (like \"False\", \"True\", etc.)\n            values = []\n            for v in values_str.split(\",\"):\n                v = v.strip()\n                try:\n                    values.append(int(v))\n                except ValueError:\n                    # Skip non-integer values\n                    continue\n            if values:\n                return pl.col(id_column).is_in(values)\n            else:\n                # If no valid values, return False condition\n                return pl.lit(False)\n\n        # Handle equality: \"cell_integer_id = 5\"\n        if \" = \" in sql_condition:\n            value = int(sql_condition.split(\" = \")[1].strip())\n            return pl.col(id_column) == value\n\n        return pl.lit(True)  # Fallback: no filtering\n\n    def _build_filtered_query(self, columns: list[str]) -&gt; pl.LazyFrame:\n        \"\"\"Build filtered query for table with selectors\"\"\"\n        table = getattr(self._slaf_array, self.table_name)\n        query = pl.scan_pyarrow_dataset(table).select(columns)\n\n        # Apply selector filtering using mixin utilities\n        selector = self._get_current_selector()\n        if selector is not None:\n            # Convert selector to SQL condition, then to Polars filter\n            axis = self._get_axis()\n            entity_type = \"cell\" if self.table_type == \"obs\" else \"gene\"\n            sql_condition = self._selector_to_sql_condition(\n                selector, axis=axis, entity_type=entity_type\n            )\n            filter_expr = self._sql_condition_to_polars(sql_condition, self.id_column)\n            query = query.filter(filter_expr)\n\n        return query\n\n    def _get_columns_from_config(self) -&gt; list[str]:\n        \"\"\"Get column names from config.json (fast path)\"\"\"\n        config = self._slaf_array.config\n        if self.table_type in config and \"available\" in config[self.table_type]:\n            return config[self.table_type][\"available\"]\n        return []\n\n    def _get_columns_from_table(self) -&gt; list[str]:\n        \"\"\"Get column names by querying table (fallback)\"\"\"\n        table = getattr(self._slaf_array, self.table_name, None)\n        if table is None:\n            return []\n\n        try:\n            schema = table.schema\n            column_names = [field.name for field in schema]\n            # Filter out system/internal columns:\n            # - {id_column}: internal integer ID (not user-facing)\n            # - {table_name}_start_index: Lance internal column (if present)\n            system_cols = {self.id_column}\n            # Handle both \"cells\" -&gt; \"cell_start_index\" and \"genes\" -&gt; \"gene_start_index\"\n            if self.table_name == \"cells\":\n                system_cols.add(\"cell_start_index\")\n            elif self.table_name == \"genes\":\n                system_cols.add(\"gene_start_index\")\n            return [col for col in column_names if col not in system_cols]\n        except Exception:\n            return []\n\n    def keys(self) -&gt; list[str]:\n        \"\"\"List all available column/vector names\"\"\"\n        # Route to vector or scalar method based on table_type\n        if self.table_type in (\"obsm\", \"varm\"):\n            return self._keys_vector()\n        else:\n            # Scalar column keys\n            # Fast path: read from config.json\n            config_columns = self._get_columns_from_config()\n\n            if config_columns:\n                # Verify consistency with table if possible\n                table_columns = self._get_columns_from_table()\n                if table_columns:\n                    config_set = set(config_columns)\n                    table_set = set(table_columns)\n\n                    # Check for columns in config but not in table (real problem)\n                    missing_in_table = config_set - table_set\n                    if missing_in_table:\n                        import warnings\n\n                        warnings.warn(\n                            f\"Column names in config.json ({sorted(missing_in_table)}) \"\n                            f\"not found in {self.table_name} table. These will be ignored.\",\n                            UserWarning,\n                            stacklevel=2,\n                        )\n\n                    # Auto-sync: add columns from table that are missing in config\n                    missing_in_config = table_set - config_set\n                    if missing_in_config:\n                        # Add missing columns to config (treat as immutable if they existed before)\n                        self._sync_missing_columns_to_config(list(missing_in_config))\n                        # Return updated config columns\n                        config_columns = self._get_columns_from_config()\n\n                return config_columns\n\n            # Fallback: query table\n            return self._get_columns_from_table()\n\n    def __getitem__(self, key):\n        \"\"\"\n        Get column/vector or DataFrame slice (AnnData-compatible DataFrame interface).\n\n        - String key: Returns pandas Series (DataFrame-like behavior, matches AnnData)\n        - Other keys (slice, list, etc.): Delegates to DataFrame indexing (DataFrame-like behavior)\n\n        Args:\n            key: Column name (str) or DataFrame indexer (slice, list, etc.)\n\n        Returns:\n            pandas Series if key is string, otherwise DataFrame slice\n        \"\"\"\n        # Route to vector or scalar method based on table_type\n        if self.table_type in (\"obsm\", \"varm\"):\n            # For obsm/varm, only support string keys (dict-like)\n            if not isinstance(key, str):\n                raise TypeError(\n                    f\"{self.table_type} only supports string keys, got {type(key)}\"\n                )\n            return self._get_vector_item(key)\n        else:\n            # For obs/var, use DataFrame-like access (AnnData-compatible)\n            # Check if key exists first for better error messages\n            if isinstance(key, str) and key not in self.keys():\n                raise KeyError(f\"Column '{key}' not found\")\n            # Always delegate to underlying DataFrame to get Series for string keys\n            return self._get_dataframe().__getitem__(key)\n\n    def _is_immutable(self, key: str) -&gt; bool:\n        \"\"\"Check if column/vector key is immutable (converted from h5ad)\"\"\"\n        # Route to vector or scalar method based on table_type\n        if self.table_type in (\"obsm\", \"varm\"):\n            return self._is_immutable_vector(key)\n        else:\n            # Scalar column immutability check\n            config = self._slaf_array.config\n            if self.table_type in config and \"immutable\" in config[self.table_type]:\n                return key in config[self.table_type][\"immutable\"]\n            return False\n\n    def _optimize_dtype_for_column(\n        self, data: np.ndarray\n    ) -&gt; tuple[np.ndarray, pa.DataType]:\n        \"\"\"\n        Optimize dtype for column values.\n\n        If float32 data contains only integers within uint16 range, convert to uint16.\n        Otherwise, use float32.\n\n        Args:\n            data: Array of column values\n\n        Returns:\n            Tuple of (optimized values array, PyArrow data type)\n        \"\"\"\n        if len(data) == 0:\n            return np.array([], dtype=np.float32), pa.float32()\n\n        # Handle string arrays\n        if data.dtype.kind in [\n            \"U\",\n            \"S\",\n            \"O\",\n        ]:  # Unicode, byte string, or object (often strings)\n            # Convert to string array and use PyArrow string type\n            if data.dtype.kind == \"O\":\n                # Object array - try to convert to string\n                try:\n                    data = np.array([str(x) for x in data], dtype=\"U\")\n                except (TypeError, ValueError):\n                    # If conversion fails, keep as object\n                    return data, pa.string()\n            # Convert to UTF-8 string array\n            return data.astype(\"U\"), pa.string()\n\n        # Sample data to determine dtype\n        sample_size = min(10000, len(data))\n        sample_data = data[:sample_size]\n\n        # Check if data is integer or float\n        is_integer = np.issubdtype(sample_data.dtype, np.integer)\n        max_value = np.max(data)\n        min_value = np.min(data)\n\n        if is_integer and max_value &lt;= 65535 and min_value &gt;= 0:\n            return data.astype(np.uint16), pa.uint16()\n        elif not is_integer:\n            # Check if float data contains only integer values\n            rounded_data = np.round(data)\n            is_integer_values = np.allclose(data, rounded_data, rtol=1e-10)\n\n            if is_integer_values and max_value &lt;= 65535 and min_value &gt;= 0:\n                return rounded_data.astype(np.uint16), pa.uint16()\n            else:\n                return data.astype(np.float32), pa.float32()\n        else:\n            return data.astype(np.float32), pa.float32()\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Create or update a column/vector or DataFrame slice (dual interface).\n\n        - String key: Column assignment (dict-like behavior)\n        - Other keys: DataFrame assignment (DataFrame-like behavior)\n\n        Args:\n            key: Column name (str) or DataFrame indexer\n            value: numpy array, pandas Series, or DataFrame slice value\n        \"\"\"\n        # Route to vector or scalar method based on table_type\n        if self.table_type in (\"obsm\", \"varm\"):\n            # For obsm/varm, only support string keys (dict-like)\n            if not isinstance(key, str):\n                raise TypeError(\n                    f\"{self.table_type} only supports string keys, got {type(key)}\"\n                )\n            # Convert to numpy array if needed\n            if isinstance(value, pd.Series):\n                value = value.values\n            value = np.asarray(value)\n            # For vectors, value should be 2D (n_entities, n_dims)\n            if len(value.shape) == 1:\n                # If 1D, treat as single dimension vector\n                value = value.reshape(-1, 1)\n            self._set_vector_item(key, value)\n        else:\n            # For obs/var, support both dict-like and DataFrame-like assignment\n            if isinstance(key, str):\n                # Dict-like: column assignment\n                self._set_column_item(key, value)\n            else:\n                # DataFrame-like: delegate to underlying DataFrame\n                # Note: This will modify the DataFrame but won't persist to Lance\n                # For now, we'll raise an error to guide users to use string keys for mutations\n                raise NotImplementedError(\n                    f\"DataFrame-like assignment (e.g., obs[{key}] = ...) is not supported. \"\n                    f\"Use column assignment (e.g., obs['{key}'] = ...) for mutations.\"\n                )\n\n    def _set_column_item(self, key: str, value: np.ndarray | pd.Series):\n        \"\"\"\n        Create or update a column (eager write - immediate).\n\n        Args:\n            key: Column name (must be alphanumeric + underscore, non-empty)\n            value: numpy array or pandas Series to assign.\n                   Must have length matching entity count (considering selectors).\n\n        Raises:\n            ValueError: If column name is invalid, length doesn't match,\n                       or trying to overwrite an immutable column.\n        \"\"\"\n        # Validate column name\n        self._validate_name(key)\n\n        # Validate shape matches entity count (considering selectors)\n        expected_count = self._get_entity_count()\n        entity_name = self.table_type  # \"obs\" or \"var\"\n        if len(value) != expected_count:\n            raise ValueError(\n                f\"Column length {len(value)} doesn't match {entity_name} count {expected_count}\"\n            )\n\n        # Check if column already exists and is immutable\n        if key in self.keys() and self._is_immutable(key):\n            raise ValueError(\n                f\"Column '{key}' is immutable (converted from h5ad) and cannot be overwritten\"\n            )\n\n        # Convert to numpy array\n        if isinstance(value, pd.Series):\n            value = value.values\n        value = np.asarray(value)\n\n        # Optimize dtype\n        values, value_pa_type = self._optimize_dtype_for_column(value)\n\n        # Get integer IDs in order (respecting selectors)\n        selector = self._get_current_selector()\n        table = getattr(self._slaf_array, self.table_name)\n        if selector is None:\n            # No selector - use all entities\n            df = table.to_table().to_pandas()\n            integer_ids = df[self.id_column].values\n        else:\n            # Apply selector to get integer IDs\n            query = self._build_filtered_query([self.id_column])\n            df = query.collect().sort(self.id_column)\n            integer_ids = df[self.id_column].to_numpy()\n\n        # Determine ID column type (uint32 for cells, uint16 for genes)\n        id_pa_type = pa.uint32() if self.table_type == \"obs\" else pa.uint16()\n\n        # Create PyArrow table with the new column\n        column_table = pa.table(\n            {\n                self.id_column: pa.array(integer_ids, type=id_pa_type),\n                key: pa.array(values, type=value_pa_type),\n            }\n        )\n\n        # Write to table\n        table_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path,\n            self._slaf_array.config.get(\"tables\", {}).get(\n                self.table_name, f\"{self.table_name}.lance\"\n            ),\n        )\n\n        if table is None:\n            raise ValueError(f\"{self.table_name}.lance table not found\")\n\n        # Update table (similar to layers update logic)\n        self._update_table_with_column(column_table, key, table_path)\n\n        # Update config.json atomically\n        self._update_config_columns_list([key], add=True)\n\n        # Reload config to ensure it's up-to-date (config is cached in SLAFArray)\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            self._slaf_array.config = json.load(f)\n\n        # Invalidate cached obs/var DataFrames since table structure changed\n        self._invalidate_metadata_cache()\n\n    def _update_table_with_column(\n        self, column_table: pa.Table, column_name: str, table_path: str\n    ):\n        \"\"\"Update existing table with a new/updated column using add_columns() with UDF.\"\"\"\n        import lance\n\n        table_dataset = getattr(self._slaf_array, self.table_name)\n\n        # Check if column already exists\n        schema = table_dataset.schema\n        column_names = [field.name for field in schema]\n\n        # Drop the old column if it exists (using Lance native method - metadata-only, very fast)\n        if column_name in column_names:\n            table_dataset = table_dataset.drop_columns([column_name])\n            # Reload from path after drop_columns to get the updated dataset\n            table_dataset = lance.dataset(table_path)\n\n        # Convert column data to polars for efficient lookup in UDF\n        column_df = pl.from_arrow(column_table)\n\n        # Create UDF that returns just the new column data\n        def add_column_udf(batch):\n            \"\"\"\n            UDF to add column by joining batch with column data.\n\n            Receives batch with existing columns (cell_integer_id, etc.),\n            returns RecordBatch with just the new column.\n            \"\"\"\n            # Convert batch to polars\n            batch_df = pl.from_arrow(batch)\n\n            # Join with column data (left join preserves all rows)\n            result_df = batch_df.join(column_df, on=[self.id_column], how=\"left\")\n\n            # Extract just the new column and convert to single array\n            new_column_chunked = result_df.select([column_name]).to_arrow().column(0)\n            new_column_array = new_column_chunked.combine_chunks()\n\n            # Return RecordBatch with just the new column\n            return pa.RecordBatch.from_arrays(\n                [new_column_array],\n                names=[column_name],\n            )\n\n        # Add the column using add_columns() with UDF\n        table_dataset.add_columns(add_column_udf)\n\n        # Reload table dataset from path to ensure consistency\n        setattr(self._slaf_array, self.table_name, lance.dataset(table_path))\n\n    def _update_config_columns_list(self, column_names: list[str], add: bool):\n        \"\"\"\n        Update config.json to add or remove columns from available/mutable lists.\n\n        Args:\n            column_names: List of column names to add or remove\n            add: If True, add columns; if False, remove columns\n        \"\"\"\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n\n        # Load existing config\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            config = json.load(f)\n\n        # Ensure table config exists\n        if self.table_type not in config:\n            config[self.table_type] = {\"available\": [], \"immutable\": [], \"mutable\": []}\n\n        table_config = config[self.table_type]\n        available = set(table_config.get(\"available\", []))\n        immutable = set(table_config.get(\"immutable\", []))\n        mutable = set(table_config.get(\"mutable\", []))\n\n        if add:\n            # Add columns to available and mutable (new columns are mutable)\n            for column_name in column_names:\n                available.add(column_name)\n                mutable.add(column_name)\n                # Remove from immutable if it was there (shouldn't happen, but be safe)\n                immutable.discard(column_name)\n        else:\n            # Remove columns from all lists\n            for column_name in column_names:\n                available.discard(column_name)\n                mutable.discard(column_name)\n                immutable.discard(column_name)\n\n        # Update config\n        table_config[\"available\"] = sorted(available)\n        table_config[\"immutable\"] = sorted(immutable)\n        table_config[\"mutable\"] = sorted(mutable)\n\n        # Save updated config\n        with self._slaf_array._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n    def _sync_missing_columns_to_config(self, column_names: list[str]):\n        \"\"\"\n        Sync missing columns from table to config.json.\n        These columns are treated as immutable (they existed before Phase 6.5).\n\n        Args:\n            column_names: List of column names to add to config\n        \"\"\"\n        if not column_names:\n            return\n\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n\n        # Load existing config\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            config = json.load(f)\n\n        # Ensure table config exists\n        if self.table_type not in config:\n            config[self.table_type] = {\"available\": [], \"immutable\": [], \"mutable\": []}\n\n        table_config = config[self.table_type]\n        available = set(table_config.get(\"available\", []))\n        immutable = set(table_config.get(\"immutable\", []))\n        mutable = set(table_config.get(\"mutable\", []))\n\n        # Add missing columns to available and immutable (they existed before)\n        for column_name in column_names:\n            available.add(column_name)\n            immutable.add(column_name)\n            # Remove from mutable if it was there (shouldn't happen, but be safe)\n            mutable.discard(column_name)\n\n        # Update config\n        table_config[\"available\"] = sorted(available)\n        table_config[\"immutable\"] = sorted(immutable)\n        table_config[\"mutable\"] = sorted(mutable)\n\n        # Save updated config\n        with self._slaf_array._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n    def __delitem__(self, key: str):\n        \"\"\"\n        Delete a column/vector (only if mutable).\n\n        Args:\n            key: Column/vector name to delete\n\n        Raises:\n            KeyError: If column/vector doesn't exist\n            ValueError: If column/vector is immutable and cannot be deleted\n        \"\"\"\n        # Route to vector or scalar method based on table_type\n        if self.table_type in (\"obsm\", \"varm\"):\n            self._del_vector_item(key)\n        else:\n            # Scalar column deletion\n            import lance\n\n            # Check if column exists\n            if key not in self.keys():\n                raise KeyError(f\"Column '{key}' not found\")\n\n            # Check if column is immutable\n            if self._is_immutable(key):\n                raise ValueError(\n                    f\"Column '{key}' is immutable (converted from h5ad) and cannot be deleted\"\n                )\n\n            # Get table path\n            table_path = self._slaf_array._join_path(\n                self._slaf_array.slaf_path,\n                self._slaf_array.config.get(\"tables\", {}).get(\n                    self.table_name, f\"{self.table_name}.lance\"\n                ),\n            )\n\n            # Drop the column using Lance's drop_columns() method (metadata-only, very fast)\n            table_dataset = getattr(self._slaf_array, self.table_name)\n            table_dataset = table_dataset.drop_columns([key])\n\n            # Reload table dataset from path to ensure consistency\n            setattr(self._slaf_array, self.table_name, lance.dataset(table_path))\n\n            # Update config.json atomically\n            self._update_config_columns_list([key], add=False)\n\n            # Reload config to ensure it's up-to-date (config is cached in SLAFArray)\n            config_path = self._slaf_array._join_path(\n                self._slaf_array.slaf_path, \"config.json\"\n            )\n            with self._slaf_array._open_file(config_path) as f:\n                import json\n\n                self._slaf_array.config = json.load(f)\n\n            # Invalidate cached obs/var DataFrames since table structure changed\n            self._invalidate_metadata_cache()\n\n    # ==================== Vector-specific methods (for obsm/varm) ====================\n\n    def _detect_vector_columns(self) -&gt; dict[str, int]:\n        \"\"\"\n        Detect FixedSizeListArray columns from schema and return key -&gt; dimension mapping.\n\n        Returns:\n            Dictionary mapping vector key names to their dimensions\n        \"\"\"\n        table = getattr(self._slaf_array, self.table_name, None)\n        if table is None:\n            return {}\n\n        vector_columns = {}\n        schema = table.schema\n\n        for field in schema:\n            # Check if field is a FixedSizeListArray (vector type)\n            if isinstance(field.type, pa.FixedSizeListType):\n                # Use column name directly as the key\n                key = field.name\n                # Get dimension from FixedSizeListType\n                n_dims = field.type.list_size\n                vector_columns[key] = n_dims\n\n        return vector_columns\n\n    def _get_vector_item(self, key: str) -&gt; np.ndarray:\n        \"\"\"Retrieve multi-dimensional array (respects selectors from parent)\"\"\"\n        if key not in self.keys():\n            raise KeyError(f\"{self.table_type} key '{key}' not found\")\n\n        # Build query for the vector column (using mixin for filtering)\n        query = self._build_filtered_query([self.id_column, key])\n        df = query.collect()\n\n        # Sort by integer ID\n        df = df.sort(self.id_column)\n\n        # Extract vector column and convert to numpy array\n        # FixedSizeListArray columns are stored as lists/arrays\n        vector_data = df[key].to_numpy()\n\n        # Convert list of arrays to 2D numpy array\n        if len(vector_data) &gt; 0 and isinstance(vector_data[0], list | np.ndarray):\n            return np.array([np.array(v) for v in vector_data])\n        else:\n            # Already in correct format\n            return np.asarray(vector_data)\n\n    def _set_vector_item(self, key: str, value: np.ndarray):\n        \"\"\"Store multi-dimensional array as FixedSizeListArray column\"\"\"\n        import lance\n        import pyarrow as pa\n\n        # Validate shape using mixin utilities (respects selectors)\n        expected_count = self._get_entity_count()\n        if value.shape[0] != expected_count:\n            raise ValueError(\n                f\"Array first dimension {value.shape[0]} doesn't match {self.table_type} count {expected_count}\"\n            )\n\n        # Validate key name\n        self._validate_name(key)\n\n        # Check immutability\n        if key in self.keys() and self._is_immutable(key):\n            raise ValueError(\n                f\"{self.table_type} key '{key}' is immutable and cannot be overwritten\"\n            )\n\n        # Convert to numpy array\n        value = np.asarray(value)\n        n_dims = value.shape[1] if len(value.shape) &gt; 1 else 1\n\n        # Get integer IDs in order (respecting selectors)\n        selector = self._get_current_selector()\n        table = getattr(self._slaf_array, self.table_name)\n        if selector is None:\n            # No selector - use all entities\n            df = table.to_table().to_pandas()\n            integer_ids = df[self.id_column].values\n        else:\n            # Apply selector to get integer IDs\n            query = self._build_filtered_query([self.id_column])\n            df = query.collect().sort(self.id_column)\n            integer_ids = df[self.id_column].to_numpy()\n\n        # Create FixedSizeListArray directly from numpy array\n        # Determine value dtype (use float32 for embeddings)\n        value_dtype = pa.float32() if value.dtype.kind == \"f\" else pa.float32()\n\n        # Flatten the 2D array and create FixedSizeListArray\n        # FixedSizeListArray.from_arrays takes a flat array and list_size\n        flat_values = pa.array(value.flatten(), type=value_dtype)\n        vector_array = pa.FixedSizeListArray.from_arrays(flat_values, n_dims)\n\n        # If overwriting, drop old column first\n        if key in self.keys():\n            table_dataset = getattr(self._slaf_array, self.table_name)\n            table_dataset = table_dataset.drop_columns([key])\n            table_path = self._slaf_array._join_path(\n                self._slaf_array.slaf_path,\n                self._slaf_array.config.get(\"tables\", {}).get(\n                    self.table_name, f\"{self.table_name}.lance\"\n                ),\n            )\n            setattr(self._slaf_array, self.table_name, lance.dataset(table_path))\n\n        # Create table with id_column and vector column\n        table_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path,\n            self._slaf_array.config.get(\"tables\", {}).get(\n                self.table_name, f\"{self.table_name}.lance\"\n            ),\n        )\n\n        # Determine ID column type (uint32 for cells, uint16 for genes)\n        id_pa_type = pa.uint32() if self.table_type == \"obsm\" else pa.uint16()\n\n        # Create table with integer IDs and vector column\n        column_table = pa.table(\n            {\n                self.id_column: pa.array(integer_ids, type=id_pa_type),\n                key: vector_array,\n            }\n        )\n\n        # Update table\n        self._update_table_with_column(column_table, key, table_path)\n\n        # Update config.json\n        self._update_config_vector_list([key], add=True, n_dims=n_dims)\n\n        # Reload config to ensure it's up-to-date (config is cached in SLAFArray)\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            self._slaf_array.config = json.load(f)\n\n        # Invalidate cached obs/var DataFrames since table structure changed\n        self._invalidate_metadata_cache()\n\n    def _del_vector_item(self, key: str):\n        \"\"\"Delete vector key (drops the vector column)\"\"\"\n        import lance\n\n        if key not in self.keys():\n            raise KeyError(f\"{self.table_type} key '{key}' not found\")\n\n        if self._is_immutable(key):\n            raise ValueError(\n                f\"{self.table_type} key '{key}' is immutable and cannot be deleted\"\n            )\n\n        # Drop the vector column\n        table_dataset = getattr(self._slaf_array, self.table_name)\n        table_dataset = table_dataset.drop_columns([key])\n\n        # Reload dataset\n        table_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path,\n            self._slaf_array.config.get(\"tables\", {}).get(\n                self.table_name, f\"{self.table_name}.lance\"\n            ),\n        )\n        setattr(self._slaf_array, self.table_name, lance.dataset(table_path))\n\n        # Update config.json\n        self._update_config_vector_list([key], add=False)\n\n        # Reload config to ensure it's up-to-date (config is cached in SLAFArray)\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            self._slaf_array.config = json.load(f)\n\n        # Invalidate cached obs/var DataFrames since table structure changed\n        self._invalidate_metadata_cache()\n\n    def _keys_vector(self) -&gt; list[str]:\n        \"\"\"List all available vector keys by detecting FixedSizeListArray columns\"\"\"\n        # Fast path: read from config.json\n        config = self._slaf_array.config\n        if self.table_type in config and \"available\" in config[self.table_type]:\n            config_keys = config[self.table_type][\"available\"]\n            # Verify against schema (auto-sync if needed)\n            schema_keys = list(self._detect_vector_columns().keys())\n            if set(config_keys) != set(schema_keys):\n                # Auto-sync: update config with schema keys\n                missing_in_config = set(schema_keys) - set(config_keys)\n                if missing_in_config:\n                    # Add missing keys to config (treat as immutable if they existed before)\n                    self._update_config_vector_list(list(missing_in_config), add=True)\n                    # Re-read config\n                    config = self._slaf_array.config\n                    return config.get(self.table_type, {}).get(\"available\", [])\n            return config_keys\n\n        # Fallback: detect from schema\n        return list(self._detect_vector_columns().keys())\n\n    def _is_immutable_vector(self, key: str) -&gt; bool:\n        \"\"\"Check if vector key is immutable\"\"\"\n        config = self._slaf_array.config\n        if self.table_type in config and \"immutable\" in config[self.table_type]:\n            return key in config[self.table_type][\"immutable\"]\n        return False\n\n    def _update_config_vector_list(\n        self, keys: list[str], add: bool, n_dims: int | None = None\n    ):\n        \"\"\"Update config.json to add or remove vector keys (unified for obsm/varm)\"\"\"\n        config_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"config.json\"\n        )\n\n        # Load existing config\n        with self._slaf_array._open_file(config_path) as f:\n            import json\n\n            config = json.load(f)\n\n        # Ensure vector config exists\n        if self.table_type not in config:\n            config[self.table_type] = {\n                \"available\": [],\n                \"immutable\": [],\n                \"mutable\": [],\n                \"dimensions\": {},\n            }\n\n        vector_config = config[self.table_type]\n        available = set(vector_config.get(\"available\", []))\n        immutable = set(vector_config.get(\"immutable\", []))\n        mutable = set(vector_config.get(\"mutable\", []))\n        dimensions = vector_config.get(\"dimensions\", {})\n\n        if add:\n            # Add keys to available and mutable (new keys are mutable)\n            for key in keys:\n                available.add(key)\n                mutable.add(key)\n                immutable.discard(key)\n                if n_dims is not None:\n                    dimensions[key] = n_dims\n        else:\n            # Remove keys from all lists\n            for key in keys:\n                available.discard(key)\n                mutable.discard(key)\n                immutable.discard(key)\n                dimensions.pop(key, None)\n\n        # Update config\n        vector_config[\"available\"] = sorted(available)\n        vector_config[\"immutable\"] = sorted(immutable)\n        vector_config[\"mutable\"] = sorted(mutable)\n        vector_config[\"dimensions\"] = dimensions\n\n        # Save updated config\n        with self._slaf_array._open_file(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyMetadataViewMixin-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyMetadataViewMixin.keys","title":"<code>keys() -&gt; list[str]</code>","text":"<p>List all available column/vector names</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def keys(self) -&gt; list[str]:\n    \"\"\"List all available column/vector names\"\"\"\n    # Route to vector or scalar method based on table_type\n    if self.table_type in (\"obsm\", \"varm\"):\n        return self._keys_vector()\n    else:\n        # Scalar column keys\n        # Fast path: read from config.json\n        config_columns = self._get_columns_from_config()\n\n        if config_columns:\n            # Verify consistency with table if possible\n            table_columns = self._get_columns_from_table()\n            if table_columns:\n                config_set = set(config_columns)\n                table_set = set(table_columns)\n\n                # Check for columns in config but not in table (real problem)\n                missing_in_table = config_set - table_set\n                if missing_in_table:\n                    import warnings\n\n                    warnings.warn(\n                        f\"Column names in config.json ({sorted(missing_in_table)}) \"\n                        f\"not found in {self.table_name} table. These will be ignored.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n\n                # Auto-sync: add columns from table that are missing in config\n                missing_in_config = table_set - config_set\n                if missing_in_config:\n                    # Add missing columns to config (treat as immutable if they existed before)\n                    self._sync_missing_columns_to_config(list(missing_in_config))\n                    # Return updated config columns\n                    config_columns = self._get_columns_from_config()\n\n            return config_columns\n\n        # Fallback: query table\n        return self._get_columns_from_table()\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsView","title":"<code>LazyObsView</code>","text":"<p>               Bases: <code>LazyMetadataViewMixin</code></p> <p>Dual-interface view of obs columns: DataFrame-like and dictionary-like.</p> <p>LazyObsView provides both DataFrame-like and dictionary-like interfaces for accessing and mutating cell metadata columns stored in the cells.lance table.</p> Key Features <ul> <li>DataFrame-like interface: obs.columns, obs.head(), obs[slice] (AnnData-compatible)</li> <li>AnnData-compatible access: obs[\"col\"] returns pd.Series (not np.ndarray)</li> <li>Dictionary-like interface: \"col\" in obs, len(obs), obs.keys()</li> <li>Lazy evaluation: columns are accessed on-demand</li> <li>Selector support: respects cell selectors from parent LazyAnnData</li> <li>Immutability: prevents deletion/modification of converted columns</li> <li>Config.json consistency: reads from config for fast column discovery</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; df = adata.obs  # Returns DataFrame-like view\n&gt;&gt;&gt; print(df.columns)  # DataFrame columns\n&gt;&gt;&gt; print(df.head())  # DataFrame methods work\n</code></pre> <pre><code>&gt;&gt;&gt; # AnnData-compatible column access (returns Series)\n&gt;&gt;&gt; cluster = adata.obs[\"cluster\"]  # Returns pd.Series (AnnData-compatible)\n&gt;&gt;&gt; print(type(cluster))\n&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a new column\n&gt;&gt;&gt; adata.obs[\"new_cluster\"] = new_cluster_labels\n&gt;&gt;&gt; assert \"new_cluster\" in adata.obs\n</code></pre> <pre><code>&gt;&gt;&gt; # List available columns\n&gt;&gt;&gt; print(list(adata.obs.keys()))\n['cell_id', 'total_counts', 'cluster', 'new_cluster']\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyObsView(LazyMetadataViewMixin):\n    \"\"\"\n    Dual-interface view of obs columns: DataFrame-like and dictionary-like.\n\n    LazyObsView provides both DataFrame-like and dictionary-like interfaces for accessing\n    and mutating cell metadata columns stored in the cells.lance table.\n\n    Key Features:\n        - DataFrame-like interface: obs.columns, obs.head(), obs[slice] (AnnData-compatible)\n        - AnnData-compatible access: obs[\"col\"] returns pd.Series (not np.ndarray)\n        - Dictionary-like interface: \"col\" in obs, len(obs), obs.keys()\n        - Lazy evaluation: columns are accessed on-demand\n        - Selector support: respects cell selectors from parent LazyAnnData\n        - Immutability: prevents deletion/modification of converted columns\n        - Config.json consistency: reads from config for fast column discovery\n\n    Examples:\n        &gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; df = adata.obs  # Returns DataFrame-like view\n        &gt;&gt;&gt; print(df.columns)  # DataFrame columns\n        &gt;&gt;&gt; print(df.head())  # DataFrame methods work\n\n        &gt;&gt;&gt; # AnnData-compatible column access (returns Series)\n        &gt;&gt;&gt; cluster = adata.obs[\"cluster\"]  # Returns pd.Series (AnnData-compatible)\n        &gt;&gt;&gt; print(type(cluster))\n        &lt;class 'pandas.core.series.Series'&gt;\n\n        &gt;&gt;&gt; # Create a new column\n        &gt;&gt;&gt; adata.obs[\"new_cluster\"] = new_cluster_labels\n        &gt;&gt;&gt; assert \"new_cluster\" in adata.obs\n\n        &gt;&gt;&gt; # List available columns\n        &gt;&gt;&gt; print(list(adata.obs.keys()))\n        ['cell_id', 'total_counts', 'cluster', 'new_cluster']\n    \"\"\"\n\n    def __init__(self, lazy_adata: \"LazyAnnData\"):\n        \"\"\"\n        Initialize LazyObsView with LazyAnnData.\n\n        Args:\n            lazy_adata: LazyAnnData instance containing the single-cell data.\n        \"\"\"\n        super().__init__()\n        self.lazy_adata = lazy_adata\n        self._slaf_array = lazy_adata.slaf\n        # Required by LazySparseMixin\n        self.slaf_array = lazy_adata.slaf\n        self._shape = lazy_adata.shape  # (n_cells, n_genes)\n        self.table_name = \"cells\"\n        self.id_column = \"cell_integer_id\"\n        self.table_type = \"obs\"\n        self._dataframe: pd.DataFrame | None = None  # Cached DataFrame\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Required by LazySparseMixin - uses parent's shape\"\"\"\n        return self._shape\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return number of rows (DataFrame-like behavior).\n\n        For DataFrame compatibility, len(obs) should return the number of rows,\n        not the number of columns (which is what the dictionary interface would return).\n        \"\"\"\n        return len(self._get_dataframe())\n\n    def _get_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get underlying DataFrame (lazy-loaded, respects selectors).\n\n        Returns:\n            pandas DataFrame with all columns from cells.lance (excluding vector columns)\n        \"\"\"\n        # If parent has filtered_obs function, use it instead\n        if (\n            hasattr(self.lazy_adata, \"_filtered_obs\")\n            and self.lazy_adata._filtered_obs is not None\n        ):\n            return self.lazy_adata._filtered_obs()\n\n        if self._dataframe is None:\n            # Get all column names from table schema\n            table = getattr(self._slaf_array, self.table_name)\n            schema = table.schema\n            all_columns = [field.name for field in schema]\n\n            # Build DataFrame from cells.lance with selectors\n            query = self._build_filtered_query(all_columns)\n            df = query.collect()\n\n            # Sort by integer ID to match order\n            df = df.sort(self.id_column)\n\n            # Convert to pandas DataFrame\n            obs_df = df.to_pandas()\n\n            # Drop system columns\n            if self.id_column in obs_df.columns:\n                obs_df = obs_df.drop(columns=[self.id_column])\n\n            # Filter out vector columns (obsm) - these are FixedSizeListArray columns\n            cells_table = getattr(self._slaf_array, self.table_name, None)\n            if cells_table is not None:\n                schema = cells_table.schema\n                vector_column_names = {\n                    field.name\n                    for field in schema\n                    if isinstance(field.type, pa.FixedSizeListType)\n                }\n                columns_to_drop = [\n                    col for col in obs_df.columns if col in vector_column_names\n                ]\n                if columns_to_drop:\n                    obs_df = obs_df.drop(columns=columns_to_drop)\n\n            # Set cell_id as index if present\n            if \"cell_id\" in obs_df.columns:\n                obs_df = obs_df.set_index(\"cell_id\")\n                obs_df.index.name = \"cell_id\"\n\n            self._dataframe = obs_df\n\n        return self._dataframe\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        Delegate DataFrame attributes to underlying DataFrame.\n\n        This allows obs to behave like a DataFrame when accessed directly,\n        e.g., obs.columns, obs.head(), obs.shape, etc.\n        \"\"\"\n        # Don't delegate special methods or our own methods\n        if name.startswith(\"_\") or name in dir(self):\n            raise AttributeError(\n                f\"'{type(self).__name__}' object has no attribute '{name}'\"\n            )\n\n        # Delegate to underlying DataFrame\n        try:\n            return getattr(self._get_dataframe(), name)\n        except AttributeError as err:\n            raise AttributeError(\n                f\"'{type(self).__name__}' object has no attribute '{name}'\"\n            ) from err\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsView-attributes","title":"Attributes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsView.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Required by LazySparseMixin - uses parent's shape</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsView-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsView.__init__","title":"<code>__init__(lazy_adata: LazyAnnData)</code>","text":"<p>Initialize LazyObsView with LazyAnnData.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(self, lazy_adata: \"LazyAnnData\"):\n    \"\"\"\n    Initialize LazyObsView with LazyAnnData.\n\n    Args:\n        lazy_adata: LazyAnnData instance containing the single-cell data.\n    \"\"\"\n    super().__init__()\n    self.lazy_adata = lazy_adata\n    self._slaf_array = lazy_adata.slaf\n    # Required by LazySparseMixin\n    self.slaf_array = lazy_adata.slaf\n    self._shape = lazy_adata.shape  # (n_cells, n_genes)\n    self.table_name = \"cells\"\n    self.id_column = \"cell_integer_id\"\n    self.table_type = \"obs\"\n    self._dataframe: pd.DataFrame | None = None  # Cached DataFrame\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarView","title":"<code>LazyVarView</code>","text":"<p>               Bases: <code>LazyMetadataViewMixin</code></p> <p>Dual-interface view of var columns: DataFrame-like and dictionary-like.</p> <p>LazyVarView provides both DataFrame-like and dictionary-like interfaces for accessing and mutating gene metadata columns stored in the genes.lance table.</p> Key Features <ul> <li>DataFrame-like interface: var.columns, var.head(), var[slice] (AnnData-compatible)</li> <li>AnnData-compatible access: var[\"col\"] returns pd.Series (not np.ndarray)</li> <li>Dictionary-like interface: \"col\" in var, len(var), var.keys()</li> <li>Lazy evaluation: columns are accessed on-demand</li> <li>Selector support: respects gene selectors from parent LazyAnnData</li> <li>Immutability: prevents deletion/modification of converted columns</li> <li>Config.json consistency: reads from config for fast column discovery</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; df = adata.var  # Returns DataFrame-like view\n&gt;&gt;&gt; print(df.columns)  # DataFrame columns\n&gt;&gt;&gt; print(df.head())  # DataFrame methods work\n</code></pre> <pre><code>&gt;&gt;&gt; # AnnData-compatible column access (returns Series)\n&gt;&gt;&gt; hvg = adata.var[\"highly_variable\"]  # Returns pd.Series (AnnData-compatible)\n&gt;&gt;&gt; print(type(hvg))\n&lt;class 'pandas.core.series.Series'&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a new column\n&gt;&gt;&gt; adata.var[\"new_annotation\"] = new_annotations\n&gt;&gt;&gt; assert \"new_annotation\" in adata.var\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyVarView(LazyMetadataViewMixin):\n    \"\"\"\n    Dual-interface view of var columns: DataFrame-like and dictionary-like.\n\n    LazyVarView provides both DataFrame-like and dictionary-like interfaces for accessing\n    and mutating gene metadata columns stored in the genes.lance table.\n\n    Key Features:\n        - DataFrame-like interface: var.columns, var.head(), var[slice] (AnnData-compatible)\n        - AnnData-compatible access: var[\"col\"] returns pd.Series (not np.ndarray)\n        - Dictionary-like interface: \"col\" in var, len(var), var.keys()\n        - Lazy evaluation: columns are accessed on-demand\n        - Selector support: respects gene selectors from parent LazyAnnData\n        - Immutability: prevents deletion/modification of converted columns\n        - Config.json consistency: reads from config for fast column discovery\n\n    Examples:\n        &gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; df = adata.var  # Returns DataFrame-like view\n        &gt;&gt;&gt; print(df.columns)  # DataFrame columns\n        &gt;&gt;&gt; print(df.head())  # DataFrame methods work\n\n        &gt;&gt;&gt; # AnnData-compatible column access (returns Series)\n        &gt;&gt;&gt; hvg = adata.var[\"highly_variable\"]  # Returns pd.Series (AnnData-compatible)\n        &gt;&gt;&gt; print(type(hvg))\n        &lt;class 'pandas.core.series.Series'&gt;\n\n        &gt;&gt;&gt; # Create a new column\n        &gt;&gt;&gt; adata.var[\"new_annotation\"] = new_annotations\n        &gt;&gt;&gt; assert \"new_annotation\" in adata.var\n    \"\"\"\n\n    def __init__(self, lazy_adata: \"LazyAnnData\"):\n        \"\"\"\n        Initialize LazyVarView with LazyAnnData.\n\n        Args:\n            lazy_adata: LazyAnnData instance containing the single-cell data.\n        \"\"\"\n        super().__init__()\n        self.lazy_adata = lazy_adata\n        self._slaf_array = lazy_adata.slaf\n        # Required by LazySparseMixin\n        self.slaf_array = lazy_adata.slaf\n        self._shape = lazy_adata.shape  # (n_cells, n_genes)\n        self.table_name = \"genes\"\n        self.id_column = \"gene_integer_id\"\n        self.table_type = \"var\"\n        self._dataframe: pd.DataFrame | None = None  # Cached DataFrame\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Required by LazySparseMixin - uses parent's shape\"\"\"\n        return self._shape\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return number of rows (DataFrame-like behavior).\n\n        For DataFrame compatibility, len(var) should return the number of rows,\n        not the number of columns (which is what the dictionary interface would return).\n        \"\"\"\n        return len(self._get_dataframe())\n\n    def _get_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get underlying DataFrame (lazy-loaded, respects selectors).\n\n        Returns:\n            pandas DataFrame with all columns from genes.lance (excluding vector columns)\n        \"\"\"\n        # If parent has filtered_var function, use it instead\n        if (\n            hasattr(self.lazy_adata, \"_filtered_var\")\n            and self.lazy_adata._filtered_var is not None\n        ):\n            return self.lazy_adata._filtered_var()\n\n        if self._dataframe is None:\n            # Get all column names from table schema\n            table = getattr(self._slaf_array, self.table_name)\n            schema = table.schema\n            all_columns = [field.name for field in schema]\n\n            # Build DataFrame from genes.lance with selectors\n            query = self._build_filtered_query(all_columns)\n            df = query.collect()\n\n            # Sort by integer ID to match order\n            df = df.sort(self.id_column)\n\n            # Convert to pandas DataFrame\n            var_df = df.to_pandas()\n\n            # Drop system columns\n            if self.id_column in var_df.columns:\n                var_df = var_df.drop(columns=[self.id_column])\n\n            # Filter out vector columns (varm) - these are FixedSizeListArray columns\n            genes_table = getattr(self._slaf_array, self.table_name, None)\n            if genes_table is not None:\n                schema = genes_table.schema\n                vector_column_names = {\n                    field.name\n                    for field in schema\n                    if isinstance(field.type, pa.FixedSizeListType)\n                }\n                columns_to_drop = [\n                    col for col in var_df.columns if col in vector_column_names\n                ]\n                if columns_to_drop:\n                    var_df = var_df.drop(columns=columns_to_drop)\n\n            # Set gene_id as index if present\n            if \"gene_id\" in var_df.columns:\n                var_df = var_df.set_index(\"gene_id\")\n                var_df.index.name = \"gene_id\"\n\n            self._dataframe = var_df\n\n        return self._dataframe\n\n    def __getattr__(self, name: str):\n        \"\"\"\n        Delegate DataFrame attributes to underlying DataFrame.\n\n        This allows var to behave like a DataFrame when accessed directly,\n        e.g., var.columns, var.head(), var.shape, etc.\n        \"\"\"\n        # Don't delegate special methods or our own methods\n        if name.startswith(\"_\") or name in dir(self):\n            raise AttributeError(\n                f\"'{type(self).__name__}' object has no attribute '{name}'\"\n            )\n\n        # Delegate to underlying DataFrame\n        try:\n            return getattr(self._get_dataframe(), name)\n        except AttributeError as err:\n            raise AttributeError(\n                f\"'{type(self).__name__}' object has no attribute '{name}'\"\n            ) from err\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarView-attributes","title":"Attributes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarView.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Required by LazySparseMixin - uses parent's shape</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarView-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarView.__init__","title":"<code>__init__(lazy_adata: LazyAnnData)</code>","text":"<p>Initialize LazyVarView with LazyAnnData.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(self, lazy_adata: \"LazyAnnData\"):\n    \"\"\"\n    Initialize LazyVarView with LazyAnnData.\n\n    Args:\n        lazy_adata: LazyAnnData instance containing the single-cell data.\n    \"\"\"\n    super().__init__()\n    self.lazy_adata = lazy_adata\n    self._slaf_array = lazy_adata.slaf\n    # Required by LazySparseMixin\n    self.slaf_array = lazy_adata.slaf\n    self._shape = lazy_adata.shape  # (n_cells, n_genes)\n    self.table_name = \"genes\"\n    self.id_column = \"gene_integer_id\"\n    self.table_type = \"var\"\n    self._dataframe: pd.DataFrame | None = None  # Cached DataFrame\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsmView","title":"<code>LazyObsmView</code>","text":"<p>               Bases: <code>LazyMetadataViewMixin</code></p> <p>Dictionary-like view of obsm (multi-dimensional obs annotations).</p> <p>LazyObsmView provides a dictionary-like interface for accessing and mutating multi-dimensional cell annotations (e.g., UMAP, PCA embeddings) stored as separate columns in the cells.lance table. Each key maps to a 2D numpy array.</p> Key Features <ul> <li>Dictionary-like interface: obsm[\"X_umap\"], \"X_umap\" in obsm, len(obsm)</li> <li>Multi-dimensional arrays: stored as FixedSizeListArray columns (native Lance vector type)</li> <li>Schema-based detection: automatically detects vector columns from Lance schema</li> <li>Selector support: respects cell selectors from parent LazyAnnData</li> <li>Immutability: prevents deletion/modification of converted embeddings</li> <li>Config.json consistency: reads from config for fast key discovery</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access an embedding\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; umap = adata.obsm[\"X_umap\"]\n&gt;&gt;&gt; print(f\"UMAP shape: {umap.shape}\")\nUMAP shape: (1000, 2)\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a new embedding\n&gt;&gt;&gt; adata.obsm[\"X_pca\"] = pca_coords  # shape: (1000, 50)\n&gt;&gt;&gt; assert \"X_pca\" in adata.obsm\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyObsmView(LazyMetadataViewMixin):\n    \"\"\"\n    Dictionary-like view of obsm (multi-dimensional obs annotations).\n\n    LazyObsmView provides a dictionary-like interface for accessing and mutating\n    multi-dimensional cell annotations (e.g., UMAP, PCA embeddings) stored as\n    separate columns in the cells.lance table. Each key maps to a 2D numpy array.\n\n    Key Features:\n        - Dictionary-like interface: obsm[\"X_umap\"], \"X_umap\" in obsm, len(obsm)\n        - Multi-dimensional arrays: stored as FixedSizeListArray columns (native Lance vector type)\n        - Schema-based detection: automatically detects vector columns from Lance schema\n        - Selector support: respects cell selectors from parent LazyAnnData\n        - Immutability: prevents deletion/modification of converted embeddings\n        - Config.json consistency: reads from config for fast key discovery\n\n    Examples:\n        &gt;&gt;&gt; # Access an embedding\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; umap = adata.obsm[\"X_umap\"]\n        &gt;&gt;&gt; print(f\"UMAP shape: {umap.shape}\")\n        UMAP shape: (1000, 2)\n\n        &gt;&gt;&gt; # Create a new embedding\n        &gt;&gt;&gt; adata.obsm[\"X_pca\"] = pca_coords  # shape: (1000, 50)\n        &gt;&gt;&gt; assert \"X_pca\" in adata.obsm\n    \"\"\"\n\n    def __init__(self, lazy_adata: \"LazyAnnData\"):\n        \"\"\"\n        Initialize LazyObsmView with LazyAnnData.\n\n        Args:\n            lazy_adata: LazyAnnData instance containing the single-cell data.\n        \"\"\"\n        super().__init__()\n        self.lazy_adata = lazy_adata\n        self._slaf_array = lazy_adata.slaf\n        # Required by LazySparseMixin\n        self.slaf_array = lazy_adata.slaf\n        self._shape = lazy_adata.shape  # (n_cells, n_genes)\n        self.table_name = \"cells\"\n        self.id_column = \"cell_integer_id\"\n        self.table_type = \"obsm\"\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Required by LazySparseMixin - uses parent's shape\"\"\"\n        return self._shape\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsmView-attributes","title":"Attributes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsmView.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Required by LazySparseMixin - uses parent's shape</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsmView-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyObsmView.__init__","title":"<code>__init__(lazy_adata: LazyAnnData)</code>","text":"<p>Initialize LazyObsmView with LazyAnnData.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(self, lazy_adata: \"LazyAnnData\"):\n    \"\"\"\n    Initialize LazyObsmView with LazyAnnData.\n\n    Args:\n        lazy_adata: LazyAnnData instance containing the single-cell data.\n    \"\"\"\n    super().__init__()\n    self.lazy_adata = lazy_adata\n    self._slaf_array = lazy_adata.slaf\n    # Required by LazySparseMixin\n    self.slaf_array = lazy_adata.slaf\n    self._shape = lazy_adata.shape  # (n_cells, n_genes)\n    self.table_name = \"cells\"\n    self.id_column = \"cell_integer_id\"\n    self.table_type = \"obsm\"\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarmView","title":"<code>LazyVarmView</code>","text":"<p>               Bases: <code>LazyMetadataViewMixin</code></p> <p>Dictionary-like view of varm (multi-dimensional var annotations).</p> <p>LazyVarmView provides a dictionary-like interface for accessing and mutating multi-dimensional gene annotations (e.g., PCA loadings) stored as separate columns in the genes.lance table. Each key maps to a 2D numpy array.</p> Key Features <ul> <li>Dictionary-like interface: varm[\"PCs\"], \"PCs\" in varm, len(varm)</li> <li>Multi-dimensional arrays: stored as FixedSizeListArray columns (native Lance vector type)</li> <li>Schema-based detection: automatically detects vector columns from Lance schema</li> <li>Selector support: respects gene selectors from parent LazyAnnData</li> <li>Immutability: prevents deletion/modification of converted embeddings</li> <li>Config.json consistency: reads from config for fast key discovery</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access gene loadings\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; pcs = adata.varm[\"PCs\"]\n&gt;&gt;&gt; print(f\"PCs shape: {pcs.shape}\")\nPCs shape: (20000, 50)\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyVarmView(LazyMetadataViewMixin):\n    \"\"\"\n    Dictionary-like view of varm (multi-dimensional var annotations).\n\n    LazyVarmView provides a dictionary-like interface for accessing and mutating\n    multi-dimensional gene annotations (e.g., PCA loadings) stored as separate\n    columns in the genes.lance table. Each key maps to a 2D numpy array.\n\n    Key Features:\n        - Dictionary-like interface: varm[\"PCs\"], \"PCs\" in varm, len(varm)\n        - Multi-dimensional arrays: stored as FixedSizeListArray columns (native Lance vector type)\n        - Schema-based detection: automatically detects vector columns from Lance schema\n        - Selector support: respects gene selectors from parent LazyAnnData\n        - Immutability: prevents deletion/modification of converted embeddings\n        - Config.json consistency: reads from config for fast key discovery\n\n    Examples:\n        &gt;&gt;&gt; # Access gene loadings\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; pcs = adata.varm[\"PCs\"]\n        &gt;&gt;&gt; print(f\"PCs shape: {pcs.shape}\")\n        PCs shape: (20000, 50)\n    \"\"\"\n\n    def __init__(self, lazy_adata: \"LazyAnnData\"):\n        \"\"\"\n        Initialize LazyVarmView with LazyAnnData.\n\n        Args:\n            lazy_adata: LazyAnnData instance containing the single-cell data.\n        \"\"\"\n        super().__init__()\n        self.lazy_adata = lazy_adata\n        self._slaf_array = lazy_adata.slaf\n        # Required by LazySparseMixin\n        self.slaf_array = lazy_adata.slaf\n        self._shape = lazy_adata.shape  # (n_cells, n_genes)\n        self.table_name = \"genes\"\n        self.id_column = \"gene_integer_id\"\n        self.table_type = \"varm\"\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Required by LazySparseMixin - uses parent's shape\"\"\"\n        return self._shape\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarmView-attributes","title":"Attributes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarmView.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Required by LazySparseMixin - uses parent's shape</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarmView-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyVarmView.__init__","title":"<code>__init__(lazy_adata: LazyAnnData)</code>","text":"<p>Initialize LazyVarmView with LazyAnnData.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(self, lazy_adata: \"LazyAnnData\"):\n    \"\"\"\n    Initialize LazyVarmView with LazyAnnData.\n\n    Args:\n        lazy_adata: LazyAnnData instance containing the single-cell data.\n    \"\"\"\n    super().__init__()\n    self.lazy_adata = lazy_adata\n    self._slaf_array = lazy_adata.slaf\n    # Required by LazySparseMixin\n    self.slaf_array = lazy_adata.slaf\n    self._shape = lazy_adata.shape  # (n_cells, n_genes)\n    self.table_name = \"genes\"\n    self.id_column = \"gene_integer_id\"\n    self.table_type = \"varm\"\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyUnsView","title":"<code>LazyUnsView</code>","text":"<p>               Bases: <code>LazyDictionaryViewMixin</code></p> <p>Dictionary-like view of uns (unstructured metadata).</p> <p>LazyUnsView provides a dictionary-like interface for accessing and mutating unstructured metadata stored in uns.json. Unlike obs/var/obsm/varm, uns metadata is always mutable and stored as JSON.</p> Key Features <ul> <li>Dictionary-like interface: uns[\"key\"], \"key\" in uns, len(uns)</li> <li>JSON storage: stored in uns.json file</li> <li>Always mutable: no immutability tracking</li> <li>JSON serialization: automatically converts numpy/pandas objects</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Store metadata\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; adata.uns[\"neighbors\"] = {\"params\": {\"n_neighbors\": 15}}\n&gt;&gt;&gt; print(adata.uns[\"neighbors\"])\n{'params': {'n_neighbors': 15}}\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyUnsView(LazyDictionaryViewMixin):\n    \"\"\"\n    Dictionary-like view of uns (unstructured metadata).\n\n    LazyUnsView provides a dictionary-like interface for accessing and mutating\n    unstructured metadata stored in uns.json. Unlike obs/var/obsm/varm, uns\n    metadata is always mutable and stored as JSON.\n\n    Key Features:\n        - Dictionary-like interface: uns[\"key\"], \"key\" in uns, len(uns)\n        - JSON storage: stored in uns.json file\n        - Always mutable: no immutability tracking\n        - JSON serialization: automatically converts numpy/pandas objects\n\n    Examples:\n        &gt;&gt;&gt; # Store metadata\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; adata.uns[\"neighbors\"] = {\"params\": {\"n_neighbors\": 15}}\n        &gt;&gt;&gt; print(adata.uns[\"neighbors\"])\n        {'params': {'n_neighbors': 15}}\n    \"\"\"\n\n    def __init__(self, lazy_adata: \"LazyAnnData\"):\n        \"\"\"\n        Initialize LazyUnsView with LazyAnnData.\n\n        Args:\n            lazy_adata: LazyAnnData instance containing the single-cell data.\n        \"\"\"\n        self.lazy_adata = lazy_adata\n        self._slaf_array = lazy_adata.slaf\n        self._uns_path = self._slaf_array._join_path(\n            self._slaf_array.slaf_path, \"uns.json\"\n        )\n        self._uns_data: dict | None = None\n\n    def _load_uns(self) -&gt; dict:\n        \"\"\"Load uns.json if it exists, otherwise return empty dict\"\"\"\n        if self._uns_data is not None:\n            return self._uns_data\n\n        # Check if file exists (not directory)\n        import os\n\n        if self._slaf_array._is_cloud_path(self._uns_path):\n            # For cloud paths, try to open the file\n            try:\n                with self._slaf_array._open_file(self._uns_path) as f:\n                    import json\n\n                    self._uns_data = json.load(f)\n            except Exception:\n                self._uns_data = {}\n        else:\n            # For local paths, check if file exists\n            if os.path.exists(self._uns_path) and os.path.isfile(self._uns_path):\n                with self._slaf_array._open_file(self._uns_path) as f:\n                    import json\n\n                    self._uns_data = json.load(f)\n            else:\n                self._uns_data = {}\n\n        return self._uns_data\n\n    def keys(self) -&gt; list[str]:\n        \"\"\"List all available keys\"\"\"\n        return list(self._load_uns().keys())\n\n    def __getitem__(self, key: str) -&gt; Any:\n        \"\"\"Retrieve metadata value\"\"\"\n        uns_data = self._load_uns()\n        if key not in uns_data:\n            raise KeyError(f\"uns key '{key}' not found\")\n        return uns_data[key]\n\n    def __setitem__(self, key: str, value: Any):\n        \"\"\"Store metadata value\"\"\"\n        uns_data = self._load_uns()\n\n        # Validate key name\n        self._validate_name(key)\n\n        # Convert numpy arrays and pandas objects to JSON-serializable\n        value = self._json_serialize(value)\n\n        uns_data[key] = value\n        self._save_uns(uns_data)\n\n    def __delitem__(self, key: str):\n        \"\"\"Delete metadata key\"\"\"\n        uns_data = self._load_uns()\n        if key not in uns_data:\n            raise KeyError(f\"uns key '{key}' not found\")\n        del uns_data[key]\n        self._save_uns(uns_data)\n\n    def _json_serialize(self, value: Any) -&gt; Any:\n        \"\"\"Convert value to JSON-serializable format\"\"\"\n        import pandas as pd\n\n        if isinstance(value, np.ndarray):\n            return value.tolist()\n        elif isinstance(value, np.integer | np.floating):\n            return value.item()\n        elif isinstance(value, pd.Series):\n            return value.tolist()\n        elif isinstance(value, dict):\n            return {k: self._json_serialize(v) for k, v in value.items()}\n        elif isinstance(value, list):\n            return [self._json_serialize(item) for item in value]\n        return value\n\n    def _save_uns(self, uns_data: dict):\n        \"\"\"Save uns.json atomically\"\"\"\n        import json\n\n        # Use same cloud-compatible file writing as config.json\n        with self._slaf_array._open_file(self._uns_path, \"w\") as f:\n            json.dump(uns_data, f, indent=2)\n\n        # Invalidate cache\n        self._uns_data = uns_data\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyUnsView-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyUnsView.__init__","title":"<code>__init__(lazy_adata: LazyAnnData)</code>","text":"<p>Initialize LazyUnsView with LazyAnnData.</p> <p>Parameters:</p> Name Type Description Default <code>lazy_adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(self, lazy_adata: \"LazyAnnData\"):\n    \"\"\"\n    Initialize LazyUnsView with LazyAnnData.\n\n    Args:\n        lazy_adata: LazyAnnData instance containing the single-cell data.\n    \"\"\"\n    self.lazy_adata = lazy_adata\n    self._slaf_array = lazy_adata.slaf\n    self._uns_path = self._slaf_array._join_path(\n        self._slaf_array.slaf_path, \"uns.json\"\n    )\n    self._uns_data: dict | None = None\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyUnsView.keys","title":"<code>keys() -&gt; list[str]</code>","text":"<p>List all available keys</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def keys(self) -&gt; list[str]:\n    \"\"\"List all available keys\"\"\"\n    return list(self._load_uns().keys())\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData","title":"<code>LazyAnnData</code>","text":"<p>               Bases: <code>LazySparseMixin</code></p> <p>AnnData-compatible interface for SLAF data with lazy evaluation.</p> <p>LazyAnnData provides a drop-in replacement for AnnData objects that works with SLAF datasets. It implements lazy evaluation to avoid loading all data into memory, making it suitable for large single-cell datasets.</p> Key Features <ul> <li>AnnData-compatible interface</li> <li>Lazy evaluation for memory efficiency</li> <li>Support for cell and gene subsetting</li> <li>Integration with scanpy workflows</li> <li>Automatic metadata loading</li> <li>Transformation caching</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; print(f\"AnnData shape: {adata.shape}\")\nAnnData shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Access expression data\n&gt;&gt;&gt; print(f\"Expression matrix shape: {adata.X.shape}\")\nExpression matrix shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Access metadata\n&gt;&gt;&gt; print(f\"Cell metadata columns: {list(adata.obs.columns)}\")\nCell metadata columns: ['cell_type', 'total_counts', 'batch']\n&gt;&gt;&gt; print(f\"Gene metadata columns: {list(adata.var.columns)}\")\nGene metadata columns: ['gene_type', 'chromosome']\n</code></pre> <pre><code>&gt;&gt;&gt; # Subsetting operations\n&gt;&gt;&gt; subset = adata[:100, :5000]  # First 100 cells, first 5000 genes\n&gt;&gt;&gt; print(f\"Subset shape: {subset.shape}\")\nSubset shape: (100, 5000)\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>class LazyAnnData(LazySparseMixin):\n    \"\"\"\n    AnnData-compatible interface for SLAF data with lazy evaluation.\n\n    LazyAnnData provides a drop-in replacement for AnnData objects that works with\n    SLAF datasets. It implements lazy evaluation to avoid loading all data into memory,\n    making it suitable for large single-cell datasets.\n\n    Key Features:\n        - AnnData-compatible interface\n        - Lazy evaluation for memory efficiency\n        - Support for cell and gene subsetting\n        - Integration with scanpy workflows\n        - Automatic metadata loading\n        - Transformation caching\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; print(f\"AnnData shape: {adata.shape}\")\n        AnnData shape: (1000, 20000)\n\n        &gt;&gt;&gt; # Access expression data\n        &gt;&gt;&gt; print(f\"Expression matrix shape: {adata.X.shape}\")\n        Expression matrix shape: (1000, 20000)\n\n        &gt;&gt;&gt; # Access metadata\n        &gt;&gt;&gt; print(f\"Cell metadata columns: {list(adata.obs.columns)}\")\n        Cell metadata columns: ['cell_type', 'total_counts', 'batch']\n        &gt;&gt;&gt; print(f\"Gene metadata columns: {list(adata.var.columns)}\")\n        Gene metadata columns: ['gene_type', 'chromosome']\n\n        &gt;&gt;&gt; # Subsetting operations\n        &gt;&gt;&gt; subset = adata[:100, :5000]  # First 100 cells, first 5000 genes\n        &gt;&gt;&gt; print(f\"Subset shape: {subset.shape}\")\n        Subset shape: (100, 5000)\n    \"\"\"\n\n    def __init__(\n        self,\n        slaf_array: SLAFArray,\n        backend: str = \"auto\",\n    ):\n        \"\"\"\n        Initialize LazyAnnData with SLAF array.\n\n        Args:\n            slaf_array: SLAFArray instance containing the single-cell data.\n                       Used for database queries and metadata access.\n            backend: Backend for expression matrix. Currently supports \"scipy\" and \"auto\".\n                    \"auto\" defaults to \"scipy\" for sparse matrix operations.\n\n        Raises:\n            ValueError: If the backend is not supported.\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; print(f\"Backend: {adata.backend}\")\n            Backend: auto\n\n            &gt;&gt;&gt; # With explicit backend\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array, backend=\"scipy\")\n            &gt;&gt;&gt; print(f\"Backend: {adata.backend}\")\n            Backend: scipy\n\n            &gt;&gt;&gt; # Error handling for unsupported backend\n            &gt;&gt;&gt; try:\n            ...     adata = LazyAnnData(slaf_array, backend=\"unsupported\")\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Unknown backend: unsupported\n        \"\"\"\n        super().__init__()\n        self.slaf = slaf_array\n        self.backend = backend\n\n        # Initialize expression matrix\n        if backend == \"scipy\" or backend == \"auto\":\n            self._X = LazyExpressionMatrix(slaf_array)\n            self._X.parent_adata = self  # Set parent reference for transformations\n        else:\n            raise ValueError(f\"Unknown backend: {backend}\")\n\n        # Lazy-loaded metadata\n        self._obs = None\n        self._var = None\n        self._cached_obs_names: pd.Index | None = None\n        self._cached_var_names: pd.Index | None = None\n\n        # Filter selectors for subsetting\n        self._cell_selector: Any = None\n        self._gene_selector: Any = None\n        self._filtered_obs: Callable[[], pd.DataFrame] | None = None\n        self._filtered_var: Callable[[], pd.DataFrame] | None = None\n\n        # Reference to parent LazyAnnData (for sliced objects)\n        self._parent_adata: LazyAnnData | None = None\n\n        # Transformations for lazy evaluation\n        self._transformations: dict[str, Any] = {}\n\n        # Layers view (lazy initialization)\n        self._layers: LazyLayersView | None = None\n\n    @property\n    def layers(self) -&gt; LazyLayersView:\n        \"\"\"\n        Access to layers (dictionary-like interface).\n\n        Returns a dictionary-like view of layers that provides access to alternative\n        representations of the expression matrix (e.g., spliced, unspliced, counts).\n        Layers have the same dimensions as X.\n\n        Returns:\n            LazyLayersView providing dictionary-like access to layers.\n\n        Examples:\n            &gt;&gt;&gt; # Access a layer\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; spliced = adata.layers[\"spliced\"]\n            &gt;&gt;&gt; print(f\"Layer shape: {spliced.shape}\")\n            Layer shape: (1000, 20000)\n\n            &gt;&gt;&gt; # List available layers\n            &gt;&gt;&gt; print(list(adata.layers.keys()))\n            ['spliced', 'unspliced']\n\n            &gt;&gt;&gt; # Check if layer exists\n            &gt;&gt;&gt; assert \"spliced\" in adata.layers\n        \"\"\"\n        if self._layers is None:\n            self._layers = LazyLayersView(self)\n        return self._layers\n\n    @property\n    def obs(self) -&gt; LazyObsView:\n        \"\"\"\n        Cell metadata (observations) - mutable view with DataFrame-like and dict-like interfaces.\n\n        Returns a view that provides both DataFrame-like and dictionary-like access to cell\n        metadata columns with support for creating, updating, and deleting columns. This view\n        respects cell selectors from parent LazyAnnData.\n\n        When accessed with a string key (e.g., ``obs[\"cluster\"]``), returns a pandas Series\n        (AnnData-compatible). When accessed directly, behaves like a DataFrame.\n\n        Returns:\n            LazyObsView providing DataFrame-like and dictionary-like access to obs columns.\n\n        Examples:\n            &gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; df = adata.obs  # Returns DataFrame-like view\n            &gt;&gt;&gt; cluster = adata.obs[\"cluster\"]  # Returns pd.Series (AnnData-compatible)\n            &gt;&gt;&gt; print(cluster.head())\n\n            &gt;&gt;&gt; # Create a new column\n            &gt;&gt;&gt; adata.obs[\"new_cluster\"] = new_cluster_labels\n            &gt;&gt;&gt; assert \"new_cluster\" in adata.obs\n\n            &gt;&gt;&gt; # List available columns\n            &gt;&gt;&gt; print(list(adata.obs.keys()))\n            ['cell_id', 'total_counts', 'cluster', 'new_cluster']\n        \"\"\"\n        if not hasattr(self, \"_obs_view\"):\n            self._obs_view = LazyObsView(self)\n        return self._obs_view\n\n    @property\n    def var(self) -&gt; LazyVarView:\n        \"\"\"\n        Gene metadata (variables) - mutable view with DataFrame-like and dict-like interfaces.\n\n        Returns a view that provides both DataFrame-like and dictionary-like access to gene\n        metadata columns with support for creating, updating, and deleting columns. This view\n        respects gene selectors from parent LazyAnnData.\n\n        When accessed with a string key (e.g., ``var[\"highly_variable\"]``), returns a pandas\n        Series (AnnData-compatible). When accessed directly, behaves like a DataFrame.\n\n        Returns:\n            LazyVarView providing DataFrame-like and dictionary-like access to var columns.\n\n        Examples:\n            &gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; df = adata.var  # Returns DataFrame-like view\n            &gt;&gt;&gt; hvg = adata.var[\"highly_variable\"]  # Returns pd.Series (AnnData-compatible)\n            &gt;&gt;&gt; print(hvg.head())\n\n            &gt;&gt;&gt; # Create a new column\n            &gt;&gt;&gt; adata.var[\"new_annotation\"] = new_annotations\n            &gt;&gt;&gt; assert \"new_annotation\" in adata.var\n        \"\"\"\n        if not hasattr(self, \"_var_view\"):\n            self._var_view = LazyVarView(self)\n        return self._var_view\n\n    @property\n    def obsm(self) -&gt; LazyObsmView:\n        \"\"\"\n        Multi-dimensional obs annotations (embeddings, PCA, etc.).\n\n        Returns a dictionary-like view that provides access to multi-dimensional\n        cell annotations stored as separate columns in cells.lance. Each key maps\n        to a 2D numpy array.\n\n        Returns:\n            LazyObsmView providing dictionary-like access to obsm keys.\n\n        Examples:\n            &gt;&gt;&gt; # Access an embedding\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; umap = adata.obsm[\"X_umap\"]\n            &gt;&gt;&gt; print(f\"UMAP shape: {umap.shape}\")\n            UMAP shape: (1000, 2)\n\n            &gt;&gt;&gt; # Create a new embedding\n            &gt;&gt;&gt; adata.obsm[\"X_pca\"] = pca_coords  # shape: (1000, 50)\n            &gt;&gt;&gt; assert \"X_pca\" in adata.obsm\n        \"\"\"\n        if not hasattr(self, \"_obsm\"):\n            self._obsm = LazyObsmView(self)\n        return self._obsm\n\n    @property\n    def varm(self) -&gt; LazyVarmView:\n        \"\"\"\n        Multi-dimensional var annotations (gene loadings, etc.).\n\n        Returns a dictionary-like view that provides access to multi-dimensional\n        gene annotations stored as separate columns in genes.lance. Each key maps\n        to a 2D numpy array.\n\n        Returns:\n            LazyVarmView providing dictionary-like access to varm keys.\n\n        Examples:\n            &gt;&gt;&gt; # Access gene loadings\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; pcs = adata.varm[\"PCs\"]\n            &gt;&gt;&gt; print(f\"PCs shape: {pcs.shape}\")\n            PCs shape: (20000, 50)\n        \"\"\"\n        if not hasattr(self, \"_varm\"):\n            self._varm = LazyVarmView(self)\n        return self._varm\n\n    @property\n    def uns(self) -&gt; LazyUnsView:\n        \"\"\"\n        Unstructured metadata (analysis parameters, etc.).\n\n        Returns a dictionary-like view that provides access to unstructured\n        metadata stored in uns.json. Unlike obs/var/obsm/varm, uns metadata\n        is always mutable and stored as JSON.\n\n        Returns:\n            LazyUnsView providing dictionary-like access to uns keys.\n\n        Examples:\n            &gt;&gt;&gt; # Store metadata\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; adata.uns[\"neighbors\"] = {\"params\": {\"n_neighbors\": 15}}\n            &gt;&gt;&gt; print(adata.uns[\"neighbors\"])\n            {'params': {'n_neighbors': 15}}\n        \"\"\"\n        if not hasattr(self, \"_uns\"):\n            self._uns = LazyUnsView(self)\n        return self._uns\n\n    @property\n    def X(self) -&gt; LazyExpressionMatrix:\n        \"\"\"\n        Access to expression data.\n\n        Returns the lazy expression matrix that provides scipy.sparse-compatible\n        access to the single-cell expression data. The matrix is lazily evaluated\n        to avoid loading all data into memory.\n\n        Returns:\n            LazyExpressionMatrix providing scipy.sparse-compatible interface.\n\n        Examples:\n            &gt;&gt;&gt; # Access expression data\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; print(f\"Expression matrix shape: {adata.X.shape}\")\n            Expression matrix shape: (1000, 20000)\n\n            &gt;&gt;&gt; # Subsetting expression data\n            &gt;&gt;&gt; subset_X = adata.X[:100, :5000]\n            &gt;&gt;&gt; print(f\"Subset expression shape: {subset_X.shape}\")\n            Subset expression shape: (100, 5000)\n\n            &gt;&gt;&gt; # Check matrix type\n            &gt;&gt;&gt; print(f\"Matrix type: {type(adata.X)}\")\n            Matrix type: &lt;class 'slaf.integrations.anndata.LazyExpressionMatrix'&gt;\n        \"\"\"\n        return self._X\n\n    @property\n    def obs_deprecated(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Cell metadata (observations) - DEPRECATED.\n\n        .. deprecated:: 0.X\n            This property is deprecated. Use :attr:`obs` (formerly :attr:`obs`) instead.\n            This will be removed in a future version.\n\n        This property triggers computation of metadata when accessed.\n        For lazy access to metadata structure only, use obs.columns, obs.index, etc.\n        \"\"\"\n        import warnings\n\n        warnings.warn(\n            \"obs_deprecated is deprecated and will be removed in a future version. \"\n            \"Use obs instead, which provides both DataFrame-like and dict-like access \"\n            \"with mutation support.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self._filtered_obs is not None:\n            result = self._filtered_obs()\n            if isinstance(result, pd.DataFrame):\n                return result\n            return pd.DataFrame()\n        if self._obs is None:\n            # Use the obs from SLAFArray (now polars DataFrame)\n            obs_df = getattr(self.slaf, \"obs\", None)\n            if obs_df is not None:\n                # Work with polars DataFrame internally\n                obs_pl = obs_df\n                # Drop cell_integer_id column if present to match AnnData expectations\n                if \"cell_integer_id\" in obs_pl.columns:\n                    obs_pl = obs_pl.drop(\"cell_integer_id\")\n                # Filter out vector columns (obsm) - these are FixedSizeListArray columns\n                # Vector columns should only be accessed via adata.obsm, not adata.obs\n                cells_table = getattr(self.slaf, \"cells\", None)\n                if cells_table is not None:\n                    schema = cells_table.schema\n                    vector_column_names = {\n                        field.name\n                        for field in schema\n                        if isinstance(field.type, pa.FixedSizeListType)\n                    }\n                    # Drop vector columns from obs\n                    columns_to_drop = [\n                        col for col in obs_pl.columns if col in vector_column_names\n                    ]\n                    if columns_to_drop:\n                        obs_pl = obs_pl.drop(columns_to_drop)\n                # Convert to pandas DataFrame for AnnData compatibility at API boundary\n                obs_copy = obs_pl.to_pandas()\n                # Set cell_id as index if present, otherwise use default index\n                if \"cell_id\" in obs_copy.columns:\n                    obs_copy = obs_copy.set_index(\"cell_id\")\n                # Set index name to match AnnData format\n                if hasattr(obs_copy, \"index\"):\n                    obs_copy.index.name = \"cell_id\"\n                self._obs = obs_copy\n            else:\n                self._obs = pd.DataFrame()\n        return self._obs\n\n    @property\n    def var_deprecated(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Gene metadata (variables) - DEPRECATED.\n\n        .. deprecated:: 0.X\n            This property is deprecated. Use :attr:`var` (formerly :attr:`var`) instead.\n            This will be removed in a future version.\n\n        This property triggers computation of metadata when accessed.\n        For lazy access to metadata structure only, use var.columns, var.index, etc.\n        \"\"\"\n        import warnings\n\n        warnings.warn(\n            \"var_deprecated is deprecated and will be removed in a future version. \"\n            \"Use var instead, which provides both DataFrame-like and dict-like access \"\n            \"with mutation support.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self._filtered_var is not None:\n            result = self._filtered_var()\n            if isinstance(result, pd.DataFrame):\n                return result\n            return pd.DataFrame()\n        if self._var is None:\n            var_df = getattr(self.slaf, \"var\", None)\n            if var_df is not None:\n                # Work with polars DataFrame internally\n                var_pl = var_df\n                # Drop gene_integer_id column if present to match AnnData expectations\n                if \"gene_integer_id\" in var_pl.columns:\n                    var_pl = var_pl.drop(\"gene_integer_id\")\n                # Filter out vector columns (varm) - these are FixedSizeListArray columns\n                # Vector columns should only be accessed via adata.varm, not adata.var\n                genes_table = getattr(self.slaf, \"genes\", None)\n                if genes_table is not None:\n                    schema = genes_table.schema\n                    vector_column_names = {\n                        field.name\n                        for field in schema\n                        if isinstance(field.type, pa.FixedSizeListType)\n                    }\n                    # Drop vector columns from var\n                    columns_to_drop = [\n                        col for col in var_pl.columns if col in vector_column_names\n                    ]\n                    if columns_to_drop:\n                        var_pl = var_pl.drop(columns_to_drop)\n                # Convert to pandas DataFrame for AnnData compatibility at API boundary\n                var_copy = var_pl.to_pandas()\n                # Set gene_id as index if present, otherwise use default index\n                if \"gene_id\" in var_copy.columns:\n                    var_copy = var_copy.set_index(\"gene_id\")\n                # Set index name to match AnnData format\n                if hasattr(var_copy, \"index\"):\n                    var_copy.index.name = \"gene_id\"\n                self._var = var_copy\n            else:\n                self._var = pd.DataFrame()\n        return self._var\n\n    @property\n    def obs_names(self) -&gt; pd.Index:\n        \"\"\"Cell names\"\"\"\n        if self._cached_obs_names is None:\n            # Use the DataFrame's index from the view\n            self._cached_obs_names = self.obs._get_dataframe().index\n        return self._cached_obs_names\n\n    @property\n    def var_names(self) -&gt; pd.Index:\n        \"\"\"Gene names\"\"\"\n        if self._cached_var_names is None:\n            # Use the DataFrame's index from the view\n            self._cached_var_names = self.var._get_dataframe().index\n        return self._cached_var_names\n\n    @property\n    def n_obs(self) -&gt; int:\n        \"\"\"Number of observations (cells)\"\"\"\n        return self.shape[0]\n\n    @property\n    def n_vars(self) -&gt; int:\n        \"\"\"Number of variables (genes)\"\"\"\n        return self.shape[1]\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"Get the shape of the data, accounting for any applied filters\"\"\"\n        if (\n            getattr(self, \"_cell_selector\", None) is not None\n            or getattr(self, \"_gene_selector\", None) is not None\n        ):\n            # Calculate the shape based on selectors\n            cell_selector = (\n                self._cell_selector\n                if getattr(self, \"_cell_selector\", None) is not None\n                else slice(None)\n            )\n            gene_selector = (\n                self._gene_selector\n                if getattr(self, \"_gene_selector\", None) is not None\n                else slice(None)\n            )\n\n            # Use the same logic as _get_result_shape in LazySparseMixin\n            n_cells = self._calculate_selected_count(cell_selector, axis=0)\n            n_genes = self._calculate_selected_count(gene_selector, axis=1)\n\n            return (n_cells, n_genes)\n        else:\n            # No filters applied, return original shape\n            return self.slaf.shape\n\n    def _calculate_selected_count(self, selector, axis: int) -&gt; int:\n        \"\"\"Calculate the number of selected entities for a given selector\"\"\"\n        if selector is None or (\n            isinstance(selector, slice) and selector == slice(None)\n        ):\n            return self.slaf.shape[axis]\n\n        if isinstance(selector, slice):\n            start = selector.start or 0\n            stop = selector.stop or self.slaf.shape[axis]\n            step = selector.step or 1\n\n            # Handle negative indices\n            if start &lt; 0:\n                start = self.slaf.shape[axis] + start\n            if stop &lt; 0:\n                stop = self.slaf.shape[axis] + stop\n\n            # Clamp bounds to actual data size\n            start = max(0, min(start, self.slaf.shape[axis]))\n            stop = max(0, min(stop, self.slaf.shape[axis]))\n\n            return len(range(start, stop, step))\n        elif isinstance(selector, list | np.ndarray):\n            if isinstance(selector, np.ndarray) and selector.dtype == bool:\n                return np.sum(selector)\n            return len(selector)\n        elif isinstance(selector, int | np.integer):\n            return 1\n        else:\n            return self.slaf.shape[axis]\n\n    def __getitem__(self, key) -&gt; \"LazyAnnData\":\n        \"\"\"Subset the data, composing selectors if already sliced\"\"\"\n        # Disallow chained slicing on LazyAnnData objects\n        if self._cell_selector is not None or self._gene_selector is not None:\n            raise NotImplementedError(\n                \"Chained slicing on LazyAnnData objects is not supported. \"\n                \"Please use adata[rows, cols] for single-step slicing instead of \"\n                \"adata[rows][:, cols]. For chained slicing on the expression matrix, \"\n                \"use adata.X[rows][:, cols].\"\n            )\n\n        # Parse indexing key using the same logic as LazyExpressionMatrix\n        cell_selector, gene_selector = self._parse_key(key)\n\n        # Create a new LazyAnnData with the same backend\n        new_adata = LazyAnnData(self.slaf, backend=self.backend)\n\n        # Store reference to parent for accessing full DataFrames\n        new_adata._parent_adata = self\n\n        # Store the selectors for lazy filtering\n        new_adata._cell_selector = cell_selector\n        new_adata._gene_selector = gene_selector\n\n        # Update the LazyExpressionMatrix to know about the slicing\n        if isinstance(new_adata._X, LazyExpressionMatrix):\n            new_adata._X.parent_adata = new_adata\n            # Store the selectors in the LazyExpressionMatrix so it can use them\n            new_adata._X._cell_selector = cell_selector\n            new_adata._X._gene_selector = gene_selector\n            # Update the shape to reflect the slicing\n            new_adata._X._update_shape()\n\n        # Override obs and var properties to apply filtering\n        def filtered_obs() -&gt; pd.DataFrame:\n            # Always apply the composed selector to the original obs DataFrame\n            # Get the FULL DataFrame from parent (without selectors) to apply our selector\n            # Access parent's view without selectors\n            parent = getattr(self, \"_parent_adata\", None)\n            if parent is None:\n                parent = self\n            parent_obs_view = parent.obs\n            # Temporarily clear selectors to get full DataFrame\n            original_cell_selector = getattr(parent, \"_cell_selector\", None)\n            original_gene_selector = getattr(parent, \"_gene_selector\", None)\n            # Temporarily clear selectors\n            parent._cell_selector = None\n            parent._gene_selector = None\n            # Clear cache to force reload\n            if hasattr(parent_obs_view, \"_dataframe\"):\n                parent_obs_view._dataframe = None\n            # Get full DataFrame\n            obs_df = parent_obs_view._get_dataframe()\n            # Restore selectors\n            parent._cell_selector = original_cell_selector\n            parent._gene_selector = original_gene_selector\n            if cell_selector is None or (\n                isinstance(cell_selector, slice) and cell_selector == slice(None)\n            ):\n                pass\n            else:\n                obs_df = obs_df.copy()\n                if isinstance(cell_selector, slice):\n                    start = cell_selector.start\n                    stop = cell_selector.stop\n                    step = cell_selector.step if cell_selector.step is not None else 1\n\n                    # Handle None values\n                    if start is None:\n                        start = 0\n                    if stop is None:\n                        stop = len(obs_df)\n\n                    # Handle negative indices\n                    if start &lt; 0:\n                        start = len(obs_df) + start\n                    if stop &lt; 0:\n                        stop = len(obs_df) + stop\n\n                    # Clamp bounds to valid range\n                    start = max(0, min(start, len(obs_df)))\n                    stop = max(0, min(stop, len(obs_df)))\n                    obs_df = obs_df.iloc[start:stop:step]\n                elif (\n                    isinstance(cell_selector, np.ndarray)\n                    and cell_selector.dtype == bool\n                ):\n                    # Boolean mask - ensure it matches the length\n                    if len(cell_selector) == len(obs_df):\n                        obs_df = obs_df[cell_selector]\n                    else:\n                        # Pad or truncate the mask to match\n                        if len(cell_selector) &lt; len(obs_df):\n                            mask: np.ndarray = np.zeros(len(obs_df), dtype=bool)\n                            mask[: len(cell_selector)] = cell_selector\n                        else:\n                            mask = cell_selector[: len(obs_df)]\n                        obs_df = obs_df[mask]\n                elif isinstance(cell_selector, list | np.ndarray):\n                    # Integer indices\n                    obs_df = obs_df.iloc[cell_selector]\n                elif isinstance(cell_selector, int | np.integer):\n                    obs_df = obs_df.iloc[[int(cell_selector)]]\n                # else: leave as is\n            # Remove unused categories for all categorical columns if not empty\n            if obs_df is not None and not obs_df.empty:\n                for col in obs_df.select_dtypes(include=\"category\").columns:\n                    col_data = obs_df[col]\n                    # Type-safe check for pandas categorical data\n                    if isinstance(col_data, pd.Series) and hasattr(col_data, \"cat\"):\n                        obs_df[col] = col_data.cat.remove_unused_categories()\n            if isinstance(obs_df, pd.DataFrame):\n                return obs_df\n            return pd.DataFrame()\n\n        def filtered_var() -&gt; pd.DataFrame:\n            # Always apply the composed selector to the original var DataFrame\n            # Get the FULL DataFrame from parent (without selectors) to apply our selector\n            # Access parent's view without selectors\n            parent = getattr(self, \"_parent_adata\", None)\n            if parent is None:\n                parent = self\n            parent_var_view = parent.var\n            # Temporarily clear selectors to get full DataFrame\n            original_cell_selector = getattr(parent, \"_cell_selector\", None)\n            original_gene_selector = getattr(parent, \"_gene_selector\", None)\n            # Temporarily clear selectors\n            parent._cell_selector = None\n            parent._gene_selector = None\n            # Clear cache to force reload\n            if hasattr(parent_var_view, \"_dataframe\"):\n                parent_var_view._dataframe = None\n            # Get full DataFrame\n            var_df = parent_var_view._get_dataframe()\n            # Restore selectors\n            parent._cell_selector = original_cell_selector\n            parent._gene_selector = original_gene_selector\n\n            if gene_selector is None or (\n                isinstance(gene_selector, slice) and gene_selector == slice(None)\n            ):\n                pass\n            else:\n                var_df = var_df.copy()\n                if isinstance(gene_selector, slice):\n                    start = gene_selector.start\n                    stop = gene_selector.stop\n                    step = gene_selector.step if gene_selector.step is not None else 1\n\n                    # Handle None values\n                    if start is None:\n                        start = 0\n                    if stop is None:\n                        stop = len(var_df)\n\n                    # Handle negative indices\n                    if start &lt; 0:\n                        start = len(var_df) + start\n                    if stop &lt; 0:\n                        stop = len(var_df) + stop\n\n                    # Clamp bounds to valid range\n                    start = max(0, min(start, len(var_df)))\n                    stop = max(0, min(stop, len(var_df)))\n                    var_df = var_df.iloc[start:stop:step]\n                elif (\n                    isinstance(gene_selector, np.ndarray)\n                    and gene_selector.dtype == bool\n                ):\n                    # Boolean mask - ensure it matches the length\n                    if len(gene_selector) == len(var_df):\n                        var_df = var_df[gene_selector]\n                    else:\n                        # Pad or truncate the mask to match\n                        if len(gene_selector) &lt; len(var_df):\n                            mask: np.ndarray = np.zeros(len(var_df), dtype=bool)\n                            mask[: len(gene_selector)] = gene_selector\n                        else:\n                            mask = gene_selector[: len(var_df)]\n                        var_df = var_df[mask]\n                elif isinstance(gene_selector, list | np.ndarray):\n                    # Integer indices\n                    var_df = var_df.iloc[gene_selector]\n                elif isinstance(gene_selector, int | np.integer):\n                    var_df = var_df.iloc[[int(gene_selector)]]\n                # else: leave as is\n            # Remove unused categories for all categorical columns if not empty\n            if var_df is not None and not var_df.empty:\n                for col in var_df.select_dtypes(include=\"category\").columns:\n                    col_data = var_df[col]\n                    # Type-safe check for pandas categorical data\n                    if isinstance(col_data, pd.Series) and hasattr(col_data, \"cat\"):\n                        var_df[col] = col_data.cat.remove_unused_categories()\n            if isinstance(var_df, pd.DataFrame):\n                return var_df\n            return pd.DataFrame()\n\n        # Store the filter functions\n        new_adata._filtered_obs = filtered_obs\n        new_adata._filtered_var = filtered_var\n\n        # Ensure obs_names and var_names match the filtered DataFrames\n        new_adata._cached_obs_names = new_adata._filtered_obs().index\n        new_adata._cached_var_names = new_adata._filtered_var().index\n\n        # Copy transformations\n        new_adata._transformations = self._transformations.copy()\n\n        # Copy layers view (will be recreated with new selectors when accessed)\n        new_adata._layers = None\n\n        return new_adata\n\n    def copy(self) -&gt; \"LazyAnnData\":\n        \"\"\"Create a copy\"\"\"\n        new_adata = LazyAnnData(self.slaf, backend=self.backend)\n\n        # Copy selectors\n        new_adata._cell_selector = self._cell_selector\n        new_adata._gene_selector = self._gene_selector\n\n        # Copy filtered metadata functions\n        new_adata._filtered_obs = self._filtered_obs\n        new_adata._filtered_var = self._filtered_var\n\n        # Copy transformations\n        new_adata._transformations = self._transformations.copy()\n\n        # Set up parent reference for the expression matrix\n        if isinstance(new_adata._X, LazyExpressionMatrix):\n            new_adata._X.parent_adata = new_adata\n\n        return new_adata\n\n    def to_memory(self) -&gt; scipy.sparse.spmatrix:\n        \"\"\"Load entire matrix into memory\"\"\"\n        return self.get_expression_data()\n\n    def write(self, filename: str):\n        \"\"\"Write to h5ad format (would need implementation)\"\"\"\n        raise NotImplementedError(\"Writing LazyAnnData not yet implemented\")\n\n    def get_expression_data(self) -&gt; scipy.sparse.csr_matrix:\n        \"\"\"Get expression data with any applied filters\"\"\"\n        if isinstance(self._X, LazyExpressionMatrix):\n            # If the LazyExpressionMatrix already has selectors stored, just get the data\n            # without passing additional selectors to avoid double-composition\n            if self._X._cell_selector is not None or self._X._gene_selector is not None:\n                return self._X[:, :].compute()\n            elif self._cell_selector is not None or self._gene_selector is not None:\n                cell_sel = (\n                    self._cell_selector\n                    if self._cell_selector is not None\n                    else slice(None)\n                )\n                gene_sel = (\n                    self._gene_selector\n                    if self._gene_selector is not None\n                    else slice(None)\n                )\n                return self._X[cell_sel, gene_sel].compute()\n            else:\n                return self._X[:, :].compute()\n        else:\n            raise NotImplementedError(\n                \"Dask backend not yet implemented for get_expression_data\"\n            )\n\n    def compute(self) -&gt; \"sc.AnnData\":\n        \"\"\"Explicitly compute and return a native AnnData object\"\"\"\n        import scanpy as sc\n\n        # Create native AnnData object (get DataFrames from views)\n        adata = sc.AnnData(\n            X=self._X.compute(),\n            obs=self.obs._get_dataframe(),\n            var=self.var._get_dataframe(),\n        )\n\n        return adata\n\n    def _update_with_normalized_data(\n        self, result_df: pl.DataFrame, target_sum: float, inplace: bool\n    ) -&gt; \"LazyAnnData | None\":\n        \"\"\"\n        Update AnnData with normalized data from fragment processing.\n\n        Args:\n            result_df: Polars DataFrame with normalized values\n            target_sum: Target sum used for normalization\n            inplace: Whether to modify in place\n\n        Returns:\n            Updated LazyAnnData or None if inplace=True\n        \"\"\"\n        if inplace:\n            # Store normalization transformation\n            if not hasattr(self, \"_transformations\"):\n                self._transformations = {}\n\n            self._transformations[\"normalize_total\"] = {\n                \"type\": \"normalize_total\",\n                \"target_sum\": target_sum,\n                \"fragment_processed\": True,\n                \"result_df\": result_df,\n            }\n\n            print(\n                f\"Applied normalize_total with target_sum={target_sum} (fragment processing)\"\n            )\n            return None\n        else:\n            # Create a copy with the transformation\n            new_adata = self.copy()\n            if not hasattr(new_adata, \"_transformations\"):\n                new_adata._transformations = {}\n\n            new_adata._transformations[\"normalize_total\"] = {\n                \"type\": \"normalize_total\",\n                \"target_sum\": target_sum,\n                \"fragment_processed\": True,\n                \"result_df\": result_df,\n            }\n\n            return new_adata\n\n    def _update_with_log1p_data(\n        self, result_df: pl.DataFrame, inplace: bool\n    ) -&gt; \"LazyAnnData | None\":\n        \"\"\"\n        Update AnnData with log1p data from fragment processing.\n\n        Args:\n            result_df: Polars DataFrame with log1p values\n            inplace: Whether to modify in place\n\n        Returns:\n            Updated LazyAnnData or None if inplace=True\n        \"\"\"\n        if inplace:\n            # Store log1p transformation\n            if not hasattr(self, \"_transformations\"):\n                self._transformations = {}\n\n            self._transformations[\"log1p\"] = {\n                \"type\": \"log1p\",\n                \"fragment_processed\": True,\n                \"result_df\": result_df,\n            }\n\n            print(\"Applied log1p transformation (fragment processing)\")\n            return None\n        else:\n            # Create a copy with the transformation\n            new_adata = self.copy()\n            if not hasattr(new_adata, \"_transformations\"):\n                new_adata._transformations = {}\n\n            new_adata._transformations[\"log1p\"] = {\n                \"type\": \"log1p\",\n                \"fragment_processed\": True,\n                \"result_df\": result_df,\n            }\n\n            return new_adata\n\n    def _get_processing_strategy(self, fragments: bool | None = None) -&gt; bool:\n        \"\"\"Determine the processing strategy based on fragments and dataset size\"\"\"\n        if fragments is not None:\n            return fragments\n        try:\n            fragments_list = self.slaf.expression.get_fragments()\n            return len(fragments_list) &gt; 1\n        except Exception:\n            return False\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData-attributes","title":"Attributes","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.layers","title":"<code>layers: LazyLayersView</code>  <code>property</code>","text":"<p>Access to layers (dictionary-like interface).</p> <p>Returns a dictionary-like view of layers that provides access to alternative representations of the expression matrix (e.g., spliced, unspliced, counts). Layers have the same dimensions as X.</p> <p>Returns:</p> Type Description <code>LazyLayersView</code> <p>LazyLayersView providing dictionary-like access to layers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access a layer\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; spliced = adata.layers[\"spliced\"]\n&gt;&gt;&gt; print(f\"Layer shape: {spliced.shape}\")\nLayer shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # List available layers\n&gt;&gt;&gt; print(list(adata.layers.keys()))\n['spliced', 'unspliced']\n</code></pre> <pre><code>&gt;&gt;&gt; # Check if layer exists\n&gt;&gt;&gt; assert \"spliced\" in adata.layers\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.obs","title":"<code>obs: LazyObsView</code>  <code>property</code>","text":"<p>Cell metadata (observations) - mutable view with DataFrame-like and dict-like interfaces.</p> <p>Returns a view that provides both DataFrame-like and dictionary-like access to cell metadata columns with support for creating, updating, and deleting columns. This view respects cell selectors from parent LazyAnnData.</p> <p>When accessed with a string key (e.g., <code>obs[\"cluster\"]</code>), returns a pandas Series (AnnData-compatible). When accessed directly, behaves like a DataFrame.</p> <p>Returns:</p> Type Description <code>LazyObsView</code> <p>LazyObsView providing DataFrame-like and dictionary-like access to obs columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; df = adata.obs  # Returns DataFrame-like view\n&gt;&gt;&gt; cluster = adata.obs[\"cluster\"]  # Returns pd.Series (AnnData-compatible)\n&gt;&gt;&gt; print(cluster.head())\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a new column\n&gt;&gt;&gt; adata.obs[\"new_cluster\"] = new_cluster_labels\n&gt;&gt;&gt; assert \"new_cluster\" in adata.obs\n</code></pre> <pre><code>&gt;&gt;&gt; # List available columns\n&gt;&gt;&gt; print(list(adata.obs.keys()))\n['cell_id', 'total_counts', 'cluster', 'new_cluster']\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.var","title":"<code>var: LazyVarView</code>  <code>property</code>","text":"<p>Gene metadata (variables) - mutable view with DataFrame-like and dict-like interfaces.</p> <p>Returns a view that provides both DataFrame-like and dictionary-like access to gene metadata columns with support for creating, updating, and deleting columns. This view respects gene selectors from parent LazyAnnData.</p> <p>When accessed with a string key (e.g., <code>var[\"highly_variable\"]</code>), returns a pandas Series (AnnData-compatible). When accessed directly, behaves like a DataFrame.</p> <p>Returns:</p> Type Description <code>LazyVarView</code> <p>LazyVarView providing DataFrame-like and dictionary-like access to var columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # DataFrame-like access (AnnData-compatible)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; df = adata.var  # Returns DataFrame-like view\n&gt;&gt;&gt; hvg = adata.var[\"highly_variable\"]  # Returns pd.Series (AnnData-compatible)\n&gt;&gt;&gt; print(hvg.head())\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a new column\n&gt;&gt;&gt; adata.var[\"new_annotation\"] = new_annotations\n&gt;&gt;&gt; assert \"new_annotation\" in adata.var\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.obsm","title":"<code>obsm: LazyObsmView</code>  <code>property</code>","text":"<p>Multi-dimensional obs annotations (embeddings, PCA, etc.).</p> <p>Returns a dictionary-like view that provides access to multi-dimensional cell annotations stored as separate columns in cells.lance. Each key maps to a 2D numpy array.</p> <p>Returns:</p> Type Description <code>LazyObsmView</code> <p>LazyObsmView providing dictionary-like access to obsm keys.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access an embedding\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; umap = adata.obsm[\"X_umap\"]\n&gt;&gt;&gt; print(f\"UMAP shape: {umap.shape}\")\nUMAP shape: (1000, 2)\n</code></pre> <pre><code>&gt;&gt;&gt; # Create a new embedding\n&gt;&gt;&gt; adata.obsm[\"X_pca\"] = pca_coords  # shape: (1000, 50)\n&gt;&gt;&gt; assert \"X_pca\" in adata.obsm\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.varm","title":"<code>varm: LazyVarmView</code>  <code>property</code>","text":"<p>Multi-dimensional var annotations (gene loadings, etc.).</p> <p>Returns a dictionary-like view that provides access to multi-dimensional gene annotations stored as separate columns in genes.lance. Each key maps to a 2D numpy array.</p> <p>Returns:</p> Type Description <code>LazyVarmView</code> <p>LazyVarmView providing dictionary-like access to varm keys.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access gene loadings\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; pcs = adata.varm[\"PCs\"]\n&gt;&gt;&gt; print(f\"PCs shape: {pcs.shape}\")\nPCs shape: (20000, 50)\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.uns","title":"<code>uns: LazyUnsView</code>  <code>property</code>","text":"<p>Unstructured metadata (analysis parameters, etc.).</p> <p>Returns a dictionary-like view that provides access to unstructured metadata stored in uns.json. Unlike obs/var/obsm/varm, uns metadata is always mutable and stored as JSON.</p> <p>Returns:</p> Type Description <code>LazyUnsView</code> <p>LazyUnsView providing dictionary-like access to uns keys.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Store metadata\n&gt;&gt;&gt; slaf_array = SLAFArray(\"data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; adata.uns[\"neighbors\"] = {\"params\": {\"n_neighbors\": 15}}\n&gt;&gt;&gt; print(adata.uns[\"neighbors\"])\n{'params': {'n_neighbors': 15}}\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.X","title":"<code>X: LazyExpressionMatrix</code>  <code>property</code>","text":"<p>Access to expression data.</p> <p>Returns the lazy expression matrix that provides scipy.sparse-compatible access to the single-cell expression data. The matrix is lazily evaluated to avoid loading all data into memory.</p> <p>Returns:</p> Type Description <code>LazyExpressionMatrix</code> <p>LazyExpressionMatrix providing scipy.sparse-compatible interface.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Access expression data\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; print(f\"Expression matrix shape: {adata.X.shape}\")\nExpression matrix shape: (1000, 20000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Subsetting expression data\n&gt;&gt;&gt; subset_X = adata.X[:100, :5000]\n&gt;&gt;&gt; print(f\"Subset expression shape: {subset_X.shape}\")\nSubset expression shape: (100, 5000)\n</code></pre> <pre><code>&gt;&gt;&gt; # Check matrix type\n&gt;&gt;&gt; print(f\"Matrix type: {type(adata.X)}\")\nMatrix type: &lt;class 'slaf.integrations.anndata.LazyExpressionMatrix'&gt;\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.obs_deprecated","title":"<code>obs_deprecated: pd.DataFrame</code>  <code>property</code>","text":"<p>Cell metadata (observations) - DEPRECATED.</p> <p>.. deprecated:: 0.X     This property is deprecated. Use :attr:<code>obs</code> (formerly :attr:<code>obs</code>) instead.     This will be removed in a future version.</p> <p>This property triggers computation of metadata when accessed. For lazy access to metadata structure only, use obs.columns, obs.index, etc.</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.var_deprecated","title":"<code>var_deprecated: pd.DataFrame</code>  <code>property</code>","text":"<p>Gene metadata (variables) - DEPRECATED.</p> <p>.. deprecated:: 0.X     This property is deprecated. Use :attr:<code>var</code> (formerly :attr:<code>var</code>) instead.     This will be removed in a future version.</p> <p>This property triggers computation of metadata when accessed. For lazy access to metadata structure only, use var.columns, var.index, etc.</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.obs_names","title":"<code>obs_names: pd.Index</code>  <code>property</code>","text":"<p>Cell names</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.var_names","title":"<code>var_names: pd.Index</code>  <code>property</code>","text":"<p>Gene names</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.n_obs","title":"<code>n_obs: int</code>  <code>property</code>","text":"<p>Number of observations (cells)</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.n_vars","title":"<code>n_vars: int</code>  <code>property</code>","text":"<p>Number of variables (genes)</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.shape","title":"<code>shape: tuple[int, int]</code>  <code>property</code>","text":"<p>Get the shape of the data, accounting for any applied filters</p>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.__init__","title":"<code>__init__(slaf_array: SLAFArray, backend: str = 'auto')</code>","text":"<p>Initialize LazyAnnData with SLAF array.</p> <p>Parameters:</p> Name Type Description Default <code>slaf_array</code> <code>SLAFArray</code> <p>SLAFArray instance containing the single-cell data.        Used for database queries and metadata access.</p> required <code>backend</code> <code>str</code> <p>Backend for expression matrix. Currently supports \"scipy\" and \"auto\".     \"auto\" defaults to \"scipy\" for sparse matrix operations.</p> <code>'auto'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the backend is not supported.</p> <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; print(f\"Backend: {adata.backend}\")\nBackend: auto\n</code></pre> <pre><code>&gt;&gt;&gt; # With explicit backend\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array, backend=\"scipy\")\n&gt;&gt;&gt; print(f\"Backend: {adata.backend}\")\nBackend: scipy\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for unsupported backend\n&gt;&gt;&gt; try:\n...     adata = LazyAnnData(slaf_array, backend=\"unsupported\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Unknown backend: unsupported\n</code></pre> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def __init__(\n    self,\n    slaf_array: SLAFArray,\n    backend: str = \"auto\",\n):\n    \"\"\"\n    Initialize LazyAnnData with SLAF array.\n\n    Args:\n        slaf_array: SLAFArray instance containing the single-cell data.\n                   Used for database queries and metadata access.\n        backend: Backend for expression matrix. Currently supports \"scipy\" and \"auto\".\n                \"auto\" defaults to \"scipy\" for sparse matrix operations.\n\n    Raises:\n        ValueError: If the backend is not supported.\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; print(f\"Backend: {adata.backend}\")\n        Backend: auto\n\n        &gt;&gt;&gt; # With explicit backend\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array, backend=\"scipy\")\n        &gt;&gt;&gt; print(f\"Backend: {adata.backend}\")\n        Backend: scipy\n\n        &gt;&gt;&gt; # Error handling for unsupported backend\n        &gt;&gt;&gt; try:\n        ...     adata = LazyAnnData(slaf_array, backend=\"unsupported\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Unknown backend: unsupported\n    \"\"\"\n    super().__init__()\n    self.slaf = slaf_array\n    self.backend = backend\n\n    # Initialize expression matrix\n    if backend == \"scipy\" or backend == \"auto\":\n        self._X = LazyExpressionMatrix(slaf_array)\n        self._X.parent_adata = self  # Set parent reference for transformations\n    else:\n        raise ValueError(f\"Unknown backend: {backend}\")\n\n    # Lazy-loaded metadata\n    self._obs = None\n    self._var = None\n    self._cached_obs_names: pd.Index | None = None\n    self._cached_var_names: pd.Index | None = None\n\n    # Filter selectors for subsetting\n    self._cell_selector: Any = None\n    self._gene_selector: Any = None\n    self._filtered_obs: Callable[[], pd.DataFrame] | None = None\n    self._filtered_var: Callable[[], pd.DataFrame] | None = None\n\n    # Reference to parent LazyAnnData (for sliced objects)\n    self._parent_adata: LazyAnnData | None = None\n\n    # Transformations for lazy evaluation\n    self._transformations: dict[str, Any] = {}\n\n    # Layers view (lazy initialization)\n    self._layers: LazyLayersView | None = None\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.copy","title":"<code>copy() -&gt; LazyAnnData</code>","text":"<p>Create a copy</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def copy(self) -&gt; \"LazyAnnData\":\n    \"\"\"Create a copy\"\"\"\n    new_adata = LazyAnnData(self.slaf, backend=self.backend)\n\n    # Copy selectors\n    new_adata._cell_selector = self._cell_selector\n    new_adata._gene_selector = self._gene_selector\n\n    # Copy filtered metadata functions\n    new_adata._filtered_obs = self._filtered_obs\n    new_adata._filtered_var = self._filtered_var\n\n    # Copy transformations\n    new_adata._transformations = self._transformations.copy()\n\n    # Set up parent reference for the expression matrix\n    if isinstance(new_adata._X, LazyExpressionMatrix):\n        new_adata._X.parent_adata = new_adata\n\n    return new_adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.to_memory","title":"<code>to_memory() -&gt; scipy.sparse.spmatrix</code>","text":"<p>Load entire matrix into memory</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def to_memory(self) -&gt; scipy.sparse.spmatrix:\n    \"\"\"Load entire matrix into memory\"\"\"\n    return self.get_expression_data()\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.write","title":"<code>write(filename: str)</code>","text":"<p>Write to h5ad format (would need implementation)</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def write(self, filename: str):\n    \"\"\"Write to h5ad format (would need implementation)\"\"\"\n    raise NotImplementedError(\"Writing LazyAnnData not yet implemented\")\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.get_expression_data","title":"<code>get_expression_data() -&gt; scipy.sparse.csr_matrix</code>","text":"<p>Get expression data with any applied filters</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def get_expression_data(self) -&gt; scipy.sparse.csr_matrix:\n    \"\"\"Get expression data with any applied filters\"\"\"\n    if isinstance(self._X, LazyExpressionMatrix):\n        # If the LazyExpressionMatrix already has selectors stored, just get the data\n        # without passing additional selectors to avoid double-composition\n        if self._X._cell_selector is not None or self._X._gene_selector is not None:\n            return self._X[:, :].compute()\n        elif self._cell_selector is not None or self._gene_selector is not None:\n            cell_sel = (\n                self._cell_selector\n                if self._cell_selector is not None\n                else slice(None)\n            )\n            gene_sel = (\n                self._gene_selector\n                if self._gene_selector is not None\n                else slice(None)\n            )\n            return self._X[cell_sel, gene_sel].compute()\n        else:\n            return self._X[:, :].compute()\n    else:\n        raise NotImplementedError(\n            \"Dask backend not yet implemented for get_expression_data\"\n        )\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.LazyAnnData.compute","title":"<code>compute() -&gt; sc.AnnData</code>","text":"<p>Explicitly compute and return a native AnnData object</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def compute(self) -&gt; \"sc.AnnData\":\n    \"\"\"Explicitly compute and return a native AnnData object\"\"\"\n    import scanpy as sc\n\n    # Create native AnnData object (get DataFrames from views)\n    adata = sc.AnnData(\n        X=self._X.compute(),\n        obs=self.obs._get_dataframe(),\n        var=self.var._get_dataframe(),\n    )\n\n    return adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.anndata.ensure_h5ad_writable","title":"<code>ensure_h5ad_writable(adata: Any) -&gt; None</code>","text":"<p>Convert ArrowStringArray/string in obs/var (index and columns) to object dtype so anndata can write to HDF5. Modifies adata in place; only touches string-like index/columns to avoid extra memory use.</p> <p>Call this before adata.write_h5ad() or adata.write() when obs/var use pandas string or ArrowStringArray dtypes (e.g. from polars .to_pandas() or pandas 2.x).</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def ensure_h5ad_writable(adata: Any) -&gt; None:\n    \"\"\"\n    Convert ArrowStringArray/string in obs/var (index and columns) to object dtype\n    so anndata can write to HDF5. Modifies adata in place; only touches string-like\n    index/columns to avoid extra memory use.\n\n    Call this before adata.write_h5ad() or adata.write() when obs/var use pandas\n    string or ArrowStringArray dtypes (e.g. from polars .to_pandas() or pandas 2.x).\n    \"\"\"\n    for attr in (\"obs\", \"var\"):\n        df = getattr(adata, attr, None)\n        if df is None:\n            continue\n        try:\n            vals = df.index.values\n            if _is_arrow_string_array(vals):\n                df.index = df.index.astype(object)\n        except Exception:\n            pass\n        for col in list(df.columns):\n            try:\n                ser = df[col]\n                vals = ser.values\n                if _is_arrow_string_array(vals):\n                    df[col] = ser.astype(object)\n                elif hasattr(ser, \"cat\") and ser.dtype.name == \"category\":\n                    # Categorical: ensure categories are HDF5-writable\n                    cats = ser.cat.categories\n                    if _is_arrow_string_array(cats.values):\n                        df[col] = ser.astype(object)\n            except Exception:\n                pass\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.anndata.read_slaf","title":"<code>read_slaf(filename: str, backend: str = 'auto') -&gt; LazyAnnData</code>","text":"<p>Read SLAF file as LazyAnnData object</p> Source code in <code>slaf/integrations/anndata.py</code> <pre><code>def read_slaf(filename: str, backend: str = \"auto\") -&gt; LazyAnnData:\n    \"\"\"Read SLAF file as LazyAnnData object\"\"\"\n    slaf_array = SLAFArray(filename)\n    return LazyAnnData(slaf_array, backend=backend)\n</code></pre>"},{"location":"api/integrations/#scanpy-integration","title":"Scanpy Integration","text":""},{"location":"api/integrations/#slaf.integrations.scanpy","title":"<code>slaf.integrations.scanpy</code>","text":""},{"location":"api/integrations/#slaf.integrations.scanpy-classes","title":"Classes","text":""},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing","title":"<code>LazyPreprocessing</code>","text":"<p>Scanpy preprocessing functions with lazy evaluation</p> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>class LazyPreprocessing:\n    \"\"\"Scanpy preprocessing functions with lazy evaluation\"\"\"\n\n    @staticmethod\n    def calculate_qc_metrics(\n        adata: LazyAnnData,\n        percent_top: int | list | None = None,\n        log1p: bool = True,\n        inplace: bool = True,\n    ) -&gt; tuple | None:\n        \"\"\"\n        Calculate quality control metrics for cells and genes using lazy evaluation.\n\n        This function computes cell and gene-level QC metrics using SQL aggregation\n        for memory efficiency. It calculates metrics like total counts, number of\n        genes per cell, and number of cells per gene.\n\n        Args:\n            adata: LazyAnnData instance containing the single-cell data.\n            percent_top: Number of top genes to consider for percent_top calculation.\n                        Currently not implemented in lazy version.\n            log1p: Whether to compute log1p-transformed versions of count metrics.\n                   Adds log1p_total_counts and log1p_n_genes_by_counts to cell metrics,\n                   and log1p_total_counts and log1p_n_cells_by_counts to gene metrics.\n            inplace: Whether to modify the adata object in place. Currently not fully\n                    implemented in lazy version - returns None when True.\n\n        Returns:\n            tuple | None: If inplace=False, returns (cell_qc, gene_qc) where:\n                - cell_qc: DataFrame with cell-level QC metrics\n                - gene_qc: DataFrame with gene-level QC metrics\n                If inplace=True, returns None.\n\n        Raises:\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Calculate QC metrics\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; cell_qc, gene_qc = LazyPreprocessing.calculate_qc_metrics(\n            ...     adata, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Cell QC shape: {cell_qc.shape}\")\n            Cell QC shape: (1000, 4)\n            &gt;&gt;&gt; print(f\"Gene QC shape: {gene_qc.shape}\")\n            Gene QC shape: (20000, 4)\n\n            &gt;&gt;&gt; # With log1p transformation\n            &gt;&gt;&gt; cell_qc, gene_qc = LazyPreprocessing.calculate_qc_metrics(\n            ...     adata, log1p=True, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Cell QC columns: {list(cell_qc.columns)}\")\n            Cell QC columns: ['cell_id', 'n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'log1p_n_genes_by_counts']\n        \"\"\"\n\n        # Calculate cell-level metrics via SQL using simple aggregation (no JOINs)\n        cell_qc_sql = \"\"\"\n        SELECT\n            e.cell_integer_id,\n            COUNT(DISTINCT e.gene_integer_id) as n_genes_by_counts,\n            SUM(e.value) as total_counts\n        FROM expression e\n        GROUP BY e.cell_integer_id\n        ORDER BY e.cell_integer_id\n        \"\"\"\n\n        cell_qc = adata.slaf.query(cell_qc_sql)\n\n        # Work with polars DataFrame internally\n        cell_qc_pl = cell_qc\n\n        # Map cell_integer_id to cell_id for scanpy compatibility\n        if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n            # Create mapping from cell_integer_id to cell names\n            # Use polars DataFrame to get cell names\n            obs_df = adata.slaf.obs\n            if \"cell_id\" in obs_df.columns:\n                cell_id_to_name = dict(\n                    zip(\n                        obs_df[\"cell_integer_id\"].to_list(),\n                        obs_df[\"cell_id\"].to_list(),\n                        strict=False,\n                    )\n                )\n            else:\n                # Fallback: create cell names from integer IDs\n                cell_id_to_name = {i: f\"cell_{i}\" for i in range(len(obs_df))}\n            # Use vectorized polars operations for mapping\n            # Create mapping DataFrame\n            cell_map_df = pl.DataFrame(\n                {\n                    \"cell_integer_id\": list(cell_id_to_name.keys()),\n                    \"cell_id\": list(cell_id_to_name.values()),\n                }\n            )\n            # Join with mapping DataFrame\n            cell_qc_pl = cell_qc_pl.join(cell_map_df, on=\"cell_integer_id\", how=\"left\")\n            # Fill any missing values with default format\n            cell_qc_pl = cell_qc_pl.with_columns(\n                pl.col(\"cell_id\").fill_null(\n                    pl.col(\"cell_integer_id\")\n                    .cast(pl.Utf8)\n                    .map_elements(lambda x: f\"cell_{x}\", return_dtype=pl.Utf8)\n                )\n            )\n        else:\n            # Fallback: use cell_integer_id as cell_id\n            cell_qc_pl = cell_qc_pl.with_columns(\n                pl.col(\"cell_integer_id\").cast(pl.Utf8).alias(\"cell_id\")\n            )\n\n        # Add log1p transformed counts if requested\n        if log1p:\n            cell_qc_pl = cell_qc_pl.with_columns(\n                [\n                    pl.col(\"total_counts\").log1p().alias(\"log1p_total_counts\"),\n                    pl.col(\"n_genes_by_counts\")\n                    .log1p()\n                    .alias(\"log1p_n_genes_by_counts\"),\n                ]\n            )\n\n        # Convert to pandas for return compatibility\n        cell_qc = cell_qc_pl.to_pandas()\n\n        # Calculate gene-level metrics via SQL using simple aggregation (no JOINs)\n        gene_qc_sql = \"\"\"\n        SELECT\n            e.gene_integer_id,\n            COUNT(DISTINCT e.cell_integer_id) AS n_cells_by_counts,\n            SUM(e.value) AS total_counts\n        FROM expression e\n        GROUP BY e.gene_integer_id\n        ORDER BY e.gene_integer_id\n        \"\"\"\n\n        gene_qc = adata.slaf.query(gene_qc_sql)\n\n        # Work with polars DataFrames internally\n        gene_qc_pl = gene_qc\n\n        # For scanpy compatibility, we need to ensure all genes are present\n        # Use in-memory var if available, otherwise fall back to SQL\n        if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n            # Use the materialized var metadata directly\n            expected_genes_pl = pl.DataFrame(\n                {\"gene_integer_id\": range(len(adata.slaf.var))}\n            )\n        else:\n            expected_genes_sql = \"\"\"\n            SELECT gene_integer_id\n            FROM genes\n            ORDER BY gene_integer_id\n            \"\"\"\n            expected_genes = adata.slaf.query(expected_genes_sql)\n            expected_genes_pl = expected_genes\n\n        # Create a complete gene_qc DataFrame with all expected genes using polars\n        gene_qc_complete_pl = expected_genes_pl.join(\n            gene_qc_pl, on=\"gene_integer_id\", how=\"left\"\n        ).fill_null(0)\n\n        # Ensure proper data types\n        gene_qc_complete_pl = gene_qc_complete_pl.with_columns(\n            [\n                pl.col(\"n_cells_by_counts\").cast(pl.Int64),\n                pl.col(\"total_counts\").cast(pl.Float64),\n            ]\n        )\n\n        # Map gene_integer_id to gene names for scanpy compatibility\n        if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n            # Create mapping from gene_integer_id to gene names\n            # Use polars DataFrame to get gene names\n            var_df = adata.slaf.var\n            if \"gene_id\" in var_df.columns:\n                gene_id_to_name = dict(\n                    zip(\n                        var_df[\"gene_integer_id\"].to_list(),\n                        var_df[\"gene_id\"].to_list(),\n                        strict=False,\n                    )\n                )\n            else:\n                # Fallback: create gene names from integer IDs\n                gene_id_to_name = {i: f\"gene_{i}\" for i in range(len(var_df))}\n            # Use vectorized polars operations for mapping\n            # Create mapping DataFrame\n            gene_map_df = pl.DataFrame(\n                {\n                    \"gene_integer_id\": list(gene_id_to_name.keys()),\n                    \"gene_id\": list(gene_id_to_name.values()),\n                }\n            )\n            # Join with mapping DataFrame\n            gene_qc_complete_pl = gene_qc_complete_pl.join(\n                gene_map_df, on=\"gene_integer_id\", how=\"left\"\n            )\n            # Fill any missing values with default format\n            gene_qc_complete_pl = gene_qc_complete_pl.with_columns(\n                pl.col(\"gene_id\").fill_null(\n                    pl.col(\"gene_integer_id\")\n                    .cast(pl.Utf8)\n                    .map_elements(lambda x: f\"gene_{x}\", return_dtype=pl.Utf8)\n                )\n            )\n        else:\n            # Fallback: use gene_integer_id as gene_id\n            gene_qc_complete_pl = gene_qc_complete_pl.with_columns(\n                pl.col(\"gene_integer_id\").cast(pl.Utf8).alias(\"gene_id\")\n            )\n\n        # Convert to pandas and set gene_id as index\n        gene_qc_complete = gene_qc_complete_pl.to_pandas().set_index(\"gene_id\")\n\n        if log1p:\n            gene_qc_complete[\"log1p_total_counts\"] = np.log1p(\n                gene_qc_complete[\"total_counts\"]\n            )\n            gene_qc_complete[\"log1p_n_cells_by_counts\"] = np.log1p(\n                gene_qc_complete[\"n_cells_by_counts\"]\n            )\n\n        if inplace:\n            # Update the metadata tables in SLAF\n            # This would require implementing metadata updates in SLAF\n            # For now, just update the cached obs/var\n\n            # Update obs\n            adata._obs = None  # Clear cache\n            for _ in cell_qc.iterrows():\n                # Would need to implement metadata updates\n                pass\n\n            # Update var\n            adata._var = None  # Clear cache\n            for _ in gene_qc_complete.iterrows():\n                # Would need to implement metadata updates\n                pass\n\n            return None\n        else:\n            return cell_qc, gene_qc_complete\n\n    @staticmethod\n    def filter_cells(\n        adata: LazyAnnData,\n        min_counts: int | None = None,\n        min_genes: int | None = None,\n        max_counts: int | None = None,\n        max_genes: int | None = None,\n        inplace: bool = True,\n    ) -&gt; LazyAnnData | None:\n        \"\"\"\n        Filter cells based on quality control metrics using lazy evaluation.\n\n        This function filters cells based on their total counts and number of genes\n        using SQL aggregation for memory efficiency. It supports both minimum and\n        maximum thresholds for each metric.\n\n        Args:\n            adata: LazyAnnData instance containing the single-cell data.\n            min_counts: Minimum total counts per cell. Cells with fewer counts are filtered.\n            min_genes: Minimum number of genes per cell. Cells with fewer genes are filtered.\n            max_counts: Maximum total counts per cell. Cells with more counts are filtered.\n            max_genes: Maximum number of genes per cell. Cells with more genes are filtered.\n            inplace: Whether to modify the adata object in place. Currently not fully\n                    implemented in lazy version - returns None when True.\n\n        Returns:\n            LazyAnnData | None: If inplace=False, returns filtered LazyAnnData.\n                               If inplace=True, returns None.\n\n        Raises:\n            ValueError: If all cells are filtered out by the criteria.\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Filter cells with basic criteria\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; filtered = LazyPreprocessing.filter_cells(\n            ...     adata, min_counts=100, min_genes=50, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Original cells: {adata.n_obs}\")\n            Original cells: 1000\n            &gt;&gt;&gt; print(f\"Filtered cells: {filtered.n_obs}\")\n            Filtered cells: 850\n\n            &gt;&gt;&gt; # Filter with maximum thresholds\n            &gt;&gt;&gt; filtered = LazyPreprocessing.filter_cells(\n            ...     adata, max_counts=10000, max_genes=5000, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Cells after max filtering: {filtered.n_obs}\")\n            Cells after max filtering: 920\n\n            &gt;&gt;&gt; # Error when all cells filtered out\n            &gt;&gt;&gt; try:\n            ...     LazyPreprocessing.filter_cells(adata, min_counts=1000000)\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: All cells were filtered out\n        \"\"\"\n\n        # Build filter conditions\n        conditions = []\n\n        if min_counts is not None:\n            conditions.append(f\"cell_stats.total_counts &gt;= {min_counts}\")\n        if max_counts is not None:\n            conditions.append(f\"cell_stats.total_counts &lt;= {max_counts}\")\n        if min_genes is not None:\n            conditions.append(f\"cell_stats.n_genes_by_counts &gt;= {min_genes}\")\n        if max_genes is not None:\n            conditions.append(f\"cell_stats.n_genes_by_counts &lt;= {max_genes}\")\n\n        if not conditions:\n            return adata if not inplace else None\n\n        where_clause = \" AND \".join(conditions)\n\n        # Get filtered cell IDs using simple aggregation (no JOINs)\n        filter_sql = f\"\"\"\n        SELECT cell_stats.cell_integer_id\n        FROM (\n            SELECT\n                e.cell_integer_id,\n                COUNT(DISTINCT e.gene_integer_id) as n_genes_by_counts,\n                SUM(e.value) as total_counts\n            FROM expression e\n            GROUP BY e.cell_integer_id\n        ) cell_stats\n        WHERE ({where_clause})\n        ORDER BY cell_stats.cell_integer_id\n        \"\"\"\n\n        filtered_cells = adata.slaf.query(filter_sql)\n\n        if len(filtered_cells) == 0:\n            raise ValueError(\"All cells were filtered out\")\n\n        # Create boolean mask from the filtered cell IDs\n        # Use the materialized obs metadata to map cell_integer_id to cell names\n        if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n            # Get all cell integer IDs that pass the filter\n            filtered_cell_ids = set(filtered_cells[\"cell_integer_id\"].to_list())\n\n            # Create boolean mask for all cells\n            all_cell_ids = adata.slaf.obs[\"cell_integer_id\"].to_list()\n            boolean_mask = [cell_id in filtered_cell_ids for cell_id in all_cell_ids]\n\n            # Actually subset the LazyAnnData object (like scanpy does)\n            if inplace:\n                # Set up the filtered obs function to apply the boolean mask\n                def filtered_obs() -&gt; pd.DataFrame:\n                    obs_df = adata.slaf.obs\n                    if obs_df is not None:\n                        # Work with polars DataFrame internally\n                        obs_pl = obs_df\n                        # Drop cell_integer_id column if present\n                        if \"cell_integer_id\" in obs_pl.columns:\n                            obs_pl = obs_pl.drop(\"cell_integer_id\")\n                        # Apply boolean mask\n                        obs_pl = obs_pl.filter(pl.Series(boolean_mask))\n                        # Convert to pandas DataFrame for AnnData compatibility\n                        obs_copy = obs_pl.to_pandas()\n                        # Set cell_id as index if present\n                        if \"cell_id\" in obs_copy.columns:\n                            obs_copy = obs_copy.set_index(\"cell_id\")\n                        # Set index name to match AnnData format\n                        if hasattr(obs_copy, \"index\"):\n                            obs_copy.index.name = \"cell_id\"\n                        return obs_copy\n                    return pd.DataFrame()\n\n                # Set the filtered functions\n                adata._filtered_obs = filtered_obs\n                adata._filtered_var = None  # No gene filtering\n                adata._cell_selector = boolean_mask\n                adata._gene_selector = None\n\n                # Clear cached names to force recalculation\n                adata._cached_obs_names = None\n                adata._cached_var_names = None\n\n                return None\n            else:\n                # Create a new filtered LazyAnnData\n                filtered_adata = LazyAnnData(adata.slaf, backend=adata.backend)\n\n                # Set up the filtered obs function\n                def filtered_obs() -&gt; pd.DataFrame:\n                    obs_df = adata.slaf.obs\n                    if obs_df is not None:\n                        # Work with polars DataFrame internally\n                        obs_pl = obs_df\n                        # Drop cell_integer_id column if present\n                        if \"cell_integer_id\" in obs_pl.columns:\n                            obs_pl = obs_pl.drop(\"cell_integer_id\")\n                        # Apply boolean mask\n                        obs_pl = obs_pl.filter(pl.Series(boolean_mask))\n                        # Convert to pandas DataFrame for AnnData compatibility\n                        obs_copy = obs_pl.to_pandas()\n                        # Set cell_id as index if present\n                        if \"cell_id\" in obs_copy.columns:\n                            obs_copy = obs_copy.set_index(\"cell_id\")\n                        # Set index name to match AnnData format\n                        if hasattr(obs_copy, \"index\"):\n                            obs_copy.index.name = \"cell_id\"\n                        return obs_copy\n                    return pd.DataFrame()\n\n                # Set the filtered functions\n                filtered_adata._filtered_obs = filtered_obs\n                filtered_adata._filtered_var = None  # No gene filtering\n                filtered_adata._cell_selector = boolean_mask\n                filtered_adata._gene_selector = None\n\n                return filtered_adata\n        else:\n            # Fallback: create boolean mask by mapping integer IDs to cell names\n            # obs_names contains string cell IDs, so we need to map integer IDs to cell names\n            if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n                # Create mapping from cell_integer_id to cell_id\n                obs_df = adata.slaf.obs\n                if \"cell_id\" in obs_df.columns and \"cell_integer_id\" in obs_df.columns:\n                    id_to_name = dict(\n                        zip(\n                            obs_df[\"cell_integer_id\"].to_list(),\n                            obs_df[\"cell_id\"].to_list(),\n                            strict=False,\n                        )\n                    )\n                    # Map filtered integer IDs to cell names\n                    filtered_cell_names = {\n                        id_to_name.get(cid)\n                        for cid in filtered_cells[\"cell_integer_id\"].to_list()\n                        if id_to_name.get(cid) is not None\n                    }\n                    # Create boolean mask\n                    cell_mask = adata.obs_names.isin(filtered_cell_names)\n                else:\n                    # Fallback: use integer IDs directly (assume obs_names are integer strings)\n                    filtered_cell_ids = {\n                        str(cid) for cid in filtered_cells[\"cell_integer_id\"].to_list()\n                    }\n                    cell_mask = adata.obs_names.astype(str).isin(filtered_cell_ids)\n            else:\n                # Last resort: assume obs_names match integer IDs as strings\n                filtered_cell_ids = {\n                    str(cid) for cid in filtered_cells[\"cell_integer_id\"].to_list()\n                }\n                cell_mask = adata.obs_names.astype(str).isin(filtered_cell_ids)\n\n        if inplace:\n            # Apply filter to adata (would need proper implementation)\n            # For now, just return the original adata\n            print(\n                f\"Filtered out {np.sum(~cell_mask)} cells, {np.sum(cell_mask)} remaining\"\n            )\n            return None\n        else:\n            # Create new filtered LazyAnnData\n            filtered_adata = adata.copy()\n            # Apply mask (would need proper implementation)\n            return filtered_adata\n\n    @staticmethod\n    def filter_genes(\n        adata: LazyAnnData,\n        min_counts: int | None = None,\n        min_cells: int | None = None,\n        max_counts: int | None = None,\n        max_cells: int | None = None,\n        inplace: bool = True,\n    ) -&gt; LazyAnnData | None:\n        \"\"\"\n        Filter genes based on quality control metrics using lazy evaluation.\n\n        This function filters genes based on their total counts and number of cells\n        using SQL aggregation for memory efficiency. It supports both minimum and\n        maximum thresholds for each metric.\n\n        Args:\n            adata: LazyAnnData instance containing the single-cell data.\n            min_counts: Minimum total counts per gene. Genes with fewer counts are filtered.\n            min_cells: Minimum number of cells per gene. Genes expressed in fewer cells are filtered.\n            max_counts: Maximum total counts per gene. Genes with more counts are filtered.\n            max_cells: Maximum number of cells per gene. Genes expressed in more cells are filtered.\n            inplace: Whether to modify the adata object in place. Currently not fully\n                    implemented in lazy version - returns None when True.\n\n        Returns:\n            LazyAnnData | None: If inplace=False, returns filtered LazyAnnData.\n                               If inplace=True, returns None.\n\n        Raises:\n            ValueError: If all genes are filtered out by the criteria.\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Filter genes with basic criteria\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; filtered = LazyPreprocessing.filter_genes(\n            ...     adata, min_counts=10, min_cells=5, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Original genes: {adata.n_vars}\")\n            Original genes: 20000\n            &gt;&gt;&gt; print(f\"Filtered genes: {filtered.n_vars}\")\n            Filtered genes: 15000\n\n            &gt;&gt;&gt; # Filter with maximum thresholds\n            &gt;&gt;&gt; filtered = LazyPreprocessing.filter_genes(\n            ...     adata, max_counts=100000, max_cells=1000, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Genes after max filtering: {filtered.n_vars}\")\n            Genes after max filtering: 18000\n\n            &gt;&gt;&gt; # Error when all genes filtered out\n            &gt;&gt;&gt; try:\n            ...     LazyPreprocessing.filter_genes(adata, min_counts=1000000)\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: All genes were filtered out\n        \"\"\"\n\n        # Build filter conditions for genes\n        conditions = []\n\n        if min_counts is not None:\n            conditions.append(f\"gene_stats.total_counts &gt;= {min_counts}\")\n        if max_counts is not None:\n            conditions.append(f\"gene_stats.total_counts &lt;= {max_counts}\")\n        if min_cells is not None:\n            conditions.append(f\"gene_stats.n_cells_by_counts &gt;= {min_cells}\")\n        if max_cells is not None:\n            conditions.append(f\"gene_stats.n_cells_by_counts &lt;= {max_cells}\")\n\n        if not conditions:\n            return adata if not inplace else None\n\n        where_clause = \" AND \".join(conditions)\n\n        # Get filtered gene IDs using simple aggregation (no JOINs)\n        filter_sql = f\"\"\"\n        SELECT gene_stats.gene_integer_id\n        FROM (\n            SELECT\n                e.gene_integer_id,\n                COUNT(DISTINCT e.cell_integer_id) AS n_cells_by_counts,\n                SUM(e.value) AS total_counts\n            FROM expression e\n            GROUP BY e.gene_integer_id\n        ) gene_stats\n        WHERE {where_clause}\n        ORDER BY gene_stats.gene_integer_id\n        \"\"\"\n\n        filtered_genes = adata.slaf.query(filter_sql)\n\n        if len(filtered_genes) == 0:\n            raise ValueError(\"All genes were filtered out\")\n\n        # Create boolean mask from the filtered gene IDs\n        # Use the materialized var metadata to map gene_integer_id to gene names\n        if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n            # Create a mapping from gene_integer_id to gene names\n            # Use polars DataFrame to get gene names\n            var_df = adata.slaf.var\n            if \"gene_id\" in var_df.columns:\n                gene_id_to_name = dict(\n                    zip(\n                        var_df[\"gene_integer_id\"].to_list(),\n                        var_df[\"gene_id\"].to_list(),\n                        strict=False,\n                    )\n                )\n            else:\n                # Fallback: create gene names from integer IDs\n                gene_id_to_name = {i: f\"gene_{i}\" for i in range(len(var_df))}\n            filtered_gene_names = [\n                gene_id_to_name.get(gid, f\"gene_{gid}\")\n                for gid in filtered_genes[\"gene_integer_id\"]\n            ]\n            gene_mask = adata.var_names.isin(filtered_gene_names)\n        else:\n            # Fallback to using gene_integer_id directly\n            gene_mask = adata.var_names.isin(filtered_genes[\"gene_integer_id\"])\n\n        if inplace:\n            # Apply filter to adata (would need proper implementation)\n            print(\n                f\"Filtered out {np.sum(~gene_mask)} genes, {np.sum(gene_mask)} remaining\"\n            )\n            return None\n        else:\n            # Create new filtered LazyAnnData\n            filtered_adata = adata.copy()\n            # Apply mask (would need proper implementation)\n            return filtered_adata\n\n    @staticmethod\n    def normalize_total(\n        adata: LazyAnnData,\n        target_sum: float | None = 1e4,\n        inplace: bool = True,\n        fragments: bool | None = None,\n    ) -&gt; LazyAnnData | None:\n        \"\"\"\n        Normalize counts per cell to target sum using lazy evaluation.\n\n        This function normalizes the expression data so that each cell has a total\n        count equal to target_sum. The normalization is applied lazily and stored\n        as a transformation that will be computed when the data is accessed.\n\n        Args:\n            adata: LazyAnnData instance containing the single-cell data.\n            target_sum: Target sum for normalization. Default is 10,000.\n            inplace: Whether to modify the adata object in place. If False, returns\n                    a copy with the transformation applied.\n            fragments: Whether to use fragment-based processing. If None, automatically\n                      selects based on dataset characteristics.\n\n        Returns:\n            LazyAnnData | None: If inplace=False, returns LazyAnnData with transformation.\n                               If inplace=True, returns None.\n\n        Raises:\n            ValueError: If target_sum is not positive.\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Basic normalization\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; LazyPreprocessing.normalize_total(adata, target_sum=10000)\n            Applied normalize_total with target_sum=10000\n\n            &gt;&gt;&gt; # Custom target sum\n            &gt;&gt;&gt; adata_copy = adata.copy()\n            &gt;&gt;&gt; LazyPreprocessing.normalize_total(\n            ...     adata_copy, target_sum=5000, inplace=False\n            ... )\n            &gt;&gt;&gt; print(\"Normalization applied to copy\")\n\n            &gt;&gt;&gt; # Error with invalid target_sum\n            &gt;&gt;&gt; try:\n            ...     LazyPreprocessing.normalize_total(adata, target_sum=0)\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: target_sum must be positive\n        \"\"\"\n        if target_sum is None:\n            target_sum = 1e4\n\n        # Validate target_sum\n        if target_sum &lt;= 0:\n            raise ValueError(\"target_sum must be positive\")\n\n        # Determine processing strategy\n        if fragments is not None:\n            use_fragments = fragments\n        else:\n            # Check if dataset has multiple fragments\n            try:\n                fragments_list = adata.slaf.expression.get_fragments()\n                use_fragments = len(fragments_list) &gt; 1\n            except Exception:\n                use_fragments = False\n\n        if use_fragments:\n            # Use fragment-based processing\n            try:\n                from slaf.core.fragment_processor import FragmentProcessor\n\n                # Get selectors from the LazyAnnData if it's sliced\n                cell_selector = getattr(adata, \"_cell_selector\", None)\n                gene_selector = getattr(adata, \"_gene_selector\", None)\n\n                processor = FragmentProcessor(\n                    adata.slaf,\n                    cell_selector=cell_selector,\n                    gene_selector=gene_selector,\n                    max_workers=4,\n                    enable_caching=True,\n                )\n                # Use smart strategy selection for optimal performance\n                lazy_pipeline = processor.build_lazy_pipeline_smart(\n                    \"normalize_total\", target_sum=target_sum\n                )\n                result_df = processor.compute(lazy_pipeline)\n\n                # Update adata with normalized values\n                return adata._update_with_normalized_data(\n                    result_df, target_sum, inplace\n                )\n\n            except Exception as e:\n                print(\n                    f\"Fragment processing failed, falling back to global processing: {e}\"\n                )\n                # Fall back to global processing\n                use_fragments = False\n\n        if not use_fragments:\n            # Use global processing (original implementation)\n            # Get cell totals for normalization using only the expression table\n            cell_totals_sql = \"\"\"\n            SELECT\n                e.cell_integer_id,\n                SUM(e.value) as total_counts\n            FROM expression e\n            GROUP BY e.cell_integer_id\n            ORDER BY e.cell_integer_id\n            \"\"\"\n\n            cell_totals = adata.slaf.query(cell_totals_sql)\n\n        # Work with polars DataFrame internally\n        cell_totals_pl = cell_totals\n\n        # Create normalization factors using polars\n        # Map cell_integer_id to cell names for compatibility with anndata.py\n        if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n            # Create mapping from cell_integer_id to cell names\n            # Use polars DataFrame to get cell names\n            obs_df = adata.slaf.obs\n            if \"cell_id\" in obs_df.columns:\n                cell_id_to_name = dict(\n                    zip(\n                        obs_df[\"cell_integer_id\"].to_list(),\n                        obs_df[\"cell_id\"].to_list(),\n                        strict=False,\n                    )\n                )\n            else:\n                # Fallback: create cell names from integer IDs\n                cell_id_to_name = {i: f\"cell_{i}\" for i in range(len(obs_df))}\n            # Use vectorized polars operations for mapping\n            # Create mapping DataFrame\n            cell_map_df = pl.DataFrame(\n                {\n                    \"cell_integer_id\": list(cell_id_to_name.keys()),\n                    \"cell_id\": list(cell_id_to_name.values()),\n                }\n            )\n            # Join with mapping DataFrame\n            cell_totals_pl = cell_totals_pl.join(\n                cell_map_df, on=\"cell_integer_id\", how=\"left\"\n            )\n            # Fill any missing values with default format\n            cell_totals_pl = cell_totals_pl.with_columns(\n                [\n                    pl.col(\"cell_id\").fill_null(\n                        pl.col(\"cell_integer_id\")\n                        .cast(pl.Utf8)\n                        .map_elements(lambda x: f\"cell_{x}\", return_dtype=pl.Utf8)\n                    ),\n                    (target_sum / pl.col(\"total_counts\")).alias(\"normalization_factor\"),\n                ]\n            )\n            # Convert to dictionary for compatibility\n            normalization_dict = dict(\n                zip(\n                    cell_totals_pl[\"cell_id\"].to_list(),\n                    cell_totals_pl[\"normalization_factor\"].to_list(),\n                    strict=False,\n                )\n            )\n        else:\n            # Fallback: use cell_integer_id as string keys\n            cell_totals_pl = cell_totals_pl.with_columns(\n                [\n                    pl.col(\"cell_integer_id\").cast(pl.Utf8).alias(\"cell_id\"),\n                    (target_sum / pl.col(\"total_counts\")).alias(\"normalization_factor\"),\n                ]\n            )\n            # Convert to dictionary for compatibility\n            normalization_dict = dict(\n                zip(\n                    cell_totals_pl[\"cell_id\"].to_list(),\n                    cell_totals_pl[\"normalization_factor\"].to_list(),\n                    strict=False,\n                )\n            )\n\n        if inplace:\n            # Store normalization factors for lazy application\n            if not hasattr(adata, \"_transformations\"):\n                adata._transformations = {}\n\n            adata._transformations[\"normalize_total\"] = {\n                \"type\": \"normalize_total\",\n                \"target_sum\": float(\n                    f\"{target_sum:.10f}\"\n                ),  # Convert to regular decimal format\n                \"cell_factors\": normalization_dict,\n            }\n\n            print(f\"Applied normalize_total with target_sum={target_sum}\")\n            return None\n        else:\n            # Create a copy with the transformation (copy-on-write)\n            new_adata = adata.copy()\n            if not hasattr(new_adata, \"_transformations\"):\n                new_adata._transformations = {}\n\n            new_adata._transformations[\"normalize_total\"] = {\n                \"type\": \"normalize_total\",\n                \"target_sum\": float(\n                    f\"{target_sum:.10f}\"\n                ),  # Convert to regular decimal format\n                \"cell_factors\": normalization_dict,\n            }\n\n            return new_adata\n\n    @staticmethod\n    def log1p(\n        adata: LazyAnnData, inplace: bool = True, fragments: bool | None = None\n    ) -&gt; LazyAnnData | None:\n        \"\"\"\n        Apply log1p transformation to expression data using lazy evaluation.\n\n        This function applies log(1 + x) transformation to the expression data.\n        The transformation is stored lazily and will be computed when the data\n        is accessed, avoiding memory-intensive operations on large datasets.\n\n        Args:\n            adata: LazyAnnData instance containing the single-cell data.\n            inplace: Whether to modify the adata object in place. If False, returns\n                    a copy with the transformation applied.\n\n        Returns:\n            LazyAnnData | None: If inplace=False, returns LazyAnnData with transformation.\n                               If inplace=True, returns None.\n\n        Raises:\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Apply log1p transformation\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; LazyPreprocessing.log1p(adata)\n            Applied log1p transformation\n\n            &gt;&gt;&gt; # Apply to copy\n            &gt;&gt;&gt; adata_copy = adata.copy()\n            &gt;&gt;&gt; LazyPreprocessing.log1p(adata_copy, inplace=False)\n            &gt;&gt;&gt; print(\"Log1p transformation applied to copy\")\n\n            &gt;&gt;&gt; # Check transformation was stored\n            &gt;&gt;&gt; print(\"log1p\" in adata._transformations)\n            True\n        \"\"\"\n        # Determine processing strategy\n        if fragments is not None:\n            use_fragments = fragments\n        else:\n            # Check if dataset has multiple fragments\n            try:\n                fragments_list = adata.slaf.expression.get_fragments()\n                use_fragments = len(fragments_list) &gt; 1\n            except Exception:\n                use_fragments = False\n\n        if use_fragments:\n            # Use fragment-based processing\n            try:\n                from slaf.core.fragment_processor import FragmentProcessor\n\n                # Get selectors from the LazyAnnData if it's sliced\n                cell_selector = getattr(adata, \"_cell_selector\", None)\n                gene_selector = getattr(adata, \"_gene_selector\", None)\n\n                processor = FragmentProcessor(\n                    adata.slaf,\n                    cell_selector=cell_selector,\n                    gene_selector=gene_selector,\n                    max_workers=4,\n                    enable_caching=True,\n                )\n                # Use smart strategy selection for optimal performance\n                lazy_pipeline = processor.build_lazy_pipeline_smart(\"log1p\")\n                result_df = processor.compute(lazy_pipeline)\n\n                # Update adata with log1p values\n                return adata._update_with_log1p_data(result_df, inplace)\n\n            except Exception as e:\n                print(\n                    f\"Fragment processing failed, falling back to global processing: {e}\"\n                )\n                # Fall back to global processing\n                use_fragments = False\n\n        if not use_fragments:\n            # Use global processing (original implementation)\n            if inplace:\n                # Store log1p transformation for lazy application\n                if not hasattr(adata, \"_transformations\"):\n                    adata._transformations = {}\n\n                adata._transformations[\"log1p\"] = {\"type\": \"log1p\", \"applied\": True}\n\n                print(\"Applied log1p transformation\")\n                return None\n            else:\n                # Create a copy with the transformation (copy-on-write)\n                new_adata = adata.copy()\n                if not hasattr(new_adata, \"_transformations\"):\n                    new_adata._transformations = {}\n\n                new_adata._transformations[\"log1p\"] = {\"type\": \"log1p\", \"applied\": True}\n\n                return new_adata\n\n    @staticmethod\n    def highly_variable_genes(\n        adata: LazyAnnData,\n        min_mean: float = 0.0125,\n        max_mean: float = 3,\n        min_disp: float = 0.5,\n        max_disp: float = np.inf,\n        n_top_genes: int | None = None,\n        inplace: bool = True,\n    ) -&gt; pd.DataFrame | None:\n        \"\"\"\n        Identify highly variable genes using lazy evaluation.\n\n        This function identifies genes with high cell-to-cell variation in expression\n        using SQL aggregation for memory efficiency. It calculates mean expression\n        and dispersion for each gene and applies filtering criteria.\n\n        Args:\n            adata: LazyAnnData instance containing the single-cell data.\n            min_mean: Minimum mean expression for genes to be considered.\n            max_mean: Maximum mean expression for genes to be considered.\n            min_disp: Minimum dispersion for genes to be considered.\n            max_disp: Maximum dispersion for genes to be considered.\n            n_top_genes: Number of top genes to select by dispersion.\n                        If specified, overrides min_disp and max_disp criteria.\n            inplace: Whether to modify the adata object in place. Currently not\n                    fully implemented - returns None when True.\n\n        Returns:\n            pd.DataFrame | None: If inplace=False, returns DataFrame with gene statistics\n                                and highly_variable column. If inplace=True, returns None.\n\n        Raises:\n            RuntimeError: If the SLAF array is not properly initialized.\n\n        Examples:\n            &gt;&gt;&gt; # Find highly variable genes\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n            &gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n            ...     adata, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Highly variable genes: {hvg_stats['highly_variable'].sum()}\")\n            Highly variable genes: 1500\n\n            &gt;&gt;&gt; # With custom criteria\n            &gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n            ...     adata, min_mean=0.1, max_mean=5, min_disp=1.0, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Genes meeting criteria: {hvg_stats['highly_variable'].sum()}\")\n            Genes meeting criteria: 800\n\n            &gt;&gt;&gt; # Select top N genes\n            &gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n            ...     adata, n_top_genes=1000, inplace=False\n            ... )\n            &gt;&gt;&gt; print(f\"Top genes selected: {hvg_stats['highly_variable'].sum()}\")\n            Top genes selected: 1000\n        \"\"\"\n\n        # Calculate gene statistics via SQL using simple aggregation (no JOINs)\n        stats_sql = \"\"\"\n        SELECT\n            e.gene_integer_id,\n            COUNT(DISTINCT e.cell_integer_id) AS n_cells,\n            AVG(e.value) AS mean_expr,\n            VARIANCE(e.value) AS variance,\n            CASE WHEN AVG(e.value) &gt; 0 THEN VARIANCE(e.value) / AVG(e.value) ELSE 0 END as dispersion\n        FROM expression e\n        GROUP BY e.gene_integer_id\n        ORDER BY e.gene_integer_id\n        \"\"\"\n\n        gene_stats = adata.slaf.query(stats_sql)\n\n        # Work with polars DataFrames internally\n        gene_stats_pl = gene_stats\n\n        # Get the expected gene_integer_ids from the materialized var metadata\n        # Use in-memory var if available, otherwise fall back to SQL\n        if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n            # Use the materialized var metadata directly\n            expected_genes_pl = pl.DataFrame(\n                {\"gene_integer_id\": range(len(adata.slaf.var))}\n            )\n        else:\n            # Fallback to SQL if var is not available\n            expected_genes_sql = \"\"\"\n            SELECT gene_integer_id\n            FROM genes\n            ORDER BY gene_integer_id\n            \"\"\"\n            expected_genes = adata.slaf.query(expected_genes_sql)\n            expected_genes_pl = expected_genes\n\n        # Create a complete gene_stats DataFrame with all expected genes using polars\n        gene_stats_complete_pl = expected_genes_pl.join(\n            gene_stats_pl, on=\"gene_integer_id\", how=\"left\"\n        ).fill_null(0)\n\n        # Ensure proper data types\n        gene_stats_complete_pl = gene_stats_complete_pl.with_columns(\n            [\n                pl.col(\"n_cells\").cast(pl.Int64),\n                pl.col(\"mean_expr\").cast(pl.Float64),\n                pl.col(\"variance\").cast(pl.Float64),\n                pl.col(\"dispersion\").cast(pl.Float64),\n            ]\n        )\n\n        # Map gene_integer_id to gene_id for scanpy compatibility\n        if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n            # Create mapping from gene_integer_id to gene names\n            # Use polars DataFrame to get gene names\n            var_df = adata.slaf.var\n            if \"gene_id\" in var_df.columns:\n                gene_id_to_name = dict(\n                    zip(\n                        var_df[\"gene_integer_id\"].to_list(),\n                        var_df[\"gene_id\"].to_list(),\n                        strict=False,\n                    )\n                )\n            else:\n                # Fallback: create gene names from integer IDs\n                gene_id_to_name = {i: f\"gene_{i}\" for i in range(len(var_df))}\n            # Use vectorized polars operations for mapping\n            # Create mapping DataFrame\n            gene_map_df = pl.DataFrame(\n                {\n                    \"gene_integer_id\": list(gene_id_to_name.keys()),\n                    \"gene_id\": list(gene_id_to_name.values()),\n                }\n            )\n            # Join with mapping DataFrame\n            gene_stats_complete_pl = gene_stats_complete_pl.join(\n                gene_map_df, on=\"gene_integer_id\", how=\"left\"\n            )\n            # Fill any missing values with default format\n            gene_stats_complete_pl = gene_stats_complete_pl.with_columns(\n                pl.col(\"gene_id\").fill_null(\n                    pl.col(\"gene_integer_id\")\n                    .cast(pl.Utf8)\n                    .map_elements(lambda x: f\"gene_{x}\", return_dtype=pl.Utf8)\n                )\n            )\n        else:\n            # Fallback: use gene_integer_id as gene_id\n            gene_stats_complete_pl = gene_stats_complete_pl.with_columns(\n                pl.col(\"gene_integer_id\").cast(pl.Utf8).alias(\"gene_id\")\n            )\n\n        # Convert to pandas and set gene_id as index for scanpy compatibility\n        gene_stats_complete = gene_stats_complete_pl.to_pandas().set_index(\"gene_id\")\n\n        # Apply HVG criteria\n        hvg_mask = (\n            (gene_stats_complete[\"mean_expr\"] &gt;= min_mean)\n            &amp; (gene_stats_complete[\"mean_expr\"] &lt;= max_mean)\n            &amp; (gene_stats_complete[\"dispersion\"] &gt;= min_disp)\n            &amp; (gene_stats_complete[\"dispersion\"] &lt;= max_disp)\n        )\n\n        gene_stats_complete[\"highly_variable\"] = hvg_mask\n\n        if n_top_genes is not None:\n            # Select top N genes by dispersion\n            top_genes = gene_stats_complete.nlargest(n_top_genes, \"dispersion\")\n            gene_stats_complete[\"highly_variable\"] = gene_stats_complete.index.isin(\n                top_genes.index\n            )\n\n        if inplace:\n            # Update var metadata (would need implementation)\n            print(f\"Identified {hvg_mask.sum()} highly variable genes\")\n            return None\n        else:\n            return gene_stats_complete\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing.calculate_qc_metrics","title":"<code>calculate_qc_metrics(adata: LazyAnnData, percent_top: int | list | None = None, log1p: bool = True, inplace: bool = True) -&gt; tuple | None</code>  <code>staticmethod</code>","text":"<p>Calculate quality control metrics for cells and genes using lazy evaluation.</p> <p>This function computes cell and gene-level QC metrics using SQL aggregation for memory efficiency. It calculates metrics like total counts, number of genes per cell, and number of cells per gene.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>percent_top</code> <code>int | list | None</code> <p>Number of top genes to consider for percent_top calculation.         Currently not implemented in lazy version.</p> <code>None</code> <code>log1p</code> <code>bool</code> <p>Whether to compute log1p-transformed versions of count metrics.    Adds log1p_total_counts and log1p_n_genes_by_counts to cell metrics,    and log1p_total_counts and log1p_n_cells_by_counts to gene metrics.</p> <code>True</code> <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. Currently not fully     implemented in lazy version - returns None when True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple | None</code> <p>tuple | None: If inplace=False, returns (cell_qc, gene_qc) where: - cell_qc: DataFrame with cell-level QC metrics - gene_qc: DataFrame with gene-level QC metrics If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Calculate QC metrics\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; cell_qc, gene_qc = LazyPreprocessing.calculate_qc_metrics(\n...     adata, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Cell QC shape: {cell_qc.shape}\")\nCell QC shape: (1000, 4)\n&gt;&gt;&gt; print(f\"Gene QC shape: {gene_qc.shape}\")\nGene QC shape: (20000, 4)\n</code></pre> <pre><code>&gt;&gt;&gt; # With log1p transformation\n&gt;&gt;&gt; cell_qc, gene_qc = LazyPreprocessing.calculate_qc_metrics(\n...     adata, log1p=True, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Cell QC columns: {list(cell_qc.columns)}\")\nCell QC columns: ['cell_id', 'n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'log1p_n_genes_by_counts']\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>@staticmethod\ndef calculate_qc_metrics(\n    adata: LazyAnnData,\n    percent_top: int | list | None = None,\n    log1p: bool = True,\n    inplace: bool = True,\n) -&gt; tuple | None:\n    \"\"\"\n    Calculate quality control metrics for cells and genes using lazy evaluation.\n\n    This function computes cell and gene-level QC metrics using SQL aggregation\n    for memory efficiency. It calculates metrics like total counts, number of\n    genes per cell, and number of cells per gene.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        percent_top: Number of top genes to consider for percent_top calculation.\n                    Currently not implemented in lazy version.\n        log1p: Whether to compute log1p-transformed versions of count metrics.\n               Adds log1p_total_counts and log1p_n_genes_by_counts to cell metrics,\n               and log1p_total_counts and log1p_n_cells_by_counts to gene metrics.\n        inplace: Whether to modify the adata object in place. Currently not fully\n                implemented in lazy version - returns None when True.\n\n    Returns:\n        tuple | None: If inplace=False, returns (cell_qc, gene_qc) where:\n            - cell_qc: DataFrame with cell-level QC metrics\n            - gene_qc: DataFrame with gene-level QC metrics\n            If inplace=True, returns None.\n\n    Raises:\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Calculate QC metrics\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; cell_qc, gene_qc = LazyPreprocessing.calculate_qc_metrics(\n        ...     adata, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Cell QC shape: {cell_qc.shape}\")\n        Cell QC shape: (1000, 4)\n        &gt;&gt;&gt; print(f\"Gene QC shape: {gene_qc.shape}\")\n        Gene QC shape: (20000, 4)\n\n        &gt;&gt;&gt; # With log1p transformation\n        &gt;&gt;&gt; cell_qc, gene_qc = LazyPreprocessing.calculate_qc_metrics(\n        ...     adata, log1p=True, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Cell QC columns: {list(cell_qc.columns)}\")\n        Cell QC columns: ['cell_id', 'n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'log1p_n_genes_by_counts']\n    \"\"\"\n\n    # Calculate cell-level metrics via SQL using simple aggregation (no JOINs)\n    cell_qc_sql = \"\"\"\n    SELECT\n        e.cell_integer_id,\n        COUNT(DISTINCT e.gene_integer_id) as n_genes_by_counts,\n        SUM(e.value) as total_counts\n    FROM expression e\n    GROUP BY e.cell_integer_id\n    ORDER BY e.cell_integer_id\n    \"\"\"\n\n    cell_qc = adata.slaf.query(cell_qc_sql)\n\n    # Work with polars DataFrame internally\n    cell_qc_pl = cell_qc\n\n    # Map cell_integer_id to cell_id for scanpy compatibility\n    if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n        # Create mapping from cell_integer_id to cell names\n        # Use polars DataFrame to get cell names\n        obs_df = adata.slaf.obs\n        if \"cell_id\" in obs_df.columns:\n            cell_id_to_name = dict(\n                zip(\n                    obs_df[\"cell_integer_id\"].to_list(),\n                    obs_df[\"cell_id\"].to_list(),\n                    strict=False,\n                )\n            )\n        else:\n            # Fallback: create cell names from integer IDs\n            cell_id_to_name = {i: f\"cell_{i}\" for i in range(len(obs_df))}\n        # Use vectorized polars operations for mapping\n        # Create mapping DataFrame\n        cell_map_df = pl.DataFrame(\n            {\n                \"cell_integer_id\": list(cell_id_to_name.keys()),\n                \"cell_id\": list(cell_id_to_name.values()),\n            }\n        )\n        # Join with mapping DataFrame\n        cell_qc_pl = cell_qc_pl.join(cell_map_df, on=\"cell_integer_id\", how=\"left\")\n        # Fill any missing values with default format\n        cell_qc_pl = cell_qc_pl.with_columns(\n            pl.col(\"cell_id\").fill_null(\n                pl.col(\"cell_integer_id\")\n                .cast(pl.Utf8)\n                .map_elements(lambda x: f\"cell_{x}\", return_dtype=pl.Utf8)\n            )\n        )\n    else:\n        # Fallback: use cell_integer_id as cell_id\n        cell_qc_pl = cell_qc_pl.with_columns(\n            pl.col(\"cell_integer_id\").cast(pl.Utf8).alias(\"cell_id\")\n        )\n\n    # Add log1p transformed counts if requested\n    if log1p:\n        cell_qc_pl = cell_qc_pl.with_columns(\n            [\n                pl.col(\"total_counts\").log1p().alias(\"log1p_total_counts\"),\n                pl.col(\"n_genes_by_counts\")\n                .log1p()\n                .alias(\"log1p_n_genes_by_counts\"),\n            ]\n        )\n\n    # Convert to pandas for return compatibility\n    cell_qc = cell_qc_pl.to_pandas()\n\n    # Calculate gene-level metrics via SQL using simple aggregation (no JOINs)\n    gene_qc_sql = \"\"\"\n    SELECT\n        e.gene_integer_id,\n        COUNT(DISTINCT e.cell_integer_id) AS n_cells_by_counts,\n        SUM(e.value) AS total_counts\n    FROM expression e\n    GROUP BY e.gene_integer_id\n    ORDER BY e.gene_integer_id\n    \"\"\"\n\n    gene_qc = adata.slaf.query(gene_qc_sql)\n\n    # Work with polars DataFrames internally\n    gene_qc_pl = gene_qc\n\n    # For scanpy compatibility, we need to ensure all genes are present\n    # Use in-memory var if available, otherwise fall back to SQL\n    if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n        # Use the materialized var metadata directly\n        expected_genes_pl = pl.DataFrame(\n            {\"gene_integer_id\": range(len(adata.slaf.var))}\n        )\n    else:\n        expected_genes_sql = \"\"\"\n        SELECT gene_integer_id\n        FROM genes\n        ORDER BY gene_integer_id\n        \"\"\"\n        expected_genes = adata.slaf.query(expected_genes_sql)\n        expected_genes_pl = expected_genes\n\n    # Create a complete gene_qc DataFrame with all expected genes using polars\n    gene_qc_complete_pl = expected_genes_pl.join(\n        gene_qc_pl, on=\"gene_integer_id\", how=\"left\"\n    ).fill_null(0)\n\n    # Ensure proper data types\n    gene_qc_complete_pl = gene_qc_complete_pl.with_columns(\n        [\n            pl.col(\"n_cells_by_counts\").cast(pl.Int64),\n            pl.col(\"total_counts\").cast(pl.Float64),\n        ]\n    )\n\n    # Map gene_integer_id to gene names for scanpy compatibility\n    if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n        # Create mapping from gene_integer_id to gene names\n        # Use polars DataFrame to get gene names\n        var_df = adata.slaf.var\n        if \"gene_id\" in var_df.columns:\n            gene_id_to_name = dict(\n                zip(\n                    var_df[\"gene_integer_id\"].to_list(),\n                    var_df[\"gene_id\"].to_list(),\n                    strict=False,\n                )\n            )\n        else:\n            # Fallback: create gene names from integer IDs\n            gene_id_to_name = {i: f\"gene_{i}\" for i in range(len(var_df))}\n        # Use vectorized polars operations for mapping\n        # Create mapping DataFrame\n        gene_map_df = pl.DataFrame(\n            {\n                \"gene_integer_id\": list(gene_id_to_name.keys()),\n                \"gene_id\": list(gene_id_to_name.values()),\n            }\n        )\n        # Join with mapping DataFrame\n        gene_qc_complete_pl = gene_qc_complete_pl.join(\n            gene_map_df, on=\"gene_integer_id\", how=\"left\"\n        )\n        # Fill any missing values with default format\n        gene_qc_complete_pl = gene_qc_complete_pl.with_columns(\n            pl.col(\"gene_id\").fill_null(\n                pl.col(\"gene_integer_id\")\n                .cast(pl.Utf8)\n                .map_elements(lambda x: f\"gene_{x}\", return_dtype=pl.Utf8)\n            )\n        )\n    else:\n        # Fallback: use gene_integer_id as gene_id\n        gene_qc_complete_pl = gene_qc_complete_pl.with_columns(\n            pl.col(\"gene_integer_id\").cast(pl.Utf8).alias(\"gene_id\")\n        )\n\n    # Convert to pandas and set gene_id as index\n    gene_qc_complete = gene_qc_complete_pl.to_pandas().set_index(\"gene_id\")\n\n    if log1p:\n        gene_qc_complete[\"log1p_total_counts\"] = np.log1p(\n            gene_qc_complete[\"total_counts\"]\n        )\n        gene_qc_complete[\"log1p_n_cells_by_counts\"] = np.log1p(\n            gene_qc_complete[\"n_cells_by_counts\"]\n        )\n\n    if inplace:\n        # Update the metadata tables in SLAF\n        # This would require implementing metadata updates in SLAF\n        # For now, just update the cached obs/var\n\n        # Update obs\n        adata._obs = None  # Clear cache\n        for _ in cell_qc.iterrows():\n            # Would need to implement metadata updates\n            pass\n\n        # Update var\n        adata._var = None  # Clear cache\n        for _ in gene_qc_complete.iterrows():\n            # Would need to implement metadata updates\n            pass\n\n        return None\n    else:\n        return cell_qc, gene_qc_complete\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing.filter_cells","title":"<code>filter_cells(adata: LazyAnnData, min_counts: int | None = None, min_genes: int | None = None, max_counts: int | None = None, max_genes: int | None = None, inplace: bool = True) -&gt; LazyAnnData | None</code>  <code>staticmethod</code>","text":"<p>Filter cells based on quality control metrics using lazy evaluation.</p> <p>This function filters cells based on their total counts and number of genes using SQL aggregation for memory efficiency. It supports both minimum and maximum thresholds for each metric.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>min_counts</code> <code>int | None</code> <p>Minimum total counts per cell. Cells with fewer counts are filtered.</p> <code>None</code> <code>min_genes</code> <code>int | None</code> <p>Minimum number of genes per cell. Cells with fewer genes are filtered.</p> <code>None</code> <code>max_counts</code> <code>int | None</code> <p>Maximum total counts per cell. Cells with more counts are filtered.</p> <code>None</code> <code>max_genes</code> <code>int | None</code> <p>Maximum number of genes per cell. Cells with more genes are filtered.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. Currently not fully     implemented in lazy version - returns None when True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LazyAnnData | None</code> <p>LazyAnnData | None: If inplace=False, returns filtered LazyAnnData.                If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all cells are filtered out by the criteria.</p> <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Filter cells with basic criteria\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; filtered = LazyPreprocessing.filter_cells(\n...     adata, min_counts=100, min_genes=50, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Original cells: {adata.n_obs}\")\nOriginal cells: 1000\n&gt;&gt;&gt; print(f\"Filtered cells: {filtered.n_obs}\")\nFiltered cells: 850\n</code></pre> <pre><code>&gt;&gt;&gt; # Filter with maximum thresholds\n&gt;&gt;&gt; filtered = LazyPreprocessing.filter_cells(\n...     adata, max_counts=10000, max_genes=5000, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Cells after max filtering: {filtered.n_obs}\")\nCells after max filtering: 920\n</code></pre> <pre><code>&gt;&gt;&gt; # Error when all cells filtered out\n&gt;&gt;&gt; try:\n...     LazyPreprocessing.filter_cells(adata, min_counts=1000000)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: All cells were filtered out\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>@staticmethod\ndef filter_cells(\n    adata: LazyAnnData,\n    min_counts: int | None = None,\n    min_genes: int | None = None,\n    max_counts: int | None = None,\n    max_genes: int | None = None,\n    inplace: bool = True,\n) -&gt; LazyAnnData | None:\n    \"\"\"\n    Filter cells based on quality control metrics using lazy evaluation.\n\n    This function filters cells based on their total counts and number of genes\n    using SQL aggregation for memory efficiency. It supports both minimum and\n    maximum thresholds for each metric.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        min_counts: Minimum total counts per cell. Cells with fewer counts are filtered.\n        min_genes: Minimum number of genes per cell. Cells with fewer genes are filtered.\n        max_counts: Maximum total counts per cell. Cells with more counts are filtered.\n        max_genes: Maximum number of genes per cell. Cells with more genes are filtered.\n        inplace: Whether to modify the adata object in place. Currently not fully\n                implemented in lazy version - returns None when True.\n\n    Returns:\n        LazyAnnData | None: If inplace=False, returns filtered LazyAnnData.\n                           If inplace=True, returns None.\n\n    Raises:\n        ValueError: If all cells are filtered out by the criteria.\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Filter cells with basic criteria\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; filtered = LazyPreprocessing.filter_cells(\n        ...     adata, min_counts=100, min_genes=50, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Original cells: {adata.n_obs}\")\n        Original cells: 1000\n        &gt;&gt;&gt; print(f\"Filtered cells: {filtered.n_obs}\")\n        Filtered cells: 850\n\n        &gt;&gt;&gt; # Filter with maximum thresholds\n        &gt;&gt;&gt; filtered = LazyPreprocessing.filter_cells(\n        ...     adata, max_counts=10000, max_genes=5000, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Cells after max filtering: {filtered.n_obs}\")\n        Cells after max filtering: 920\n\n        &gt;&gt;&gt; # Error when all cells filtered out\n        &gt;&gt;&gt; try:\n        ...     LazyPreprocessing.filter_cells(adata, min_counts=1000000)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: All cells were filtered out\n    \"\"\"\n\n    # Build filter conditions\n    conditions = []\n\n    if min_counts is not None:\n        conditions.append(f\"cell_stats.total_counts &gt;= {min_counts}\")\n    if max_counts is not None:\n        conditions.append(f\"cell_stats.total_counts &lt;= {max_counts}\")\n    if min_genes is not None:\n        conditions.append(f\"cell_stats.n_genes_by_counts &gt;= {min_genes}\")\n    if max_genes is not None:\n        conditions.append(f\"cell_stats.n_genes_by_counts &lt;= {max_genes}\")\n\n    if not conditions:\n        return adata if not inplace else None\n\n    where_clause = \" AND \".join(conditions)\n\n    # Get filtered cell IDs using simple aggregation (no JOINs)\n    filter_sql = f\"\"\"\n    SELECT cell_stats.cell_integer_id\n    FROM (\n        SELECT\n            e.cell_integer_id,\n            COUNT(DISTINCT e.gene_integer_id) as n_genes_by_counts,\n            SUM(e.value) as total_counts\n        FROM expression e\n        GROUP BY e.cell_integer_id\n    ) cell_stats\n    WHERE ({where_clause})\n    ORDER BY cell_stats.cell_integer_id\n    \"\"\"\n\n    filtered_cells = adata.slaf.query(filter_sql)\n\n    if len(filtered_cells) == 0:\n        raise ValueError(\"All cells were filtered out\")\n\n    # Create boolean mask from the filtered cell IDs\n    # Use the materialized obs metadata to map cell_integer_id to cell names\n    if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n        # Get all cell integer IDs that pass the filter\n        filtered_cell_ids = set(filtered_cells[\"cell_integer_id\"].to_list())\n\n        # Create boolean mask for all cells\n        all_cell_ids = adata.slaf.obs[\"cell_integer_id\"].to_list()\n        boolean_mask = [cell_id in filtered_cell_ids for cell_id in all_cell_ids]\n\n        # Actually subset the LazyAnnData object (like scanpy does)\n        if inplace:\n            # Set up the filtered obs function to apply the boolean mask\n            def filtered_obs() -&gt; pd.DataFrame:\n                obs_df = adata.slaf.obs\n                if obs_df is not None:\n                    # Work with polars DataFrame internally\n                    obs_pl = obs_df\n                    # Drop cell_integer_id column if present\n                    if \"cell_integer_id\" in obs_pl.columns:\n                        obs_pl = obs_pl.drop(\"cell_integer_id\")\n                    # Apply boolean mask\n                    obs_pl = obs_pl.filter(pl.Series(boolean_mask))\n                    # Convert to pandas DataFrame for AnnData compatibility\n                    obs_copy = obs_pl.to_pandas()\n                    # Set cell_id as index if present\n                    if \"cell_id\" in obs_copy.columns:\n                        obs_copy = obs_copy.set_index(\"cell_id\")\n                    # Set index name to match AnnData format\n                    if hasattr(obs_copy, \"index\"):\n                        obs_copy.index.name = \"cell_id\"\n                    return obs_copy\n                return pd.DataFrame()\n\n            # Set the filtered functions\n            adata._filtered_obs = filtered_obs\n            adata._filtered_var = None  # No gene filtering\n            adata._cell_selector = boolean_mask\n            adata._gene_selector = None\n\n            # Clear cached names to force recalculation\n            adata._cached_obs_names = None\n            adata._cached_var_names = None\n\n            return None\n        else:\n            # Create a new filtered LazyAnnData\n            filtered_adata = LazyAnnData(adata.slaf, backend=adata.backend)\n\n            # Set up the filtered obs function\n            def filtered_obs() -&gt; pd.DataFrame:\n                obs_df = adata.slaf.obs\n                if obs_df is not None:\n                    # Work with polars DataFrame internally\n                    obs_pl = obs_df\n                    # Drop cell_integer_id column if present\n                    if \"cell_integer_id\" in obs_pl.columns:\n                        obs_pl = obs_pl.drop(\"cell_integer_id\")\n                    # Apply boolean mask\n                    obs_pl = obs_pl.filter(pl.Series(boolean_mask))\n                    # Convert to pandas DataFrame for AnnData compatibility\n                    obs_copy = obs_pl.to_pandas()\n                    # Set cell_id as index if present\n                    if \"cell_id\" in obs_copy.columns:\n                        obs_copy = obs_copy.set_index(\"cell_id\")\n                    # Set index name to match AnnData format\n                    if hasattr(obs_copy, \"index\"):\n                        obs_copy.index.name = \"cell_id\"\n                    return obs_copy\n                return pd.DataFrame()\n\n            # Set the filtered functions\n            filtered_adata._filtered_obs = filtered_obs\n            filtered_adata._filtered_var = None  # No gene filtering\n            filtered_adata._cell_selector = boolean_mask\n            filtered_adata._gene_selector = None\n\n            return filtered_adata\n    else:\n        # Fallback: create boolean mask by mapping integer IDs to cell names\n        # obs_names contains string cell IDs, so we need to map integer IDs to cell names\n        if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n            # Create mapping from cell_integer_id to cell_id\n            obs_df = adata.slaf.obs\n            if \"cell_id\" in obs_df.columns and \"cell_integer_id\" in obs_df.columns:\n                id_to_name = dict(\n                    zip(\n                        obs_df[\"cell_integer_id\"].to_list(),\n                        obs_df[\"cell_id\"].to_list(),\n                        strict=False,\n                    )\n                )\n                # Map filtered integer IDs to cell names\n                filtered_cell_names = {\n                    id_to_name.get(cid)\n                    for cid in filtered_cells[\"cell_integer_id\"].to_list()\n                    if id_to_name.get(cid) is not None\n                }\n                # Create boolean mask\n                cell_mask = adata.obs_names.isin(filtered_cell_names)\n            else:\n                # Fallback: use integer IDs directly (assume obs_names are integer strings)\n                filtered_cell_ids = {\n                    str(cid) for cid in filtered_cells[\"cell_integer_id\"].to_list()\n                }\n                cell_mask = adata.obs_names.astype(str).isin(filtered_cell_ids)\n        else:\n            # Last resort: assume obs_names match integer IDs as strings\n            filtered_cell_ids = {\n                str(cid) for cid in filtered_cells[\"cell_integer_id\"].to_list()\n            }\n            cell_mask = adata.obs_names.astype(str).isin(filtered_cell_ids)\n\n    if inplace:\n        # Apply filter to adata (would need proper implementation)\n        # For now, just return the original adata\n        print(\n            f\"Filtered out {np.sum(~cell_mask)} cells, {np.sum(cell_mask)} remaining\"\n        )\n        return None\n    else:\n        # Create new filtered LazyAnnData\n        filtered_adata = adata.copy()\n        # Apply mask (would need proper implementation)\n        return filtered_adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing.filter_genes","title":"<code>filter_genes(adata: LazyAnnData, min_counts: int | None = None, min_cells: int | None = None, max_counts: int | None = None, max_cells: int | None = None, inplace: bool = True) -&gt; LazyAnnData | None</code>  <code>staticmethod</code>","text":"<p>Filter genes based on quality control metrics using lazy evaluation.</p> <p>This function filters genes based on their total counts and number of cells using SQL aggregation for memory efficiency. It supports both minimum and maximum thresholds for each metric.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>min_counts</code> <code>int | None</code> <p>Minimum total counts per gene. Genes with fewer counts are filtered.</p> <code>None</code> <code>min_cells</code> <code>int | None</code> <p>Minimum number of cells per gene. Genes expressed in fewer cells are filtered.</p> <code>None</code> <code>max_counts</code> <code>int | None</code> <p>Maximum total counts per gene. Genes with more counts are filtered.</p> <code>None</code> <code>max_cells</code> <code>int | None</code> <p>Maximum number of cells per gene. Genes expressed in more cells are filtered.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. Currently not fully     implemented in lazy version - returns None when True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LazyAnnData | None</code> <p>LazyAnnData | None: If inplace=False, returns filtered LazyAnnData.                If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all genes are filtered out by the criteria.</p> <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Filter genes with basic criteria\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; filtered = LazyPreprocessing.filter_genes(\n...     adata, min_counts=10, min_cells=5, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Original genes: {adata.n_vars}\")\nOriginal genes: 20000\n&gt;&gt;&gt; print(f\"Filtered genes: {filtered.n_vars}\")\nFiltered genes: 15000\n</code></pre> <pre><code>&gt;&gt;&gt; # Filter with maximum thresholds\n&gt;&gt;&gt; filtered = LazyPreprocessing.filter_genes(\n...     adata, max_counts=100000, max_cells=1000, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Genes after max filtering: {filtered.n_vars}\")\nGenes after max filtering: 18000\n</code></pre> <pre><code>&gt;&gt;&gt; # Error when all genes filtered out\n&gt;&gt;&gt; try:\n...     LazyPreprocessing.filter_genes(adata, min_counts=1000000)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: All genes were filtered out\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>@staticmethod\ndef filter_genes(\n    adata: LazyAnnData,\n    min_counts: int | None = None,\n    min_cells: int | None = None,\n    max_counts: int | None = None,\n    max_cells: int | None = None,\n    inplace: bool = True,\n) -&gt; LazyAnnData | None:\n    \"\"\"\n    Filter genes based on quality control metrics using lazy evaluation.\n\n    This function filters genes based on their total counts and number of cells\n    using SQL aggregation for memory efficiency. It supports both minimum and\n    maximum thresholds for each metric.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        min_counts: Minimum total counts per gene. Genes with fewer counts are filtered.\n        min_cells: Minimum number of cells per gene. Genes expressed in fewer cells are filtered.\n        max_counts: Maximum total counts per gene. Genes with more counts are filtered.\n        max_cells: Maximum number of cells per gene. Genes expressed in more cells are filtered.\n        inplace: Whether to modify the adata object in place. Currently not fully\n                implemented in lazy version - returns None when True.\n\n    Returns:\n        LazyAnnData | None: If inplace=False, returns filtered LazyAnnData.\n                           If inplace=True, returns None.\n\n    Raises:\n        ValueError: If all genes are filtered out by the criteria.\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Filter genes with basic criteria\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; filtered = LazyPreprocessing.filter_genes(\n        ...     adata, min_counts=10, min_cells=5, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Original genes: {adata.n_vars}\")\n        Original genes: 20000\n        &gt;&gt;&gt; print(f\"Filtered genes: {filtered.n_vars}\")\n        Filtered genes: 15000\n\n        &gt;&gt;&gt; # Filter with maximum thresholds\n        &gt;&gt;&gt; filtered = LazyPreprocessing.filter_genes(\n        ...     adata, max_counts=100000, max_cells=1000, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Genes after max filtering: {filtered.n_vars}\")\n        Genes after max filtering: 18000\n\n        &gt;&gt;&gt; # Error when all genes filtered out\n        &gt;&gt;&gt; try:\n        ...     LazyPreprocessing.filter_genes(adata, min_counts=1000000)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: All genes were filtered out\n    \"\"\"\n\n    # Build filter conditions for genes\n    conditions = []\n\n    if min_counts is not None:\n        conditions.append(f\"gene_stats.total_counts &gt;= {min_counts}\")\n    if max_counts is not None:\n        conditions.append(f\"gene_stats.total_counts &lt;= {max_counts}\")\n    if min_cells is not None:\n        conditions.append(f\"gene_stats.n_cells_by_counts &gt;= {min_cells}\")\n    if max_cells is not None:\n        conditions.append(f\"gene_stats.n_cells_by_counts &lt;= {max_cells}\")\n\n    if not conditions:\n        return adata if not inplace else None\n\n    where_clause = \" AND \".join(conditions)\n\n    # Get filtered gene IDs using simple aggregation (no JOINs)\n    filter_sql = f\"\"\"\n    SELECT gene_stats.gene_integer_id\n    FROM (\n        SELECT\n            e.gene_integer_id,\n            COUNT(DISTINCT e.cell_integer_id) AS n_cells_by_counts,\n            SUM(e.value) AS total_counts\n        FROM expression e\n        GROUP BY e.gene_integer_id\n    ) gene_stats\n    WHERE {where_clause}\n    ORDER BY gene_stats.gene_integer_id\n    \"\"\"\n\n    filtered_genes = adata.slaf.query(filter_sql)\n\n    if len(filtered_genes) == 0:\n        raise ValueError(\"All genes were filtered out\")\n\n    # Create boolean mask from the filtered gene IDs\n    # Use the materialized var metadata to map gene_integer_id to gene names\n    if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n        # Create a mapping from gene_integer_id to gene names\n        # Use polars DataFrame to get gene names\n        var_df = adata.slaf.var\n        if \"gene_id\" in var_df.columns:\n            gene_id_to_name = dict(\n                zip(\n                    var_df[\"gene_integer_id\"].to_list(),\n                    var_df[\"gene_id\"].to_list(),\n                    strict=False,\n                )\n            )\n        else:\n            # Fallback: create gene names from integer IDs\n            gene_id_to_name = {i: f\"gene_{i}\" for i in range(len(var_df))}\n        filtered_gene_names = [\n            gene_id_to_name.get(gid, f\"gene_{gid}\")\n            for gid in filtered_genes[\"gene_integer_id\"]\n        ]\n        gene_mask = adata.var_names.isin(filtered_gene_names)\n    else:\n        # Fallback to using gene_integer_id directly\n        gene_mask = adata.var_names.isin(filtered_genes[\"gene_integer_id\"])\n\n    if inplace:\n        # Apply filter to adata (would need proper implementation)\n        print(\n            f\"Filtered out {np.sum(~gene_mask)} genes, {np.sum(gene_mask)} remaining\"\n        )\n        return None\n    else:\n        # Create new filtered LazyAnnData\n        filtered_adata = adata.copy()\n        # Apply mask (would need proper implementation)\n        return filtered_adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing.normalize_total","title":"<code>normalize_total(adata: LazyAnnData, target_sum: float | None = 10000.0, inplace: bool = True, fragments: bool | None = None) -&gt; LazyAnnData | None</code>  <code>staticmethod</code>","text":"<p>Normalize counts per cell to target sum using lazy evaluation.</p> <p>This function normalizes the expression data so that each cell has a total count equal to target_sum. The normalization is applied lazily and stored as a transformation that will be computed when the data is accessed.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>target_sum</code> <code>float | None</code> <p>Target sum for normalization. Default is 10,000.</p> <code>10000.0</code> <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. If False, returns     a copy with the transformation applied.</p> <code>True</code> <code>fragments</code> <code>bool | None</code> <p>Whether to use fragment-based processing. If None, automatically       selects based on dataset characteristics.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyAnnData | None</code> <p>LazyAnnData | None: If inplace=False, returns LazyAnnData with transformation.                If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If target_sum is not positive.</p> <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic normalization\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; LazyPreprocessing.normalize_total(adata, target_sum=10000)\nApplied normalize_total with target_sum=10000\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom target sum\n&gt;&gt;&gt; adata_copy = adata.copy()\n&gt;&gt;&gt; LazyPreprocessing.normalize_total(\n...     adata_copy, target_sum=5000, inplace=False\n... )\n&gt;&gt;&gt; print(\"Normalization applied to copy\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Error with invalid target_sum\n&gt;&gt;&gt; try:\n...     LazyPreprocessing.normalize_total(adata, target_sum=0)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: target_sum must be positive\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>@staticmethod\ndef normalize_total(\n    adata: LazyAnnData,\n    target_sum: float | None = 1e4,\n    inplace: bool = True,\n    fragments: bool | None = None,\n) -&gt; LazyAnnData | None:\n    \"\"\"\n    Normalize counts per cell to target sum using lazy evaluation.\n\n    This function normalizes the expression data so that each cell has a total\n    count equal to target_sum. The normalization is applied lazily and stored\n    as a transformation that will be computed when the data is accessed.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        target_sum: Target sum for normalization. Default is 10,000.\n        inplace: Whether to modify the adata object in place. If False, returns\n                a copy with the transformation applied.\n        fragments: Whether to use fragment-based processing. If None, automatically\n                  selects based on dataset characteristics.\n\n    Returns:\n        LazyAnnData | None: If inplace=False, returns LazyAnnData with transformation.\n                           If inplace=True, returns None.\n\n    Raises:\n        ValueError: If target_sum is not positive.\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Basic normalization\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; LazyPreprocessing.normalize_total(adata, target_sum=10000)\n        Applied normalize_total with target_sum=10000\n\n        &gt;&gt;&gt; # Custom target sum\n        &gt;&gt;&gt; adata_copy = adata.copy()\n        &gt;&gt;&gt; LazyPreprocessing.normalize_total(\n        ...     adata_copy, target_sum=5000, inplace=False\n        ... )\n        &gt;&gt;&gt; print(\"Normalization applied to copy\")\n\n        &gt;&gt;&gt; # Error with invalid target_sum\n        &gt;&gt;&gt; try:\n        ...     LazyPreprocessing.normalize_total(adata, target_sum=0)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: target_sum must be positive\n    \"\"\"\n    if target_sum is None:\n        target_sum = 1e4\n\n    # Validate target_sum\n    if target_sum &lt;= 0:\n        raise ValueError(\"target_sum must be positive\")\n\n    # Determine processing strategy\n    if fragments is not None:\n        use_fragments = fragments\n    else:\n        # Check if dataset has multiple fragments\n        try:\n            fragments_list = adata.slaf.expression.get_fragments()\n            use_fragments = len(fragments_list) &gt; 1\n        except Exception:\n            use_fragments = False\n\n    if use_fragments:\n        # Use fragment-based processing\n        try:\n            from slaf.core.fragment_processor import FragmentProcessor\n\n            # Get selectors from the LazyAnnData if it's sliced\n            cell_selector = getattr(adata, \"_cell_selector\", None)\n            gene_selector = getattr(adata, \"_gene_selector\", None)\n\n            processor = FragmentProcessor(\n                adata.slaf,\n                cell_selector=cell_selector,\n                gene_selector=gene_selector,\n                max_workers=4,\n                enable_caching=True,\n            )\n            # Use smart strategy selection for optimal performance\n            lazy_pipeline = processor.build_lazy_pipeline_smart(\n                \"normalize_total\", target_sum=target_sum\n            )\n            result_df = processor.compute(lazy_pipeline)\n\n            # Update adata with normalized values\n            return adata._update_with_normalized_data(\n                result_df, target_sum, inplace\n            )\n\n        except Exception as e:\n            print(\n                f\"Fragment processing failed, falling back to global processing: {e}\"\n            )\n            # Fall back to global processing\n            use_fragments = False\n\n    if not use_fragments:\n        # Use global processing (original implementation)\n        # Get cell totals for normalization using only the expression table\n        cell_totals_sql = \"\"\"\n        SELECT\n            e.cell_integer_id,\n            SUM(e.value) as total_counts\n        FROM expression e\n        GROUP BY e.cell_integer_id\n        ORDER BY e.cell_integer_id\n        \"\"\"\n\n        cell_totals = adata.slaf.query(cell_totals_sql)\n\n    # Work with polars DataFrame internally\n    cell_totals_pl = cell_totals\n\n    # Create normalization factors using polars\n    # Map cell_integer_id to cell names for compatibility with anndata.py\n    if hasattr(adata.slaf, \"obs\") and adata.slaf.obs is not None:\n        # Create mapping from cell_integer_id to cell names\n        # Use polars DataFrame to get cell names\n        obs_df = adata.slaf.obs\n        if \"cell_id\" in obs_df.columns:\n            cell_id_to_name = dict(\n                zip(\n                    obs_df[\"cell_integer_id\"].to_list(),\n                    obs_df[\"cell_id\"].to_list(),\n                    strict=False,\n                )\n            )\n        else:\n            # Fallback: create cell names from integer IDs\n            cell_id_to_name = {i: f\"cell_{i}\" for i in range(len(obs_df))}\n        # Use vectorized polars operations for mapping\n        # Create mapping DataFrame\n        cell_map_df = pl.DataFrame(\n            {\n                \"cell_integer_id\": list(cell_id_to_name.keys()),\n                \"cell_id\": list(cell_id_to_name.values()),\n            }\n        )\n        # Join with mapping DataFrame\n        cell_totals_pl = cell_totals_pl.join(\n            cell_map_df, on=\"cell_integer_id\", how=\"left\"\n        )\n        # Fill any missing values with default format\n        cell_totals_pl = cell_totals_pl.with_columns(\n            [\n                pl.col(\"cell_id\").fill_null(\n                    pl.col(\"cell_integer_id\")\n                    .cast(pl.Utf8)\n                    .map_elements(lambda x: f\"cell_{x}\", return_dtype=pl.Utf8)\n                ),\n                (target_sum / pl.col(\"total_counts\")).alias(\"normalization_factor\"),\n            ]\n        )\n        # Convert to dictionary for compatibility\n        normalization_dict = dict(\n            zip(\n                cell_totals_pl[\"cell_id\"].to_list(),\n                cell_totals_pl[\"normalization_factor\"].to_list(),\n                strict=False,\n            )\n        )\n    else:\n        # Fallback: use cell_integer_id as string keys\n        cell_totals_pl = cell_totals_pl.with_columns(\n            [\n                pl.col(\"cell_integer_id\").cast(pl.Utf8).alias(\"cell_id\"),\n                (target_sum / pl.col(\"total_counts\")).alias(\"normalization_factor\"),\n            ]\n        )\n        # Convert to dictionary for compatibility\n        normalization_dict = dict(\n            zip(\n                cell_totals_pl[\"cell_id\"].to_list(),\n                cell_totals_pl[\"normalization_factor\"].to_list(),\n                strict=False,\n            )\n        )\n\n    if inplace:\n        # Store normalization factors for lazy application\n        if not hasattr(adata, \"_transformations\"):\n            adata._transformations = {}\n\n        adata._transformations[\"normalize_total\"] = {\n            \"type\": \"normalize_total\",\n            \"target_sum\": float(\n                f\"{target_sum:.10f}\"\n            ),  # Convert to regular decimal format\n            \"cell_factors\": normalization_dict,\n        }\n\n        print(f\"Applied normalize_total with target_sum={target_sum}\")\n        return None\n    else:\n        # Create a copy with the transformation (copy-on-write)\n        new_adata = adata.copy()\n        if not hasattr(new_adata, \"_transformations\"):\n            new_adata._transformations = {}\n\n        new_adata._transformations[\"normalize_total\"] = {\n            \"type\": \"normalize_total\",\n            \"target_sum\": float(\n                f\"{target_sum:.10f}\"\n            ),  # Convert to regular decimal format\n            \"cell_factors\": normalization_dict,\n        }\n\n        return new_adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing.log1p","title":"<code>log1p(adata: LazyAnnData, inplace: bool = True, fragments: bool | None = None) -&gt; LazyAnnData | None</code>  <code>staticmethod</code>","text":"<p>Apply log1p transformation to expression data using lazy evaluation.</p> <p>This function applies log(1 + x) transformation to the expression data. The transformation is stored lazily and will be computed when the data is accessed, avoiding memory-intensive operations on large datasets.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. If False, returns     a copy with the transformation applied.</p> <code>True</code> <p>Returns:</p> Type Description <code>LazyAnnData | None</code> <p>LazyAnnData | None: If inplace=False, returns LazyAnnData with transformation.                If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Apply log1p transformation\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; LazyPreprocessing.log1p(adata)\nApplied log1p transformation\n</code></pre> <pre><code>&gt;&gt;&gt; # Apply to copy\n&gt;&gt;&gt; adata_copy = adata.copy()\n&gt;&gt;&gt; LazyPreprocessing.log1p(adata_copy, inplace=False)\n&gt;&gt;&gt; print(\"Log1p transformation applied to copy\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Check transformation was stored\n&gt;&gt;&gt; print(\"log1p\" in adata._transformations)\nTrue\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>@staticmethod\ndef log1p(\n    adata: LazyAnnData, inplace: bool = True, fragments: bool | None = None\n) -&gt; LazyAnnData | None:\n    \"\"\"\n    Apply log1p transformation to expression data using lazy evaluation.\n\n    This function applies log(1 + x) transformation to the expression data.\n    The transformation is stored lazily and will be computed when the data\n    is accessed, avoiding memory-intensive operations on large datasets.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        inplace: Whether to modify the adata object in place. If False, returns\n                a copy with the transformation applied.\n\n    Returns:\n        LazyAnnData | None: If inplace=False, returns LazyAnnData with transformation.\n                           If inplace=True, returns None.\n\n    Raises:\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Apply log1p transformation\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; LazyPreprocessing.log1p(adata)\n        Applied log1p transformation\n\n        &gt;&gt;&gt; # Apply to copy\n        &gt;&gt;&gt; adata_copy = adata.copy()\n        &gt;&gt;&gt; LazyPreprocessing.log1p(adata_copy, inplace=False)\n        &gt;&gt;&gt; print(\"Log1p transformation applied to copy\")\n\n        &gt;&gt;&gt; # Check transformation was stored\n        &gt;&gt;&gt; print(\"log1p\" in adata._transformations)\n        True\n    \"\"\"\n    # Determine processing strategy\n    if fragments is not None:\n        use_fragments = fragments\n    else:\n        # Check if dataset has multiple fragments\n        try:\n            fragments_list = adata.slaf.expression.get_fragments()\n            use_fragments = len(fragments_list) &gt; 1\n        except Exception:\n            use_fragments = False\n\n    if use_fragments:\n        # Use fragment-based processing\n        try:\n            from slaf.core.fragment_processor import FragmentProcessor\n\n            # Get selectors from the LazyAnnData if it's sliced\n            cell_selector = getattr(adata, \"_cell_selector\", None)\n            gene_selector = getattr(adata, \"_gene_selector\", None)\n\n            processor = FragmentProcessor(\n                adata.slaf,\n                cell_selector=cell_selector,\n                gene_selector=gene_selector,\n                max_workers=4,\n                enable_caching=True,\n            )\n            # Use smart strategy selection for optimal performance\n            lazy_pipeline = processor.build_lazy_pipeline_smart(\"log1p\")\n            result_df = processor.compute(lazy_pipeline)\n\n            # Update adata with log1p values\n            return adata._update_with_log1p_data(result_df, inplace)\n\n        except Exception as e:\n            print(\n                f\"Fragment processing failed, falling back to global processing: {e}\"\n            )\n            # Fall back to global processing\n            use_fragments = False\n\n    if not use_fragments:\n        # Use global processing (original implementation)\n        if inplace:\n            # Store log1p transformation for lazy application\n            if not hasattr(adata, \"_transformations\"):\n                adata._transformations = {}\n\n            adata._transformations[\"log1p\"] = {\"type\": \"log1p\", \"applied\": True}\n\n            print(\"Applied log1p transformation\")\n            return None\n        else:\n            # Create a copy with the transformation (copy-on-write)\n            new_adata = adata.copy()\n            if not hasattr(new_adata, \"_transformations\"):\n                new_adata._transformations = {}\n\n            new_adata._transformations[\"log1p\"] = {\"type\": \"log1p\", \"applied\": True}\n\n            return new_adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.LazyPreprocessing.highly_variable_genes","title":"<code>highly_variable_genes(adata: LazyAnnData, min_mean: float = 0.0125, max_mean: float = 3, min_disp: float = 0.5, max_disp: float = np.inf, n_top_genes: int | None = None, inplace: bool = True) -&gt; pd.DataFrame | None</code>  <code>staticmethod</code>","text":"<p>Identify highly variable genes using lazy evaluation.</p> <p>This function identifies genes with high cell-to-cell variation in expression using SQL aggregation for memory efficiency. It calculates mean expression and dispersion for each gene and applies filtering criteria.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>min_mean</code> <code>float</code> <p>Minimum mean expression for genes to be considered.</p> <code>0.0125</code> <code>max_mean</code> <code>float</code> <p>Maximum mean expression for genes to be considered.</p> <code>3</code> <code>min_disp</code> <code>float</code> <p>Minimum dispersion for genes to be considered.</p> <code>0.5</code> <code>max_disp</code> <code>float</code> <p>Maximum dispersion for genes to be considered.</p> <code>inf</code> <code>n_top_genes</code> <code>int | None</code> <p>Number of top genes to select by dispersion.         If specified, overrides min_disp and max_disp criteria.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. Currently not     fully implemented - returns None when True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>pd.DataFrame | None: If inplace=False, returns DataFrame with gene statistics                 and highly_variable column. If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Find highly variable genes\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n...     adata, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Highly variable genes: {hvg_stats['highly_variable'].sum()}\")\nHighly variable genes: 1500\n</code></pre> <pre><code>&gt;&gt;&gt; # With custom criteria\n&gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n...     adata, min_mean=0.1, max_mean=5, min_disp=1.0, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Genes meeting criteria: {hvg_stats['highly_variable'].sum()}\")\nGenes meeting criteria: 800\n</code></pre> <pre><code>&gt;&gt;&gt; # Select top N genes\n&gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n...     adata, n_top_genes=1000, inplace=False\n... )\n&gt;&gt;&gt; print(f\"Top genes selected: {hvg_stats['highly_variable'].sum()}\")\nTop genes selected: 1000\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>@staticmethod\ndef highly_variable_genes(\n    adata: LazyAnnData,\n    min_mean: float = 0.0125,\n    max_mean: float = 3,\n    min_disp: float = 0.5,\n    max_disp: float = np.inf,\n    n_top_genes: int | None = None,\n    inplace: bool = True,\n) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Identify highly variable genes using lazy evaluation.\n\n    This function identifies genes with high cell-to-cell variation in expression\n    using SQL aggregation for memory efficiency. It calculates mean expression\n    and dispersion for each gene and applies filtering criteria.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        min_mean: Minimum mean expression for genes to be considered.\n        max_mean: Maximum mean expression for genes to be considered.\n        min_disp: Minimum dispersion for genes to be considered.\n        max_disp: Maximum dispersion for genes to be considered.\n        n_top_genes: Number of top genes to select by dispersion.\n                    If specified, overrides min_disp and max_disp criteria.\n        inplace: Whether to modify the adata object in place. Currently not\n                fully implemented - returns None when True.\n\n    Returns:\n        pd.DataFrame | None: If inplace=False, returns DataFrame with gene statistics\n                            and highly_variable column. If inplace=True, returns None.\n\n    Raises:\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Find highly variable genes\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n        ...     adata, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Highly variable genes: {hvg_stats['highly_variable'].sum()}\")\n        Highly variable genes: 1500\n\n        &gt;&gt;&gt; # With custom criteria\n        &gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n        ...     adata, min_mean=0.1, max_mean=5, min_disp=1.0, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Genes meeting criteria: {hvg_stats['highly_variable'].sum()}\")\n        Genes meeting criteria: 800\n\n        &gt;&gt;&gt; # Select top N genes\n        &gt;&gt;&gt; hvg_stats = LazyPreprocessing.highly_variable_genes(\n        ...     adata, n_top_genes=1000, inplace=False\n        ... )\n        &gt;&gt;&gt; print(f\"Top genes selected: {hvg_stats['highly_variable'].sum()}\")\n        Top genes selected: 1000\n    \"\"\"\n\n    # Calculate gene statistics via SQL using simple aggregation (no JOINs)\n    stats_sql = \"\"\"\n    SELECT\n        e.gene_integer_id,\n        COUNT(DISTINCT e.cell_integer_id) AS n_cells,\n        AVG(e.value) AS mean_expr,\n        VARIANCE(e.value) AS variance,\n        CASE WHEN AVG(e.value) &gt; 0 THEN VARIANCE(e.value) / AVG(e.value) ELSE 0 END as dispersion\n    FROM expression e\n    GROUP BY e.gene_integer_id\n    ORDER BY e.gene_integer_id\n    \"\"\"\n\n    gene_stats = adata.slaf.query(stats_sql)\n\n    # Work with polars DataFrames internally\n    gene_stats_pl = gene_stats\n\n    # Get the expected gene_integer_ids from the materialized var metadata\n    # Use in-memory var if available, otherwise fall back to SQL\n    if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n        # Use the materialized var metadata directly\n        expected_genes_pl = pl.DataFrame(\n            {\"gene_integer_id\": range(len(adata.slaf.var))}\n        )\n    else:\n        # Fallback to SQL if var is not available\n        expected_genes_sql = \"\"\"\n        SELECT gene_integer_id\n        FROM genes\n        ORDER BY gene_integer_id\n        \"\"\"\n        expected_genes = adata.slaf.query(expected_genes_sql)\n        expected_genes_pl = expected_genes\n\n    # Create a complete gene_stats DataFrame with all expected genes using polars\n    gene_stats_complete_pl = expected_genes_pl.join(\n        gene_stats_pl, on=\"gene_integer_id\", how=\"left\"\n    ).fill_null(0)\n\n    # Ensure proper data types\n    gene_stats_complete_pl = gene_stats_complete_pl.with_columns(\n        [\n            pl.col(\"n_cells\").cast(pl.Int64),\n            pl.col(\"mean_expr\").cast(pl.Float64),\n            pl.col(\"variance\").cast(pl.Float64),\n            pl.col(\"dispersion\").cast(pl.Float64),\n        ]\n    )\n\n    # Map gene_integer_id to gene_id for scanpy compatibility\n    if hasattr(adata.slaf, \"var\") and adata.slaf.var is not None:\n        # Create mapping from gene_integer_id to gene names\n        # Use polars DataFrame to get gene names\n        var_df = adata.slaf.var\n        if \"gene_id\" in var_df.columns:\n            gene_id_to_name = dict(\n                zip(\n                    var_df[\"gene_integer_id\"].to_list(),\n                    var_df[\"gene_id\"].to_list(),\n                    strict=False,\n                )\n            )\n        else:\n            # Fallback: create gene names from integer IDs\n            gene_id_to_name = {i: f\"gene_{i}\" for i in range(len(var_df))}\n        # Use vectorized polars operations for mapping\n        # Create mapping DataFrame\n        gene_map_df = pl.DataFrame(\n            {\n                \"gene_integer_id\": list(gene_id_to_name.keys()),\n                \"gene_id\": list(gene_id_to_name.values()),\n            }\n        )\n        # Join with mapping DataFrame\n        gene_stats_complete_pl = gene_stats_complete_pl.join(\n            gene_map_df, on=\"gene_integer_id\", how=\"left\"\n        )\n        # Fill any missing values with default format\n        gene_stats_complete_pl = gene_stats_complete_pl.with_columns(\n            pl.col(\"gene_id\").fill_null(\n                pl.col(\"gene_integer_id\")\n                .cast(pl.Utf8)\n                .map_elements(lambda x: f\"gene_{x}\", return_dtype=pl.Utf8)\n            )\n        )\n    else:\n        # Fallback: use gene_integer_id as gene_id\n        gene_stats_complete_pl = gene_stats_complete_pl.with_columns(\n            pl.col(\"gene_integer_id\").cast(pl.Utf8).alias(\"gene_id\")\n        )\n\n    # Convert to pandas and set gene_id as index for scanpy compatibility\n    gene_stats_complete = gene_stats_complete_pl.to_pandas().set_index(\"gene_id\")\n\n    # Apply HVG criteria\n    hvg_mask = (\n        (gene_stats_complete[\"mean_expr\"] &gt;= min_mean)\n        &amp; (gene_stats_complete[\"mean_expr\"] &lt;= max_mean)\n        &amp; (gene_stats_complete[\"dispersion\"] &gt;= min_disp)\n        &amp; (gene_stats_complete[\"dispersion\"] &lt;= max_disp)\n    )\n\n    gene_stats_complete[\"highly_variable\"] = hvg_mask\n\n    if n_top_genes is not None:\n        # Select top N genes by dispersion\n        top_genes = gene_stats_complete.nlargest(n_top_genes, \"dispersion\")\n        gene_stats_complete[\"highly_variable\"] = gene_stats_complete.index.isin(\n            top_genes.index\n        )\n\n    if inplace:\n        # Update var metadata (would need implementation)\n        print(f\"Identified {hvg_mask.sum()} highly variable genes\")\n        return None\n    else:\n        return gene_stats_complete\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy-functions","title":"Functions","text":""},{"location":"api/integrations/#slaf.integrations.scanpy.apply_transformations","title":"<code>apply_transformations(adata: LazyAnnData, transformations: list | None = None) -&gt; LazyAnnData</code>","text":"<p>Apply a list of transformations to the data.</p> <p>This function provides explicit control over when transformations are applied. It creates a copy of the LazyAnnData with only the specified transformations stored for lazy evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>transformations</code> <code>list | None</code> <p>List of transformation names to apply. If None, applies all            available transformations.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LazyAnnData</code> <code>LazyAnnData</code> <p>New LazyAnnData with specified transformations applied.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Apply all transformations\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; LazyPreprocessing.normalize_total(adata)\n&gt;&gt;&gt; LazyPreprocessing.log1p(adata)\n&gt;&gt;&gt; adata_with_transforms = apply_transformations(adata)\n&gt;&gt;&gt; print(\"Transformations applied\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Apply specific transformations\n&gt;&gt;&gt; adata_copy = adata.copy()\n&gt;&gt;&gt; adata_with_norm = apply_transformations(\n...     adata_copy, transformations=[\"normalize_total\"]\n... )\n&gt;&gt;&gt; print(\"Only normalization applied\")\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>def apply_transformations(\n    adata: LazyAnnData, transformations: list | None = None\n) -&gt; LazyAnnData:\n    \"\"\"\n    Apply a list of transformations to the data.\n\n    This function provides explicit control over when transformations are applied.\n    It creates a copy of the LazyAnnData with only the specified transformations\n    stored for lazy evaluation.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        transformations: List of transformation names to apply. If None, applies all\n                       available transformations.\n\n    Returns:\n        LazyAnnData: New LazyAnnData with specified transformations applied.\n\n    Raises:\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Apply all transformations\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; LazyPreprocessing.normalize_total(adata)\n        &gt;&gt;&gt; LazyPreprocessing.log1p(adata)\n        &gt;&gt;&gt; adata_with_transforms = apply_transformations(adata)\n        &gt;&gt;&gt; print(\"Transformations applied\")\n\n        &gt;&gt;&gt; # Apply specific transformations\n        &gt;&gt;&gt; adata_copy = adata.copy()\n        &gt;&gt;&gt; adata_with_norm = apply_transformations(\n        ...     adata_copy, transformations=[\"normalize_total\"]\n        ... )\n        &gt;&gt;&gt; print(\"Only normalization applied\")\n    \"\"\"\n    if transformations is None:\n        # Apply all transformations\n        transformations = list(adata._transformations.keys())\n\n    # Create a copy to avoid modifying the original\n    new_adata = adata.copy()\n\n    # Apply only the specified transformations\n    new_adata._transformations = {\n        name: adata._transformations[name]\n        for name in transformations\n        if name in adata._transformations\n    }\n\n    return new_adata\n</code></pre>"},{"location":"api/integrations/#slaf.integrations.scanpy.clear_transformations","title":"<code>clear_transformations(adata: LazyAnnData, inplace: bool = False) -&gt; LazyAnnData | None</code>","text":"<p>Clear all transformations from the data.</p> <p>This function removes all stored transformations from the LazyAnnData object, effectively resetting it to its original state without any preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>adata</code> <code>LazyAnnData</code> <p>LazyAnnData instance containing the single-cell data.</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the adata object in place. If False, returns     a copy with transformations cleared.</p> <code>False</code> <p>Returns:</p> Type Description <code>LazyAnnData | None</code> <p>LazyAnnData | None: If inplace=False, returns LazyAnnData with transformations                cleared. If inplace=True, returns None.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the SLAF array is not properly initialized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Clear transformations in place\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n&gt;&gt;&gt; LazyPreprocessing.normalize_total(adata)\n&gt;&gt;&gt; LazyPreprocessing.log1p(adata)\n&gt;&gt;&gt; clear_transformations(adata, inplace=True)\n&gt;&gt;&gt; print(\"Transformations cleared\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Clear transformations to copy\n&gt;&gt;&gt; adata_copy = adata.copy()\n&gt;&gt;&gt; LazyPreprocessing.normalize_total(adata_copy)\n&gt;&gt;&gt; adata_clean = clear_transformations(adata_copy, inplace=False)\n&gt;&gt;&gt; print(\"Clean copy created\")\n</code></pre> Source code in <code>slaf/integrations/scanpy.py</code> <pre><code>def clear_transformations(\n    adata: LazyAnnData, inplace: bool = False\n) -&gt; LazyAnnData | None:\n    \"\"\"\n    Clear all transformations from the data.\n\n    This function removes all stored transformations from the LazyAnnData object,\n    effectively resetting it to its original state without any preprocessing.\n\n    Args:\n        adata: LazyAnnData instance containing the single-cell data.\n        inplace: Whether to modify the adata object in place. If False, returns\n                a copy with transformations cleared.\n\n    Returns:\n        LazyAnnData | None: If inplace=False, returns LazyAnnData with transformations\n                           cleared. If inplace=True, returns None.\n\n    Raises:\n        RuntimeError: If the SLAF array is not properly initialized.\n\n    Examples:\n        &gt;&gt;&gt; # Clear transformations in place\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; adata = LazyAnnData(slaf_array)\n        &gt;&gt;&gt; LazyPreprocessing.normalize_total(adata)\n        &gt;&gt;&gt; LazyPreprocessing.log1p(adata)\n        &gt;&gt;&gt; clear_transformations(adata, inplace=True)\n        &gt;&gt;&gt; print(\"Transformations cleared\")\n\n        &gt;&gt;&gt; # Clear transformations to copy\n        &gt;&gt;&gt; adata_copy = adata.copy()\n        &gt;&gt;&gt; LazyPreprocessing.normalize_total(adata_copy)\n        &gt;&gt;&gt; adata_clean = clear_transformations(adata_copy, inplace=False)\n        &gt;&gt;&gt; print(\"Clean copy created\")\n    \"\"\"\n    if inplace:\n        adata._transformations = {}\n        return None\n    else:\n        new_adata = adata.copy()\n        new_adata._transformations = {}\n        return new_adata\n</code></pre>"},{"location":"api/ml/","title":"Machine Learning API","text":"<p>ML utilities including dataloaders and tokenizers.</p>"},{"location":"api/ml/#dataloaders","title":"DataLoaders","text":""},{"location":"api/ml/#slaf.ml.dataloaders","title":"<code>slaf.ml.dataloaders</code>","text":""},{"location":"api/ml/#slaf.ml.dataloaders-classes","title":"Classes","text":""},{"location":"api/ml/#slaf.ml.dataloaders.SLAFDataLoader","title":"<code>SLAFDataLoader</code>","text":"<p>High-performance DataLoader for SLAF data optimized for ML training.</p> <p>SLAFDataLoader provides efficient streaming of single-cell data for machine learning applications with multiple loading strategies for different use cases. It uses async batch processing and provides device-agnostic CPU tensor output for maximum training flexibility.</p> Key Features <ul> <li>Multiple tokenization strategies (GeneFormer, scGPT)</li> <li>Multiple loading modes for different entropy requirements:<ul> <li>Mixture of Scanners (MoS): Maximum entropy, best randomization (default)</li> <li>Fragment-based loading: Higher entropy, moderate performance</li> <li>Sequential loading: Fastest, lowest entropy</li> </ul> </li> <li>Pre-tokenized sequences for maximum performance (tokenized mode)</li> <li>Raw data output for external processing (raw mode)</li> <li>Device-agnostic CPU tensor output</li> <li>Async batch processing with background prefetching</li> <li>Memory-efficient streaming</li> <li>Multi-epoch training support</li> <li>Comprehensive error handling and validation</li> </ul> Loading Modes <ol> <li>Mixture of Scanners (default): Randomly samples from multiple fragment generators    for maximum entropy and randomization (88% of random entropy)</li> <li>Fragment-based: Loads complete Lance fragments for higher data entropy</li> <li>Sequential: Loads contiguous Lance batches for maximum throughput</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage with default settings (MoS loading)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n&gt;&gt;&gt; for batch in dataloader:\n...     print(f\"Batch shape: {batch['input_ids'].shape}\")\n...     print(f\"Cell IDs: {batch['cell_ids']}\")\n...     break\nBatch shape: torch.Size([32, 2048])\nCell IDs: tensor([0, 1, 2, ..., 29, 30, 31])\n&gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\nMoS enabled: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     use_mixture_of_scanners=False,\n...     by_fragment=False\n... )\n&gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\nSequential loading: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Fragment-based loading for higher entropy\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     use_mixture_of_scanners=False,\n...     by_fragment=True\n... )\n&gt;&gt;&gt; print(f\"Fragment-based loading: {dataloader.by_fragment}\")\nFragment-based loading: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Raw mode for external processing\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     raw_mode=True\n... )\n&gt;&gt;&gt; for batch in dataloader:\n...     print(f\"Raw data type: {type(batch['x'])}\")\n...     break\nRaw data type: &lt;class 'polars.dataframe.frame.DataFrame'&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; # Multi-epoch training\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     n_epochs=5\n... )\n&gt;&gt;&gt; print(f\"Number of epochs: {dataloader.n_epochs}\")\nNumber of epochs: 5\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom configuration for training\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     tokenizer_type=\"scgpt\",\n...     batch_size=64,\n...     max_genes=1024\n... )\n&gt;&gt;&gt; print(f\"Tokenizer type: {dataloader.tokenizer_type}\")\nTokenizer type: scgpt\n</code></pre> <pre><code>&gt;&gt;&gt; # Training loop example\n&gt;&gt;&gt; for batch_idx, batch in enumerate(dataloader):\n...     input_ids = batch[\"input_ids\"]\n...     attention_mask = batch[\"attention_mask\"]\n...     cell_ids = batch[\"cell_ids\"]\n...     # Your training code here\n...     if batch_idx &gt;= 2:  # Just test first few batches\n...         break\n&gt;&gt;&gt; print(\"Training loop completed\")\nTraining loop completed\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid tokenizer type\n&gt;&gt;&gt; try:\n...     dataloader = SLAFDataLoader(slaf_array, tokenizer_type=\"invalid\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Unsupported tokenizer type: invalid\n</code></pre> Source code in <code>slaf/ml/dataloaders.py</code> <pre><code>class SLAFDataLoader:\n    \"\"\"\n    High-performance DataLoader for SLAF data optimized for ML training.\n\n    SLAFDataLoader provides efficient streaming of single-cell data for machine learning\n    applications with multiple loading strategies for different use cases. It uses async\n    batch processing and provides device-agnostic CPU tensor output for maximum training flexibility.\n\n    Key Features:\n        - Multiple tokenization strategies (GeneFormer, scGPT)\n        - Multiple loading modes for different entropy requirements:\n            * Mixture of Scanners (MoS): Maximum entropy, best randomization (default)\n            * Fragment-based loading: Higher entropy, moderate performance\n            * Sequential loading: Fastest, lowest entropy\n        - Pre-tokenized sequences for maximum performance (tokenized mode)\n        - Raw data output for external processing (raw mode)\n        - Device-agnostic CPU tensor output\n        - Async batch processing with background prefetching\n        - Memory-efficient streaming\n        - Multi-epoch training support\n        - Comprehensive error handling and validation\n\n    Loading Modes:\n        1. Mixture of Scanners (default): Randomly samples from multiple fragment generators\n           for maximum entropy and randomization (88% of random entropy)\n        2. Fragment-based: Loads complete Lance fragments for higher data entropy\n        3. Sequential: Loads contiguous Lance batches for maximum throughput\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage with default settings (MoS loading)\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n        &gt;&gt;&gt; for batch in dataloader:\n        ...     print(f\"Batch shape: {batch['input_ids'].shape}\")\n        ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n        ...     break\n        Batch shape: torch.Size([32, 2048])\n        Cell IDs: tensor([0, 1, 2, ..., 29, 30, 31])\n        &gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\n        MoS enabled: True\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     use_mixture_of_scanners=False,\n        ...     by_fragment=False\n        ... )\n        &gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\n        Sequential loading: True\n\n        &gt;&gt;&gt; # Fragment-based loading for higher entropy\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     use_mixture_of_scanners=False,\n        ...     by_fragment=True\n        ... )\n        &gt;&gt;&gt; print(f\"Fragment-based loading: {dataloader.by_fragment}\")\n        Fragment-based loading: True\n\n        &gt;&gt;&gt; # Raw mode for external processing\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     raw_mode=True\n        ... )\n        &gt;&gt;&gt; for batch in dataloader:\n        ...     print(f\"Raw data type: {type(batch['x'])}\")\n        ...     break\n        Raw data type: &lt;class 'polars.dataframe.frame.DataFrame'&gt;\n\n        &gt;&gt;&gt; # Multi-epoch training\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     n_epochs=5\n        ... )\n        &gt;&gt;&gt; print(f\"Number of epochs: {dataloader.n_epochs}\")\n        Number of epochs: 5\n\n        &gt;&gt;&gt; # Custom configuration for training\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     tokenizer_type=\"scgpt\",\n        ...     batch_size=64,\n        ...     max_genes=1024\n        ... )\n        &gt;&gt;&gt; print(f\"Tokenizer type: {dataloader.tokenizer_type}\")\n        Tokenizer type: scgpt\n\n        &gt;&gt;&gt; # Training loop example\n        &gt;&gt;&gt; for batch_idx, batch in enumerate(dataloader):\n        ...     input_ids = batch[\"input_ids\"]\n        ...     attention_mask = batch[\"attention_mask\"]\n        ...     cell_ids = batch[\"cell_ids\"]\n        ...     # Your training code here\n        ...     if batch_idx &gt;= 2:  # Just test first few batches\n        ...         break\n        &gt;&gt;&gt; print(\"Training loop completed\")\n        Training loop completed\n\n        &gt;&gt;&gt; # Error handling for invalid tokenizer type\n        &gt;&gt;&gt; try:\n        ...     dataloader = SLAFDataLoader(slaf_array, tokenizer_type=\"invalid\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Unsupported tokenizer type: invalid\n    \"\"\"\n\n    device: Optional[\"torch.device\"]  # type: ignore\n    tokenizer: Optional[\"SLAFTokenizer\"]  # type: ignore\n\n    def __init__(\n        self,\n        slaf_array: SLAFArray,\n        tokenizer_type: str = \"geneformer\",\n        batch_size: int = 32,\n        max_genes: int = 2048,\n        vocab_size: int = 50000,\n        n_expression_bins: int = 10,\n        n_epochs: int = 1,  # Add n_epochs parameter\n        raw_mode: bool = False,  # Add raw_mode parameter\n        verbose: bool = True,  # Add verbose parameter\n        batches_per_chunk: int = 1,  # Default to 1 for MoS (was 50 for sequential)\n        by_fragment: bool = True,  # Default to True for MoS (was False for sequential)\n        use_mixture_of_scanners: bool = True,  # Default to True for MoS (was False)\n        n_scanners: int = 16,  # Add n_scanners parameter for MoS\n        prefetch_batch_size: int = 4194304,  # Add prefetch_batch_size parameter for MoS\n        max_queue_size: int = 5000,  # Add max_queue_size parameter\n        parallelize_fragment_reads: bool = False,  # Parallelize fragment reads in MoS (cloud optimization)\n    ):\n        \"\"\"\n        Initialize the SLAF DataLoader with training configuration.\n\n        Args:\n            slaf_array: SLAFArray instance containing the single-cell data.\n                       Must be a valid SLAFArray with proper Lance dataset structure.\n\n            # Tokenization Configuration\n            tokenizer_type: Tokenization strategy to use. Options: \"geneformer\", \"scgpt\".\n                          Geneformer uses ranked gene sequences, scGPT uses interleaved\n                          gene-expression pairs. Ignored when raw_mode=True.\n            max_genes: Maximum number of genes to include in each cell's tokenization.\n                     For Geneformer: same as sequence length. For scGPT: number of\n                     gene-expression pairs (sequence length = 2*max_genes+2).\n            vocab_size: Size of the tokenizer vocabulary. Higher values allow more\n                       genes but use more memory. Range: 1000-100000, default: 50000.\n            n_expression_bins: Number of expression level bins for scGPT discretization.\n                             Higher values provide finer expression resolution.\n                             Range: 1-1000, default: 10.\n\n            # Training Configuration\n            batch_size: Number of cells per batch. Larger batches use more memory\n                       but may improve training efficiency. Range: 1-512, default: 32.\n            n_epochs: Number of epochs to run. The generator will automatically reset\n                     after each epoch, enabling multi-epoch training on small datasets.\n                     Default: 1.\n\n            # Output Mode Configuration\n            raw_mode: If True, return raw cell \u00d7 gene data as Polars DataFrames\n                     instead of pre-tokenized sequences. This bypasses tokenization\n                     and windowing for maximum flexibility. Default: False.\n\n            # Loading Strategy Configuration (MoS is now default)\n            batches_per_chunk: Number of Lance batches to load per chunk for sequential loading.\n                             Higher values use more memory but may improve throughput.\n                             Range: 1-200, default: 1 (optimized for MoS). Only used when by_fragment=False.\n            by_fragment: If True, use fragment-based loading instead of batch-based loading.\n                        Fragment-based loading provides higher entropy but may be slightly slower.\n                        Automatically enabled when use_mixture_of_scanners=True.\n                        Default: True (enabled for MoS).\n            use_mixture_of_scanners: If True, use mixture of scanners (MoS) approach for higher\n                                   entropy by randomly sampling from multiple fragment generators.\n                                   This provides the best randomization and is now the default\n                                   for foundation model training. Default: True.\n            n_scanners: Number of fragment generators to sample from simultaneously when using MoS.\n                       Higher values provide better entropy but use more memory.\n                       Range: 1-100, default: 16. Only used when use_mixture_of_scanners=True.\n            prefetch_batch_size: Target number of rows to load per prefetch batch when using MoS.\n                               Higher values improve throughput but use more memory.\n                               Range: 1000-10000000, default: 4194304. Only used when\n                               use_mixture_of_scanners=True.\n            parallelize_fragment_reads: Whether to parallelize fragment reads in MoS mode.\n                                       Critical for cloud scenarios where network latency\n                                       dominates (can improve throughput 10-30x). For local\n                                       data, sequential reads are typically more efficient as\n                                       parallelization adds overhead without benefit.\n                                       Default: False. Only used when use_mixture_of_scanners=True.\n\n            # System Configuration\n            verbose: If True, print detailed timing and progress information.\n                    If False, suppress all SLAF internal prints for clean output.\n                    Default: True.\n\n        Raises:\n            ValueError: If tokenizer_type is not supported or parameters are invalid.\n            RuntimeError: If PyTorch is not available or datasets module is missing.\n            TypeError: If slaf_array is not a valid SLAFArray instance.\n            ImportError: If required dependencies are not available.\n\n        Loading Strategy Selection Guide:\n            - For foundation model training: Use default settings (MoS provides 88% random entropy)\n            - For cloud data: Enable parallelize_fragment_reads=True for 10-30x throughput improvement\n            - For local data: Keep parallelize_fragment_reads=False (sequential reads are more efficient)\n            - For maximum throughput: Set use_mixture_of_scanners=False, by_fragment=False\n            - For external processing: Set raw_mode=True\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization (MoS is now default)\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n            &gt;&gt;&gt; print(f\"Batch size: {dataloader.batch_size}\")\n            Batch size: 32\n            &gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\n            MoS enabled: True\n\n            &gt;&gt;&gt; # Sequential loading for maximum throughput\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(\n            ...     slaf_array=slaf_array,\n            ...     use_mixture_of_scanners=False,\n            ...     by_fragment=False\n            ... )\n            &gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\n            Sequential loading: True\n\n            &gt;&gt;&gt; # Fragment-based loading for higher entropy\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(\n            ...     slaf_array=slaf_array,\n            ...     use_mixture_of_scanners=False,\n            ...     by_fragment=True\n            ... )\n            &gt;&gt;&gt; print(f\"Fragment-based loading: {dataloader.by_fragment}\")\n            Fragment-based loading: True\n\n            &gt;&gt;&gt; # Raw mode for external processing\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(\n            ...     slaf_array=slaf_array,\n            ...     raw_mode=True\n            ... )\n            &gt;&gt;&gt; print(f\"Raw mode: {dataloader.raw_mode}\")\n            Raw mode: True\n\n            &gt;&gt;&gt; # Cloud data optimization: parallelize fragment reads\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(\n            ...     slaf_array=slaf_array,\n            ...     parallelize_fragment_reads=True,  # Critical for accelerating cloud data loading\n            ...     batches_per_chunk=16,  # Combine with higher batches_per_chunk for best results\n            ... )\n            &gt;&gt;&gt; print(f\"Parallel fragment reads: {dataloader.parallelize_fragment_reads}\")\n            Parallel fragment reads: True\n\n            &gt;&gt;&gt; # Error handling for invalid parameters\n            &gt;&gt;&gt; try:\n            ...     dataloader = SLAFDataLoader(slaf_array, n_scanners=0)\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: n_scanners must be at least 1\n        \"\"\"\n        self.slaf_array = slaf_array\n        self.tokenizer_type = tokenizer_type\n        self.batch_size = batch_size\n        self.max_genes = max_genes\n        self.n_epochs = n_epochs\n        self.raw_mode = raw_mode  # Add raw_mode attribute\n        self.verbose = verbose  # Add verbose attribute\n        self.batches_per_chunk = batches_per_chunk  # Add batches_per_chunk attribute\n        self.by_fragment = by_fragment  # Add by_fragment attribute\n        self.use_mixture_of_scanners = use_mixture_of_scanners  # Add MoS attribute\n        self.n_scanners = n_scanners  # Add n_scanners attribute\n        self.prefetch_batch_size = (\n            prefetch_batch_size  # Add prefetch_batch_size attribute\n        )\n        self.max_queue_size = max_queue_size  # Add max_queue_size attribute\n        self.parallelize_fragment_reads = (\n            parallelize_fragment_reads  # Add parallelize_fragment_reads attribute\n        )\n\n        # Validate MoS parameters\n        if self.use_mixture_of_scanners:\n            if self.n_scanners &lt; 1:\n                raise ValueError(\"n_scanners must be at least 1\")\n            if self.n_scanners &gt; 100:\n                raise ValueError(\"n_scanners cannot exceed 100\")\n            if (\n                self.prefetch_batch_size &lt; 1000\n            ):  # Allow smaller values for warm-up strategy\n                raise ValueError(\"prefetch_batch_size must be at least 1,000\")\n            if self.prefetch_batch_size &gt; 10000000:\n                raise ValueError(\"prefetch_batch_size cannot exceed 10,000,000\")\n\n        # Device-agnostic: always return CPU tensors\n        self.device = None\n\n        # Check that required modules are available\n        if not DATASETS_AVAILABLE:\n            raise ImportError(\n                \"SLAFIterableDataset is required but not available. Please install required dependencies.\"\n            )\n\n        # Initialize tokenizer (only needed for non-raw mode)\n        if not self.raw_mode:\n            self.tokenizer = SLAFTokenizer(\n                slaf_array=slaf_array,\n                tokenizer_type=tokenizer_type,\n                vocab_size=vocab_size,\n                n_expression_bins=n_expression_bins,\n            )\n\n            # Get special tokens from tokenizer\n            self.special_tokens = self.tokenizer.special_tokens\n        else:\n            # For raw mode, we don't need a tokenizer\n            self.tokenizer = None\n            self.special_tokens = None\n\n        # Use IterableDataset\n        self._dataset = SLAFIterableDataset(\n            slaf_array=slaf_array,\n            tokenizer=self.tokenizer,\n            batch_size=batch_size,\n            seed=42,  # TODO: make configurable\n            max_queue_size=max_queue_size,  # Pass max_queue_size to dataset\n            tokenizer_type=tokenizer_type,\n            n_epochs=n_epochs,  # Pass n_epochs to dataset\n            raw_mode=raw_mode,  # Pass raw_mode to dataset\n            verbose=verbose,  # Pass verbose to dataset\n            batches_per_chunk=batches_per_chunk,  # Pass batches_per_chunk to dataset\n            by_fragment=by_fragment,  # Pass by_fragment to dataset\n            use_mixture_of_scanners=use_mixture_of_scanners,  # Pass MoS to dataset\n            n_scanners=n_scanners,  # Pass n_scanners to dataset\n            prefetch_batch_size=prefetch_batch_size,  # Pass prefetch_batch_size to dataset\n            parallelize_fragment_reads=parallelize_fragment_reads,  # Pass parallelize_fragment_reads\n        )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate through batches of single-cell data based on the configured mode.\n\n        Yields batches of data suitable for machine learning training. The output format\n        depends on the configuration:\n\n        - **Tokenized mode** (default): Yields pre-tokenized sequences with attention masks\n        - **Raw mode**: Yields raw Polars DataFrames for external processing\n        - **Multi-epoch**: Automatically handles epoch transitions when n_epochs &gt; 1\n\n        The loading strategy (sequential, fragment-based, or Mixture of Scanners) affects\n        data entropy and throughput but not the output format.\n\n        Yields:\n            dict: Batch dictionary containing:\n                - **Tokenized mode** (raw_mode=False):\n                    - input_ids: Pre-tokenized gene expression data (torch.Tensor)\n                    - attention_mask: Boolean mask indicating valid tokens (torch.Tensor)\n                    - cell_ids: Integer IDs of cells in the batch (torch.Tensor)\n                - **Raw mode** (raw_mode=True):\n                    - x: Raw cell \u00d7 gene data as Polars DataFrame\n                    - cell_ids: List of cell integer IDs in the batch\n                - **Multi-epoch** (when n_epochs &gt; 1):\n                    - epoch: Current epoch number (int)\n\n        Note:\n            All tensors are returned on CPU for device-agnostic training.\n            The training loop should handle device transfer as needed.\n\n        Raises:\n            ValueError: If the tokenizer type is not supported.\n            RuntimeError: If batch processing fails.\n\n        Examples:\n            &gt;&gt;&gt; # Basic iteration (tokenized mode)\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array, batch_size=16)\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     print(f\"Batch keys: {list(batch.keys())}\")\n            ...     print(f\"Input shape: {batch['input_ids'].shape}\")\n            ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n            ...     break\n            Batch keys: ['input_ids', 'attention_mask', 'cell_ids']\n            Input shape: (16, 2048)\n            Cell IDs: tensor([0, 1, 2, ..., 13, 14, 15])\n\n            &gt;&gt;&gt; # Raw mode iteration\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array, raw_mode=True, batch_size=16)\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     print(f\"Raw data type: {type(batch['x'])}\")\n            ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n            ...     break\n            Raw data type: &lt;class 'polars.dataframe.frame.DataFrame'&gt;\n            Cell IDs: [0, 1, 2, ..., 13, 14, 15]\n\n            &gt;&gt;&gt; # Multi-epoch training\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array, n_epochs=3)\n            &gt;&gt;&gt; epochs_seen = set()\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     if 'epoch' in batch:\n            ...         epochs_seen.add(batch['epoch'])\n            ...     if len(epochs_seen) &gt;= 3:  # Stop after seeing all epochs\n            ...         break\n            &gt;&gt;&gt; print(f\"Epochs completed: {sorted(epochs_seen)}\")\n            Epochs completed: [0, 1, 2]\n\n            &gt;&gt;&gt; # Training loop with error handling\n            &gt;&gt;&gt; for batch_idx, batch in enumerate(dataloader):\n            ...     try:\n            ...         if 'input_ids' in batch:  # Tokenized mode\n            ...             input_ids = batch[\"input_ids\"]\n            ...             attention_mask = batch[\"attention_mask\"]\n            ...             cell_ids = batch[\"cell_ids\"]\n            ...         else:  # Raw mode\n            ...             x = batch[\"x\"]\n            ...             cell_ids = batch[\"cell_ids\"]\n            ...         # Your training code here\n            ...         print(f\"Processed batch {batch_idx}\")\n            ...     except Exception as e:\n            ...         print(f\"Error in batch {batch_idx}: {e}\")\n            ...         continue\n            ...     if batch_idx &gt;= 2:  # Just first few batches\n            ...         break\n            Processed batch 0\n            Processed batch 1\n            Processed batch 2\n\n            &gt;&gt;&gt; # Different tokenizer types\n            &gt;&gt;&gt; dataloader_geneformer = SLAFDataLoader(slaf_array, tokenizer_type=\"geneformer\")\n            &gt;&gt;&gt; dataloader_scgpt = SLAFDataLoader(slaf_array, tokenizer_type=\"scgpt\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Compare batch shapes\n            &gt;&gt;&gt; for batch in dataloader_geneformer:\n            ...     print(f\"Geneformer input shape: {batch['input_ids'].shape}\")\n            ...     break\n            Geneformer input shape: (32, 2048)\n            &gt;&gt;&gt; for batch in dataloader_scgpt:\n            ...     print(f\"scGPT input shape: {batch['input_ids'].shape}\")\n            ...     break\n            scGPT input shape: (32, 1024)\n        \"\"\"\n        yield from self._dataset\n\n    def __len__(self):\n        \"\"\"\n        Return the number of batches in the dataset.\n\n        Note: Since SLAFDataLoader uses an IterableDataset that streams data,\n        the exact number of batches is not known in advance. This method\n        returns 0 to indicate an unknown length for streaming datasets.\n\n        Returns:\n            int: Always returns 0 to indicate unknown length for streaming datasets.\n\n        Examples:\n            &gt;&gt;&gt; # Check dataset length\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n            &gt;&gt;&gt; print(f\"Dataset length: {len(dataloader)}\")\n            Dataset length: 0\n\n            &gt;&gt;&gt; # IterableDataset behavior\n            &gt;&gt;&gt; batch_count = 0\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     batch_count += 1\n            ...     if batch_count &gt;= 5:  # Just count first 5 batches\n            ...         break\n            &gt;&gt;&gt; print(f\"Actually processed {batch_count} batches\")\n            Actually processed 5 batches\n\n            &gt;&gt;&gt; # Length is consistent\n            &gt;&gt;&gt; print(f\"Length check: {len(dataloader)}\")\n            Length check: 0\n        \"\"\"\n        return 0  # Indicates unknown length\n\n    def __del__(self):\n        \"\"\"\n        Cleanup method to stop async prefetching.\n\n        This method is called when the DataLoader object is garbage collected.\n        It ensures that the underlying dataset's prefetcher is properly cleaned up\n        to prevent resource leaks.\n\n        Examples:\n            &gt;&gt;&gt; # DataLoader cleanup happens automatically\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n            &gt;&gt;&gt; print(\"DataLoader created\")\n            DataLoader created\n            &gt;&gt;&gt; # When dataloader goes out of scope, __del__ is called automatically\n            &gt;&gt;&gt; del dataloader\n            &gt;&gt;&gt; print(\"DataLoader destroyed and cleaned up\")\n            DataLoader destroyed and cleaned up\n\n            &gt;&gt;&gt; # Manual cleanup (not usually needed)\n            &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n            &gt;&gt;&gt; dataloader.__del__()\n            &gt;&gt;&gt; print(\"Manual cleanup completed\")\n            Manual cleanup completed\n        \"\"\"\n        if hasattr(self, \"_dataset\"):\n            # The SLAFIterableDataset doesn't have a stop method,\n            # so we just let it finish its current epoch.\n            pass\n</code></pre>"},{"location":"api/ml/#slaf.ml.dataloaders.SLAFDataLoader-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.dataloaders.SLAFDataLoader.__init__","title":"<code>__init__(slaf_array: SLAFArray, tokenizer_type: str = 'geneformer', batch_size: int = 32, max_genes: int = 2048, vocab_size: int = 50000, n_expression_bins: int = 10, n_epochs: int = 1, raw_mode: bool = False, verbose: bool = True, batches_per_chunk: int = 1, by_fragment: bool = True, use_mixture_of_scanners: bool = True, n_scanners: int = 16, prefetch_batch_size: int = 4194304, max_queue_size: int = 5000, parallelize_fragment_reads: bool = False)</code>","text":"<p>Initialize the SLAF DataLoader with training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>slaf_array</code> <code>SLAFArray</code> <p>SLAFArray instance containing the single-cell data.        Must be a valid SLAFArray with proper Lance dataset structure.</p> required <code>tokenizer_type</code> <code>str</code> <p>Tokenization strategy to use. Options: \"geneformer\", \"scgpt\".           Geneformer uses ranked gene sequences, scGPT uses interleaved           gene-expression pairs. Ignored when raw_mode=True.</p> <code>'geneformer'</code> <code>max_genes</code> <code>int</code> <p>Maximum number of genes to include in each cell's tokenization.      For Geneformer: same as sequence length. For scGPT: number of      gene-expression pairs (sequence length = 2*max_genes+2).</p> <code>2048</code> <code>vocab_size</code> <code>int</code> <p>Size of the tokenizer vocabulary. Higher values allow more        genes but use more memory. Range: 1000-100000, default: 50000.</p> <code>50000</code> <code>n_expression_bins</code> <code>int</code> <p>Number of expression level bins for scGPT discretization.              Higher values provide finer expression resolution.              Range: 1-1000, default: 10.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Number of cells per batch. Larger batches use more memory        but may improve training efficiency. Range: 1-512, default: 32.</p> <code>32</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs to run. The generator will automatically reset      after each epoch, enabling multi-epoch training on small datasets.      Default: 1.</p> <code>1</code> <code>raw_mode</code> <code>bool</code> <p>If True, return raw cell \u00d7 gene data as Polars DataFrames      instead of pre-tokenized sequences. This bypasses tokenization      and windowing for maximum flexibility. Default: False.</p> <code>False</code> <code>batches_per_chunk</code> <code>int</code> <p>Number of Lance batches to load per chunk for sequential loading.              Higher values use more memory but may improve throughput.              Range: 1-200, default: 1 (optimized for MoS). Only used when by_fragment=False.</p> <code>1</code> <code>by_fragment</code> <code>bool</code> <p>If True, use fragment-based loading instead of batch-based loading.         Fragment-based loading provides higher entropy but may be slightly slower.         Automatically enabled when use_mixture_of_scanners=True.         Default: True (enabled for MoS).</p> <code>True</code> <code>use_mixture_of_scanners</code> <code>bool</code> <p>If True, use mixture of scanners (MoS) approach for higher                    entropy by randomly sampling from multiple fragment generators.                    This provides the best randomization and is now the default                    for foundation model training. Default: True.</p> <code>True</code> <code>n_scanners</code> <code>int</code> <p>Number of fragment generators to sample from simultaneously when using MoS.        Higher values provide better entropy but use more memory.        Range: 1-100, default: 16. Only used when use_mixture_of_scanners=True.</p> <code>16</code> <code>prefetch_batch_size</code> <code>int</code> <p>Target number of rows to load per prefetch batch when using MoS.                Higher values improve throughput but use more memory.                Range: 1000-10000000, default: 4194304. Only used when                use_mixture_of_scanners=True.</p> <code>4194304</code> <code>parallelize_fragment_reads</code> <code>bool</code> <p>Whether to parallelize fragment reads in MoS mode.                        Critical for cloud scenarios where network latency                        dominates (can improve throughput 10-30x). For local                        data, sequential reads are typically more efficient as                        parallelization adds overhead without benefit.                        Default: False. Only used when use_mixture_of_scanners=True.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed timing and progress information.     If False, suppress all SLAF internal prints for clean output.     Default: True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokenizer_type is not supported or parameters are invalid.</p> <code>RuntimeError</code> <p>If PyTorch is not available or datasets module is missing.</p> <code>TypeError</code> <p>If slaf_array is not a valid SLAFArray instance.</p> <code>ImportError</code> <p>If required dependencies are not available.</p> Loading Strategy Selection Guide <ul> <li>For foundation model training: Use default settings (MoS provides 88% random entropy)</li> <li>For cloud data: Enable parallelize_fragment_reads=True for 10-30x throughput improvement</li> <li>For local data: Keep parallelize_fragment_reads=False (sequential reads are more efficient)</li> <li>For maximum throughput: Set use_mixture_of_scanners=False, by_fragment=False</li> <li>For external processing: Set raw_mode=True</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization (MoS is now default)\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n&gt;&gt;&gt; print(f\"Batch size: {dataloader.batch_size}\")\nBatch size: 32\n&gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\nMoS enabled: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     use_mixture_of_scanners=False,\n...     by_fragment=False\n... )\n&gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\nSequential loading: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Fragment-based loading for higher entropy\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     use_mixture_of_scanners=False,\n...     by_fragment=True\n... )\n&gt;&gt;&gt; print(f\"Fragment-based loading: {dataloader.by_fragment}\")\nFragment-based loading: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Raw mode for external processing\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     raw_mode=True\n... )\n&gt;&gt;&gt; print(f\"Raw mode: {dataloader.raw_mode}\")\nRaw mode: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Cloud data optimization: parallelize fragment reads\n&gt;&gt;&gt; dataloader = SLAFDataLoader(\n...     slaf_array=slaf_array,\n...     parallelize_fragment_reads=True,  # Critical for accelerating cloud data loading\n...     batches_per_chunk=16,  # Combine with higher batches_per_chunk for best results\n... )\n&gt;&gt;&gt; print(f\"Parallel fragment reads: {dataloader.parallelize_fragment_reads}\")\nParallel fragment reads: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid parameters\n&gt;&gt;&gt; try:\n...     dataloader = SLAFDataLoader(slaf_array, n_scanners=0)\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: n_scanners must be at least 1\n</code></pre> Source code in <code>slaf/ml/dataloaders.py</code> <pre><code>def __init__(\n    self,\n    slaf_array: SLAFArray,\n    tokenizer_type: str = \"geneformer\",\n    batch_size: int = 32,\n    max_genes: int = 2048,\n    vocab_size: int = 50000,\n    n_expression_bins: int = 10,\n    n_epochs: int = 1,  # Add n_epochs parameter\n    raw_mode: bool = False,  # Add raw_mode parameter\n    verbose: bool = True,  # Add verbose parameter\n    batches_per_chunk: int = 1,  # Default to 1 for MoS (was 50 for sequential)\n    by_fragment: bool = True,  # Default to True for MoS (was False for sequential)\n    use_mixture_of_scanners: bool = True,  # Default to True for MoS (was False)\n    n_scanners: int = 16,  # Add n_scanners parameter for MoS\n    prefetch_batch_size: int = 4194304,  # Add prefetch_batch_size parameter for MoS\n    max_queue_size: int = 5000,  # Add max_queue_size parameter\n    parallelize_fragment_reads: bool = False,  # Parallelize fragment reads in MoS (cloud optimization)\n):\n    \"\"\"\n    Initialize the SLAF DataLoader with training configuration.\n\n    Args:\n        slaf_array: SLAFArray instance containing the single-cell data.\n                   Must be a valid SLAFArray with proper Lance dataset structure.\n\n        # Tokenization Configuration\n        tokenizer_type: Tokenization strategy to use. Options: \"geneformer\", \"scgpt\".\n                      Geneformer uses ranked gene sequences, scGPT uses interleaved\n                      gene-expression pairs. Ignored when raw_mode=True.\n        max_genes: Maximum number of genes to include in each cell's tokenization.\n                 For Geneformer: same as sequence length. For scGPT: number of\n                 gene-expression pairs (sequence length = 2*max_genes+2).\n        vocab_size: Size of the tokenizer vocabulary. Higher values allow more\n                   genes but use more memory. Range: 1000-100000, default: 50000.\n        n_expression_bins: Number of expression level bins for scGPT discretization.\n                         Higher values provide finer expression resolution.\n                         Range: 1-1000, default: 10.\n\n        # Training Configuration\n        batch_size: Number of cells per batch. Larger batches use more memory\n                   but may improve training efficiency. Range: 1-512, default: 32.\n        n_epochs: Number of epochs to run. The generator will automatically reset\n                 after each epoch, enabling multi-epoch training on small datasets.\n                 Default: 1.\n\n        # Output Mode Configuration\n        raw_mode: If True, return raw cell \u00d7 gene data as Polars DataFrames\n                 instead of pre-tokenized sequences. This bypasses tokenization\n                 and windowing for maximum flexibility. Default: False.\n\n        # Loading Strategy Configuration (MoS is now default)\n        batches_per_chunk: Number of Lance batches to load per chunk for sequential loading.\n                         Higher values use more memory but may improve throughput.\n                         Range: 1-200, default: 1 (optimized for MoS). Only used when by_fragment=False.\n        by_fragment: If True, use fragment-based loading instead of batch-based loading.\n                    Fragment-based loading provides higher entropy but may be slightly slower.\n                    Automatically enabled when use_mixture_of_scanners=True.\n                    Default: True (enabled for MoS).\n        use_mixture_of_scanners: If True, use mixture of scanners (MoS) approach for higher\n                               entropy by randomly sampling from multiple fragment generators.\n                               This provides the best randomization and is now the default\n                               for foundation model training. Default: True.\n        n_scanners: Number of fragment generators to sample from simultaneously when using MoS.\n                   Higher values provide better entropy but use more memory.\n                   Range: 1-100, default: 16. Only used when use_mixture_of_scanners=True.\n        prefetch_batch_size: Target number of rows to load per prefetch batch when using MoS.\n                           Higher values improve throughput but use more memory.\n                           Range: 1000-10000000, default: 4194304. Only used when\n                           use_mixture_of_scanners=True.\n        parallelize_fragment_reads: Whether to parallelize fragment reads in MoS mode.\n                                   Critical for cloud scenarios where network latency\n                                   dominates (can improve throughput 10-30x). For local\n                                   data, sequential reads are typically more efficient as\n                                   parallelization adds overhead without benefit.\n                                   Default: False. Only used when use_mixture_of_scanners=True.\n\n        # System Configuration\n        verbose: If True, print detailed timing and progress information.\n                If False, suppress all SLAF internal prints for clean output.\n                Default: True.\n\n    Raises:\n        ValueError: If tokenizer_type is not supported or parameters are invalid.\n        RuntimeError: If PyTorch is not available or datasets module is missing.\n        TypeError: If slaf_array is not a valid SLAFArray instance.\n        ImportError: If required dependencies are not available.\n\n    Loading Strategy Selection Guide:\n        - For foundation model training: Use default settings (MoS provides 88% random entropy)\n        - For cloud data: Enable parallelize_fragment_reads=True for 10-30x throughput improvement\n        - For local data: Keep parallelize_fragment_reads=False (sequential reads are more efficient)\n        - For maximum throughput: Set use_mixture_of_scanners=False, by_fragment=False\n        - For external processing: Set raw_mode=True\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization (MoS is now default)\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(slaf_array)\n        &gt;&gt;&gt; print(f\"Batch size: {dataloader.batch_size}\")\n        Batch size: 32\n        &gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\n        MoS enabled: True\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     use_mixture_of_scanners=False,\n        ...     by_fragment=False\n        ... )\n        &gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\n        Sequential loading: True\n\n        &gt;&gt;&gt; # Fragment-based loading for higher entropy\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     use_mixture_of_scanners=False,\n        ...     by_fragment=True\n        ... )\n        &gt;&gt;&gt; print(f\"Fragment-based loading: {dataloader.by_fragment}\")\n        Fragment-based loading: True\n\n        &gt;&gt;&gt; # Raw mode for external processing\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     raw_mode=True\n        ... )\n        &gt;&gt;&gt; print(f\"Raw mode: {dataloader.raw_mode}\")\n        Raw mode: True\n\n        &gt;&gt;&gt; # Cloud data optimization: parallelize fragment reads\n        &gt;&gt;&gt; dataloader = SLAFDataLoader(\n        ...     slaf_array=slaf_array,\n        ...     parallelize_fragment_reads=True,  # Critical for accelerating cloud data loading\n        ...     batches_per_chunk=16,  # Combine with higher batches_per_chunk for best results\n        ... )\n        &gt;&gt;&gt; print(f\"Parallel fragment reads: {dataloader.parallelize_fragment_reads}\")\n        Parallel fragment reads: True\n\n        &gt;&gt;&gt; # Error handling for invalid parameters\n        &gt;&gt;&gt; try:\n        ...     dataloader = SLAFDataLoader(slaf_array, n_scanners=0)\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: n_scanners must be at least 1\n    \"\"\"\n    self.slaf_array = slaf_array\n    self.tokenizer_type = tokenizer_type\n    self.batch_size = batch_size\n    self.max_genes = max_genes\n    self.n_epochs = n_epochs\n    self.raw_mode = raw_mode  # Add raw_mode attribute\n    self.verbose = verbose  # Add verbose attribute\n    self.batches_per_chunk = batches_per_chunk  # Add batches_per_chunk attribute\n    self.by_fragment = by_fragment  # Add by_fragment attribute\n    self.use_mixture_of_scanners = use_mixture_of_scanners  # Add MoS attribute\n    self.n_scanners = n_scanners  # Add n_scanners attribute\n    self.prefetch_batch_size = (\n        prefetch_batch_size  # Add prefetch_batch_size attribute\n    )\n    self.max_queue_size = max_queue_size  # Add max_queue_size attribute\n    self.parallelize_fragment_reads = (\n        parallelize_fragment_reads  # Add parallelize_fragment_reads attribute\n    )\n\n    # Validate MoS parameters\n    if self.use_mixture_of_scanners:\n        if self.n_scanners &lt; 1:\n            raise ValueError(\"n_scanners must be at least 1\")\n        if self.n_scanners &gt; 100:\n            raise ValueError(\"n_scanners cannot exceed 100\")\n        if (\n            self.prefetch_batch_size &lt; 1000\n        ):  # Allow smaller values for warm-up strategy\n            raise ValueError(\"prefetch_batch_size must be at least 1,000\")\n        if self.prefetch_batch_size &gt; 10000000:\n            raise ValueError(\"prefetch_batch_size cannot exceed 10,000,000\")\n\n    # Device-agnostic: always return CPU tensors\n    self.device = None\n\n    # Check that required modules are available\n    if not DATASETS_AVAILABLE:\n        raise ImportError(\n            \"SLAFIterableDataset is required but not available. Please install required dependencies.\"\n        )\n\n    # Initialize tokenizer (only needed for non-raw mode)\n    if not self.raw_mode:\n        self.tokenizer = SLAFTokenizer(\n            slaf_array=slaf_array,\n            tokenizer_type=tokenizer_type,\n            vocab_size=vocab_size,\n            n_expression_bins=n_expression_bins,\n        )\n\n        # Get special tokens from tokenizer\n        self.special_tokens = self.tokenizer.special_tokens\n    else:\n        # For raw mode, we don't need a tokenizer\n        self.tokenizer = None\n        self.special_tokens = None\n\n    # Use IterableDataset\n    self._dataset = SLAFIterableDataset(\n        slaf_array=slaf_array,\n        tokenizer=self.tokenizer,\n        batch_size=batch_size,\n        seed=42,  # TODO: make configurable\n        max_queue_size=max_queue_size,  # Pass max_queue_size to dataset\n        tokenizer_type=tokenizer_type,\n        n_epochs=n_epochs,  # Pass n_epochs to dataset\n        raw_mode=raw_mode,  # Pass raw_mode to dataset\n        verbose=verbose,  # Pass verbose to dataset\n        batches_per_chunk=batches_per_chunk,  # Pass batches_per_chunk to dataset\n        by_fragment=by_fragment,  # Pass by_fragment to dataset\n        use_mixture_of_scanners=use_mixture_of_scanners,  # Pass MoS to dataset\n        n_scanners=n_scanners,  # Pass n_scanners to dataset\n        prefetch_batch_size=prefetch_batch_size,  # Pass prefetch_batch_size to dataset\n        parallelize_fragment_reads=parallelize_fragment_reads,  # Pass parallelize_fragment_reads\n    )\n</code></pre>"},{"location":"api/ml/#slaf.ml.dataloaders-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.dataloaders.get_optimal_device","title":"<code>get_optimal_device()</code>","text":"<p>Get the optimal device for PyTorch operations (CUDA &gt; MPS &gt; CPU).</p> <p>This function determines the best available device for PyTorch operations by checking for CUDA first, then MPS (Apple Silicon), and falling back to CPU if neither is available.</p> <p>Returns:</p> Type Description <p>torch.device | None: The optimal device, or None if PyTorch is not available.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Check optimal device\n&gt;&gt;&gt; device = get_optimal_device()\n&gt;&gt;&gt; print(f\"Optimal device: {device}\")\nOptimal device: cuda\n</code></pre> <pre><code>&gt;&gt;&gt; # Device priority (CUDA &gt; MPS &gt; CPU)\n&gt;&gt;&gt; # If CUDA is available: cuda\n&gt;&gt;&gt; # If MPS is available but not CUDA: mps\n&gt;&gt;&gt; # If neither: cpu\n&gt;&gt;&gt; device = get_optimal_device()\n&gt;&gt;&gt; if device.type == \"cuda\":\n...     print(\"Using CUDA GPU\")\n... elif device.type == \"mps\":\n...     print(\"Using Apple Silicon GPU\")\n... else:\n...     print(\"Using CPU\")\nUsing CUDA GPU\n</code></pre> <pre><code>&gt;&gt;&gt; # Handle PyTorch not available\n&gt;&gt;&gt; # This would return None if PyTorch is not installed\n&gt;&gt;&gt; device = get_optimal_device()\n&gt;&gt;&gt; if device is None:\n...     print(\"PyTorch not available\")\n... else:\n...     print(f\"Device available: {device}\")\nDevice available: cuda\n</code></pre> Source code in <code>slaf/ml/dataloaders.py</code> <pre><code>def get_optimal_device():\n    \"\"\"\n    Get the optimal device for PyTorch operations (CUDA &gt; MPS &gt; CPU).\n\n    This function determines the best available device for PyTorch operations\n    by checking for CUDA first, then MPS (Apple Silicon), and falling back\n    to CPU if neither is available.\n\n    Returns:\n        torch.device | None: The optimal device, or None if PyTorch is not available.\n\n    Examples:\n        &gt;&gt;&gt; # Check optimal device\n        &gt;&gt;&gt; device = get_optimal_device()\n        &gt;&gt;&gt; print(f\"Optimal device: {device}\")\n        Optimal device: cuda\n\n        &gt;&gt;&gt; # Device priority (CUDA &gt; MPS &gt; CPU)\n        &gt;&gt;&gt; # If CUDA is available: cuda\n        &gt;&gt;&gt; # If MPS is available but not CUDA: mps\n        &gt;&gt;&gt; # If neither: cpu\n        &gt;&gt;&gt; device = get_optimal_device()\n        &gt;&gt;&gt; if device.type == \"cuda\":\n        ...     print(\"Using CUDA GPU\")\n        ... elif device.type == \"mps\":\n        ...     print(\"Using Apple Silicon GPU\")\n        ... else:\n        ...     print(\"Using CPU\")\n        Using CUDA GPU\n\n        &gt;&gt;&gt; # Handle PyTorch not available\n        &gt;&gt;&gt; # This would return None if PyTorch is not installed\n        &gt;&gt;&gt; device = get_optimal_device()\n        &gt;&gt;&gt; if device is None:\n        ...     print(\"PyTorch not available\")\n        ... else:\n        ...     print(f\"Device available: {device}\")\n        Device available: cuda\n    \"\"\"\n    if not TORCH_AVAILABLE:\n        return None\n\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"api/ml/#slaf.ml.dataloaders.get_device_info","title":"<code>get_device_info()</code>","text":"<p>Get comprehensive device information for debugging.</p> <p>This function returns detailed information about the available PyTorch devices, including CUDA and MPS availability, device counts, and capabilities. Useful for debugging device-related issues and understanding the system configuration.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Device information dictionary containing: - torch_available: Whether PyTorch is available - cuda_available: Whether CUDA is available - mps_available: Whether MPS (Apple Silicon) is available - optimal_device: String representation of the optimal device - cuda_device_count: Number of CUDA devices (if CUDA available) - cuda_device_name: Name of the first CUDA device (if available) - cuda_device_capability: Compute capability of first CUDA device</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get device information\n&gt;&gt;&gt; info = get_device_info()\n&gt;&gt;&gt; print(f\"PyTorch available: {info['torch_available']}\")\nPyTorch available: True\n&gt;&gt;&gt; print(f\"CUDA available: {info['cuda_available']}\")\nCUDA available: True\n&gt;&gt;&gt; print(f\"Optimal device: {info['optimal_device']}\")\nOptimal device: cuda\n</code></pre> <pre><code>&gt;&gt;&gt; # Check CUDA details\n&gt;&gt;&gt; if info['cuda_available']:\n...     print(f\"CUDA devices: {info['cuda_device_count']}\")\n...     print(f\"Device name: {info['cuda_device_name']}\")\n...     print(f\"Capability: {info['cuda_device_capability']}\")\nCUDA devices: 1\nDevice name: NVIDIA GeForce RTX 3080\nCapability: (8, 6)\n</code></pre> <pre><code>&gt;&gt;&gt; # Check MPS availability\n&gt;&gt;&gt; print(f\"MPS available: {info['mps_available']}\")\nMPS available: False\n</code></pre> <pre><code>&gt;&gt;&gt; # Handle PyTorch not available\n&gt;&gt;&gt; # This would show torch_available: False if PyTorch is not installed\n&gt;&gt;&gt; info = get_device_info()\n&gt;&gt;&gt; if not info['torch_available']:\n...     print(\"PyTorch not available\")\n... else:\n...     print(\"PyTorch is available\")\nPyTorch is available\n</code></pre> Source code in <code>slaf/ml/dataloaders.py</code> <pre><code>def get_device_info():\n    \"\"\"\n    Get comprehensive device information for debugging.\n\n    This function returns detailed information about the available PyTorch devices,\n    including CUDA and MPS availability, device counts, and capabilities.\n    Useful for debugging device-related issues and understanding the system\n    configuration.\n\n    Returns:\n        dict: Device information dictionary containing:\n            - torch_available: Whether PyTorch is available\n            - cuda_available: Whether CUDA is available\n            - mps_available: Whether MPS (Apple Silicon) is available\n            - optimal_device: String representation of the optimal device\n            - cuda_device_count: Number of CUDA devices (if CUDA available)\n            - cuda_device_name: Name of the first CUDA device (if available)\n            - cuda_device_capability: Compute capability of first CUDA device\n\n    Examples:\n        &gt;&gt;&gt; # Get device information\n        &gt;&gt;&gt; info = get_device_info()\n        &gt;&gt;&gt; print(f\"PyTorch available: {info['torch_available']}\")\n        PyTorch available: True\n        &gt;&gt;&gt; print(f\"CUDA available: {info['cuda_available']}\")\n        CUDA available: True\n        &gt;&gt;&gt; print(f\"Optimal device: {info['optimal_device']}\")\n        Optimal device: cuda\n\n        &gt;&gt;&gt; # Check CUDA details\n        &gt;&gt;&gt; if info['cuda_available']:\n        ...     print(f\"CUDA devices: {info['cuda_device_count']}\")\n        ...     print(f\"Device name: {info['cuda_device_name']}\")\n        ...     print(f\"Capability: {info['cuda_device_capability']}\")\n        CUDA devices: 1\n        Device name: NVIDIA GeForce RTX 3080\n        Capability: (8, 6)\n\n        &gt;&gt;&gt; # Check MPS availability\n        &gt;&gt;&gt; print(f\"MPS available: {info['mps_available']}\")\n        MPS available: False\n\n        &gt;&gt;&gt; # Handle PyTorch not available\n        &gt;&gt;&gt; # This would show torch_available: False if PyTorch is not installed\n        &gt;&gt;&gt; info = get_device_info()\n        &gt;&gt;&gt; if not info['torch_available']:\n        ...     print(\"PyTorch not available\")\n        ... else:\n        ...     print(\"PyTorch is available\")\n        PyTorch is available\n    \"\"\"\n    if not TORCH_AVAILABLE:\n        return {\n            \"torch_available\": False,\n            \"cuda_available\": False,\n            \"mps_available\": False,\n            \"optimal_device\": None,\n        }\n\n    info = {\n        \"torch_available\": True,\n        \"cuda_available\": torch.cuda.is_available(),\n        \"mps_available\": torch.backends.mps.is_available(),\n        \"optimal_device\": str(get_optimal_device()),\n    }\n\n    if torch.cuda.is_available():\n        info[\"cuda_device_count\"] = torch.cuda.device_count()\n        info[\"cuda_device_name\"] = torch.cuda.get_device_name(0)\n        info[\"cuda_device_capability\"] = torch.cuda.get_device_capability(0)\n\n    return info\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders","title":"<code>slaf.ml.tiledb_dataloaders</code>","text":"<p>TileDB Dataloader for Single-Cell Data</p> <p>This module provides efficient streaming of single-cell data from TileDB SOMA format using PyTorch IterableDataset and DataLoader. It follows a similar pattern to SLAF's dataloader implementation for consistency and performance comparison.</p>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders-classes","title":"Classes","text":""},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBPrefetchBatch","title":"<code>TileDBPrefetchBatch</code>  <code>dataclass</code>","text":"<p>Container for a batch of TileDB data with metadata.</p> <p>This dataclass holds a processed batch of TileDB SOMA data along with associated metadata for tracking performance and debugging. It serves as the primary data structure passed between the batch processor and the async prefetcher.</p> <p>Attributes:</p> Name Type Description <code>batch_id</code> <code>int</code> <p>Unique identifier for this batch within the current epoch.</p> <code>batch_df</code> <code>DataFrame</code> <p>Polars DataFrame containing the cell-gene expression data      with columns: cell_integer_id, gene_integer_id, value.</p> <code>cell_integer_ids</code> <code>list[int]</code> <p>List of unique cell IDs present in this batch.</p> <code>process_time</code> <code>float</code> <p>Time taken to process this batch (in seconds).</p> <code>memory_mb</code> <code>float</code> <p>Memory usage at the time of batch creation (in MB).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create a batch container\n&gt;&gt;&gt; batch = TileDBPrefetchBatch(\n...     batch_id=0,\n...     batch_df=df,\n...     cell_integer_ids=[0, 1, 2, 3],\n...     process_time=0.1,\n...     memory_mb=128.5\n... )\n&gt;&gt;&gt; print(f\"Batch {batch.batch_id} has {len(batch.cell_integer_ids)} cells\")\nBatch 0 has 4 cells\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>@dataclass\nclass TileDBPrefetchBatch:\n    \"\"\"\n    Container for a batch of TileDB data with metadata.\n\n    This dataclass holds a processed batch of TileDB SOMA data along with\n    associated metadata for tracking performance and debugging. It serves as\n    the primary data structure passed between the batch processor and the\n    async prefetcher.\n\n    Attributes:\n        batch_id: Unique identifier for this batch within the current epoch.\n        batch_df: Polars DataFrame containing the cell-gene expression data\n                 with columns: cell_integer_id, gene_integer_id, value.\n        cell_integer_ids: List of unique cell IDs present in this batch.\n        process_time: Time taken to process this batch (in seconds).\n        memory_mb: Memory usage at the time of batch creation (in MB).\n\n    Examples:\n        &gt;&gt;&gt; # Create a batch container\n        &gt;&gt;&gt; batch = TileDBPrefetchBatch(\n        ...     batch_id=0,\n        ...     batch_df=df,\n        ...     cell_integer_ids=[0, 1, 2, 3],\n        ...     process_time=0.1,\n        ...     memory_mb=128.5\n        ... )\n        &gt;&gt;&gt; print(f\"Batch {batch.batch_id} has {len(batch.cell_integer_ids)} cells\")\n        Batch 0 has 4 cells\n    \"\"\"\n\n    batch_id: int\n    batch_df: (\n        pl.DataFrame\n    )  # Polars DataFrame with cell_integer_id, gene_integer_id, value\n    cell_integer_ids: list[int]  # List of cell IDs in this batch\n    process_time: float\n    memory_mb: float\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBBatchProcessor","title":"<code>TileDBBatchProcessor</code>","text":"<p>High-performance batch processor for TileDB SOMA data with multiple loading strategies.</p> <p>TileDBBatchProcessor provides efficient streaming and processing of single-cell data from TileDB SOMA format. It supports multiple loading strategies including Mixture of Scanners (MoS) for maximum entropy and sequential loading for maximum throughput.</p> Key Features <ul> <li>Multiple loading strategies:<ul> <li>Mixture of Scanners (MoS): Random sampling from multiple generators for   maximum entropy and randomization (default)</li> <li>Sequential loading: Contiguous data loading for maximum throughput</li> </ul> </li> <li>Streaming data processing with configurable batch sizes</li> <li>Built-in shuffling strategies for data randomization</li> <li>Multi-epoch training support with automatic epoch transitions</li> <li>Comprehensive timing and memory monitoring</li> <li>Error handling and recovery mechanisms</li> <li>Configurable prefetch batch sizes for different dataset sizes</li> </ul> Loading Strategies <ol> <li>Mixture of Scanners (default): Randomly samples from multiple fragment    generators for maximum entropy and randomization</li> <li>Sequential: Loads contiguous data chunks for maximum throughput</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage with default MoS strategy\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     batch_size=32,\n...     prefetch_batch_size=100\n... )\n&gt;&gt;&gt; batch = processor.load_prefetch_batch()\n&gt;&gt;&gt; print(f\"Loaded batch with {len(batch.cell_integer_ids)} cells\")\nLoaded batch with 100 cells\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False,\n...     batch_size=64\n... )\n&gt;&gt;&gt; print(f\"MoS enabled: {processor.use_mixture_of_scanners}\")\nMoS enabled: False\n</code></pre> <pre><code>&gt;&gt;&gt; # Multi-epoch training\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     n_epochs=3\n... )\n&gt;&gt;&gt; print(f\"Number of epochs: {processor.n_epochs}\")\nNumber of epochs: 3\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>class TileDBBatchProcessor:\n    \"\"\"\n    High-performance batch processor for TileDB SOMA data with multiple loading strategies.\n\n    TileDBBatchProcessor provides efficient streaming and processing of single-cell data\n    from TileDB SOMA format. It supports multiple loading strategies including Mixture\n    of Scanners (MoS) for maximum entropy and sequential loading for maximum throughput.\n\n    Key Features:\n        - Multiple loading strategies:\n            * Mixture of Scanners (MoS): Random sampling from multiple generators for\n              maximum entropy and randomization (default)\n            * Sequential loading: Contiguous data loading for maximum throughput\n        - Streaming data processing with configurable batch sizes\n        - Built-in shuffling strategies for data randomization\n        - Multi-epoch training support with automatic epoch transitions\n        - Comprehensive timing and memory monitoring\n        - Error handling and recovery mechanisms\n        - Configurable prefetch batch sizes for different dataset sizes\n\n    Loading Strategies:\n        1. Mixture of Scanners (default): Randomly samples from multiple fragment\n           generators for maximum entropy and randomization\n        2. Sequential: Loads contiguous data chunks for maximum throughput\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage with default MoS strategy\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     batch_size=32,\n        ...     prefetch_batch_size=100\n        ... )\n        &gt;&gt;&gt; batch = processor.load_prefetch_batch()\n        &gt;&gt;&gt; print(f\"Loaded batch with {len(batch.cell_integer_ids)} cells\")\n        Loaded batch with 100 cells\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False,\n        ...     batch_size=64\n        ... )\n        &gt;&gt;&gt; print(f\"MoS enabled: {processor.use_mixture_of_scanners}\")\n        MoS enabled: False\n\n        &gt;&gt;&gt; # Multi-epoch training\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_epochs=3\n        ... )\n        &gt;&gt;&gt; print(f\"Number of epochs: {processor.n_epochs}\")\n        Number of epochs: 3\n    \"\"\"\n\n    def __init__(\n        self,\n        tiledb_path: str,\n        batch_size: int = 32,\n        prefetch_batch_size: int = 100,\n        seed: int = 42,\n        n_epochs: int = 1,\n        verbose: bool = True,\n        log_metrics: bool = False,\n        use_mixture_of_scanners: bool = True,\n        n_readers: int = 50,\n        n_scanners: int = 8,\n    ):\n        \"\"\"\n        Initialize the TileDB batch processor with training configuration.\n\n        Args:\n            tiledb_path: Path to the TileDB SOMA experiment directory.\n                         Must contain a valid SOMA experiment with RNA measurement data.\n            batch_size: Number of cells per training batch. Larger batches use more\n                       memory but may improve training efficiency. Range: 1-512, default: 32.\n            prefetch_batch_size: Number of cells to load per prefetch batch from TileDB.\n                               Higher values improve throughput but use more memory.\n                               Range: 10-10000, default: 100.\n            seed: Random seed for reproducible shuffling and MoS sampling.\n                  Used for consistent data ordering across runs. Default: 42.\n            n_epochs: Number of epochs to run. The processor will automatically reset\n                     after each epoch, enabling multi-epoch training. Default: 1.\n            verbose: If True, print detailed timing and progress information.\n                    If False, suppress all internal prints for clean output. Default: True.\n            log_metrics: If True, collect detailed timing metrics for performance analysis.\n                        Metrics include loading time, shuffle time, and memory usage.\n                        Default: False.\n            use_mixture_of_scanners: If True, use MoS strategy for higher entropy by\n                                   randomly sampling from multiple fragment generators.\n                                   Provides better randomization for foundation model training.\n                                   Default: True.\n            n_readers: Total number of fragment generators to create when using MoS.\n                      Higher values provide better entropy but use more memory.\n                      Range: 1-1000, default: 50.\n            n_scanners: Number of active scanners to sample from simultaneously when using MoS.\n                       Higher values provide better entropy but use more memory.\n                       Range: 1-100, default: 8.\n\n        Raises:\n            ImportError: If TileDB SOMA is not available.\n            ValueError: If MoS parameters are invalid (n_readers &lt; 1, n_scanners &lt; 1,\n                       or n_scanners &gt; n_readers).\n            RuntimeError: If the TileDB experiment cannot be opened or is invalid.\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization with default MoS strategy\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     batch_size=32,\n            ...     prefetch_batch_size=100\n            ... )\n            &gt;&gt;&gt; print(f\"Total cells: {processor.total_cells}\")\n            Total cells: 50000\n\n            &gt;&gt;&gt; # Sequential loading for maximum throughput\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     use_mixture_of_scanners=False,\n            ...     batch_size=64\n            ... )\n            &gt;&gt;&gt; print(f\"MoS enabled: {processor.use_mixture_of_scanners}\")\n            MoS enabled: False\n\n            &gt;&gt;&gt; # High-entropy MoS configuration\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     n_readers=100,\n            ...     n_scanners=16\n            ... )\n            &gt;&gt;&gt; print(f\"MoS readers: {processor.n_readers}, scanners: {processor.n_scanners}\")\n            MoS readers: 100, scanners: 16\n        \"\"\"\n        if not TILEDB_AVAILABLE:\n            raise ImportError(\"TileDB SOMA is required but not available\")\n\n        self.tiledb_path = tiledb_path\n        self.batch_size = batch_size\n        self.prefetch_batch_size = prefetch_batch_size\n        self.seed = seed\n        self.n_epochs = n_epochs\n        self.verbose = verbose\n        self.log_metrics = log_metrics\n        self.use_mixture_of_scanners = use_mixture_of_scanners\n        self.n_readers = n_readers\n        self.n_scanners = n_scanners\n\n        # Validate MoS parameters\n        if self.use_mixture_of_scanners:\n            if self.n_readers &lt; 1:\n                raise ValueError(\"n_readers must be at least 1\")\n            if self.n_scanners &lt; 1:\n                raise ValueError(\"n_scanners must be at least 1\")\n            if self.n_scanners &gt; self.n_readers:\n                raise ValueError(\"n_scanners cannot exceed n_readers\")\n\n        # Initialize state\n        self.batch_id = 0\n        self.current_epoch = 0\n        self.total_cells = 0\n\n        # Open TileDB experiment\n        self.experiment = tiledbsoma.Experiment.open(tiledb_path)\n        self.X = self.experiment.ms[\"RNA\"].X[\"data\"]\n\n        # Get total number of cells\n        self.total_cells = self.X.shape[0]\n\n        # Initialize shuffling strategy (similar to SLAF)\n        from slaf.ml.samplers import ShuffleType, create_shuffle\n\n        self.shuffle = create_shuffle(ShuffleType.RANDOM)\n\n        # Initialize MoS generators if enabled\n        if self.use_mixture_of_scanners:\n            self._initialize_mos_generators()\n\n        # Initialize timing metrics for benchmarking\n        self._timing_metrics: dict[str, list[float]] | None\n        if self.log_metrics:\n            self._timing_metrics = {\n                \"tiledb_loading\": [],\n                \"shuffle\": [],\n                \"total\": [],\n                \"cells_processed\": [],\n            }\n        else:\n            self._timing_metrics = None\n\n        # Initialize timing variables for consolidated reporting\n        self._last_load_time = 0.0\n        self._last_memory_mb = 0.0\n\n    def _initialize_mos_generators(self):\n        \"\"\"Initialize MoS generators with evenly distributed scan ranges.\"\"\"\n        # Calculate scan ranges for each generator\n        cells_per_reader = self.total_cells // self.n_readers\n        remainder = self.total_cells % self.n_readers\n\n        self.generators = []\n        current_position = 0\n\n        for i in range(self.n_readers):\n            # Distribute remainder cells among first few readers\n            reader_cell_count = cells_per_reader + (1 if i &lt; remainder else 0)\n\n            generator = {\n                \"generator_id\": i,\n                \"start_position\": current_position,\n                \"current_position\": current_position,\n                \"end_position\": current_position + reader_cell_count,\n                \"is_active\": True,\n            }\n\n            self.generators.append(generator)\n            current_position += reader_cell_count\n\n        if self.verbose:\n            print_prefetch(\n                f\"TileDB MoS initialized: {self.n_readers} generators, \"\n                f\"{self.n_scanners} active scanners, \"\n                f\"prefetch_batch_size={self.prefetch_batch_size}\",\n                self.verbose,\n            )\n\n    def reset_for_epoch(self, epoch: int) -&gt; None:\n        \"\"\"\n        Reset the processor for a new epoch.\n\n        This method resets the batch processor state to start a new epoch,\n        including resetting batch counters, MoS generator positions, and\n        shuffling seeds. It is called automatically during multi-epoch training.\n\n        Args:\n            epoch: The epoch number to start (0-based indexing).\n                  Must be 0 &lt;= epoch &lt; n_epochs.\n\n        Raises:\n            ValueError: If epoch is invalid (negative or &gt;= n_epochs).\n\n        Examples:\n            &gt;&gt;&gt; # Reset for epoch 1\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\", n_epochs=3)\n            &gt;&gt;&gt; processor.reset_for_epoch(1)\n            &gt;&gt;&gt; print(f\"Current epoch: {processor.current_epoch}\")\n            Current epoch: 1\n\n            &gt;&gt;&gt; # Invalid epoch raises error\n            &gt;&gt;&gt; try:\n            ...     processor.reset_for_epoch(5)  # n_epochs=3\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Invalid epoch 5. Must be 0 &lt;= epoch &lt; 3\n        \"\"\"\n        if epoch &lt; 0 or epoch &gt;= self.n_epochs:\n            raise ValueError(\n                f\"Invalid epoch {epoch}. Must be 0 &lt;= epoch &lt; {self.n_epochs}\"\n            )\n\n        self.current_epoch = epoch\n        self.batch_id = 0\n\n        # Reset MoS generators if enabled\n        if self.use_mixture_of_scanners:\n            for generator in self.generators:\n                generator[\"current_position\"] = generator[\"start_position\"]\n                generator[\"is_active\"] = True\n\n        if self.verbose:\n            print(f\"\ud83d\udd04 Reset TileDB processor for epoch {epoch}\")\n\n    def _record_timing(self, step: str, duration: float, cells_processed: int = 0):\n        \"\"\"Record timing for a processing step.\"\"\"\n        if not self.log_metrics or self._timing_metrics is None:\n            return\n\n        if step in self._timing_metrics:\n            self._timing_metrics[step].append(duration)\n\n        if cells_processed &gt; 0:\n            self._timing_metrics[\"cells_processed\"].append(cells_processed)\n\n    def load_prefetch_batch(self) -&gt; TileDBPrefetchBatch:\n        \"\"\"\n        Load and process a chunk of TileDB data into batches using configured strategy.\n\n        This method loads a batch of data from TileDB SOMA format, applies shuffling,\n        and returns a processed batch ready for training. It supports both MoS and\n        sequential loading strategies and handles epoch transitions automatically.\n\n        The method performs the following steps:\n        1. Load data from TileDB using the configured strategy (MoS or sequential)\n        2. Convert Arrow data to Polars DataFrame\n        3. Apply shuffling strategy for data randomization\n        4. Return processed batch with metadata\n\n        Returns:\n            TileDBPrefetchBatch: Processed batch containing:\n                - batch_df: Polars DataFrame with cell-gene expression data\n                - cell_integer_ids: List of unique cell IDs in the batch\n                - process_time: Time taken to process the batch\n                - memory_mb: Memory usage at batch creation time\n\n        Raises:\n            StopIteration: When all epochs are completed and no more data is available.\n            RuntimeError: If TileDB data loading fails.\n\n        Examples:\n            &gt;&gt;&gt; # Load a batch with MoS strategy\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     use_mixture_of_scanners=True\n            ... )\n            &gt;&gt;&gt; batch = processor.load_prefetch_batch()\n            &gt;&gt;&gt; print(f\"Batch {batch.batch_id} has {len(batch.cell_integer_ids)} cells\")\n            Batch 0 has 100 cells\n\n            &gt;&gt;&gt; # Load a batch with sequential strategy\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     use_mixture_of_scanners=False\n            ... )\n            &gt;&gt;&gt; batch = processor.load_prefetch_batch()\n            &gt;&gt;&gt; print(f\"Sequential batch shape: {batch.batch_df.shape}\")\n            Sequential batch shape: (100, 3)\n\n            &gt;&gt;&gt; # Handle epoch completion\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\", n_epochs=1)\n            &gt;&gt;&gt; try:\n            ...     while True:\n            ...         batch = processor.load_prefetch_batch()\n            ...         print(f\"Processed batch {batch.batch_id}\")\n            ... except StopIteration:\n            ...     print(\"All epochs completed\")\n            All epochs completed\n        \"\"\"\n        # Iterative approach to handle epoch transitions\n        while True:\n            start_time = time.time()\n\n            if self.use_mixture_of_scanners:\n                # MoS approach: randomly sample from active generators\n                import numpy as np\n\n                # Get indices of currently active generators\n                active_generators = [g for g in self.generators if g[\"is_active\"]]\n\n                if not active_generators:\n                    # Check if we should start a new epoch\n                    if self.current_epoch + 1 &lt; self.n_epochs:\n                        if self.verbose:\n                            print(\n                                f\"\ud83d\udd04 Epoch {self.current_epoch} complete, starting epoch {self.current_epoch + 1}\"\n                            )\n                        self.reset_for_epoch(self.current_epoch + 1)\n                        continue\n                    else:\n                        raise StopIteration(\"No more epochs available\") from None\n\n                # Randomly sample from active generators\n                n_to_sample = min(self.n_scanners, len(active_generators))\n                selected_generators = np.random.choice(\n                    active_generators, size=n_to_sample, replace=False\n                )\n\n                if self.verbose and self.batch_id % 10 == 0:\n                    print_prefetch(\n                        f\"TileDB MoS sampling: {len(active_generators)} active generators, \"\n                        f\"sampling from {n_to_sample} generators\",\n                        self.verbose,\n                    )\n\n                # Load data from selected generators\n                load_start = time.time()\n                batch_dfs = []\n\n                for generator in selected_generators:\n                    try:\n                        start_cell = generator[\"current_position\"]\n                        end_cell = min(\n                            start_cell + self.prefetch_batch_size,\n                            generator[\"end_position\"],\n                        )\n\n                        if start_cell &gt;= generator[\"end_position\"]:\n                            # Generator exhausted\n                            generator[\"is_active\"] = False\n                            continue\n\n                        # Read slice from TileDB\n                        arrow_data = (\n                            self.X.read((slice(start_cell, end_cell),))\n                            .tables()\n                            .concat()\n                        )\n\n                        # Convert Arrow table to Polars DataFrame\n                        df = pl.from_arrow(arrow_data)  # type: ignore[assignment]\n                        if not isinstance(df, pl.DataFrame):\n                            raise TypeError(\"Expected DataFrame from Arrow table\")\n\n                        # Rename SOMA columns to expected names\n                        df = df.rename(\n                            {\n                                \"soma_dim_0\": \"cell_integer_id\",\n                                \"soma_dim_1\": \"gene_integer_id\",\n                                \"soma_data\": \"value\",\n                            }\n                        )\n\n                        batch_dfs.append(df)\n\n                        # Update generator position\n                        generator[\"current_position\"] = end_cell\n\n                        # Mark as inactive if exhausted\n                        if generator[\"current_position\"] &gt;= generator[\"end_position\"]:\n                            generator[\"is_active\"] = False\n\n                    except Exception as e:\n                        logger.error(\n                            f\"Error loading TileDB data from generator {generator['generator_id']}: {e}\"\n                        )\n                        generator[\"is_active\"] = False\n                        continue\n\n                if not batch_dfs:\n                    # All selected generators are exhausted, continue to next iteration\n                    continue\n\n                # Combine all batches\n                combined_df_mos = pl.concat(batch_dfs, how=\"vertical\")\n            else:\n                # Sequential approach (original implementation)\n                current_position = (\n                    self.batch_id * self.prefetch_batch_size\n                ) % self.total_cells\n\n                # Only check for epoch transitions when we actually wrap around\n                if self.batch_id &gt; 0:\n                    prev_position = (\n                        (self.batch_id - 1) * self.prefetch_batch_size\n                    ) % self.total_cells\n                    if current_position &lt; prev_position:  # We wrapped around\n                        if self.current_epoch + 1 &lt; self.n_epochs:\n                            if self.verbose:\n                                print(\n                                    f\"\ud83d\udd04 Epoch {self.current_epoch} complete, starting epoch {self.current_epoch + 1}\"\n                                )\n                            self.reset_for_epoch(self.current_epoch + 1)\n                            continue\n                        else:\n                            raise StopIteration(\"No more epochs available\") from None\n\n                # Load data from TileDB\n                load_start = time.time()\n                try:\n                    # Read slice from TileDB as Arrow table\n                    arrow_data = (\n                        self.X.read(\n                            (\n                                slice(\n                                    current_position,\n                                    current_position + self.prefetch_batch_size,\n                                ),\n                            )\n                        )\n                        .tables()\n                        .concat()\n                    )\n\n                    # Convert Arrow table to Polars DataFrame\n                    combined_df = pl.from_arrow(arrow_data)  # type: ignore[assignment]\n                    if not isinstance(combined_df, pl.DataFrame):\n                        raise TypeError(\"Expected DataFrame from Arrow table\")\n\n                    # Rename SOMA columns to expected names\n                    combined_df = combined_df.rename(\n                        {\n                            \"soma_dim_0\": \"cell_integer_id\",\n                            \"soma_dim_1\": \"gene_integer_id\",\n                            \"soma_data\": \"value\",\n                        }\n                    )\n\n                except Exception as e:\n                    logger.error(f\"Error loading TileDB data: {e}\")\n                    raise StopIteration(f\"Failed to load TileDB data: {e}\") from e\n\n            load_time = time.time() - load_start\n            self._record_timing(\"tiledb_loading\", load_time)\n\n            # Print detailed loading breakdown every 10 batches\n            if self.batch_id % 10 == 0:\n                import psutil\n\n                process = psutil.Process()\n                memory_mb = process.memory_info().rss / 1024 / 1024\n\n                # Store timing info for consolidated report\n                self._last_load_time = load_time\n                self._last_memory_mb = memory_mb\n\n            # Apply shuffling strategy\n            shuffle_start = time.time()\n\n            # Apply shuffling with chunking\n            if self.use_mixture_of_scanners:\n                shuffled_chunks = self.shuffle.apply(\n                    combined_df_mos,  # type: ignore[arg-type]\n                    self.seed + self.batch_id + self.current_epoch * 10000,\n                    batch_size=self.batch_size,\n                )\n            else:\n                shuffled_chunks = self.shuffle.apply(\n                    combined_df,  # type: ignore[arg-type]\n                    self.seed + self.batch_id + self.current_epoch * 10000,\n                    batch_size=self.batch_size,\n                )\n\n            shuffle_time = time.time() - shuffle_start\n            total_time = time.time() - start_time\n\n            # Record timing metrics\n            self._record_timing(\"shuffle\", shuffle_time)\n            self._record_timing(\"total\", total_time)\n\n            # Count total cells across all chunks\n            total_cells_in_chunks = sum(\n                len(chunk.get_column(\"cell_integer_id\").unique())\n                for chunk in shuffled_chunks\n                if isinstance(chunk, pl.DataFrame)\n            )\n\n            # Record cells processed\n            self._record_timing(\"cells_processed\", 0, total_cells_in_chunks)\n\n            # Print consolidated prefetch batch reporting\n            if self.batch_id % 10 == 0:\n                strategy_name = \"MoS\" if self.use_mixture_of_scanners else \"sequential\"\n                prefetch_report = f\"TileDB {strategy_name} prefetch batch {self.batch_id} (epoch {self.current_epoch}):\\n\"\n                prefetch_report += f\"   TileDB loading: {self._last_load_time * 1000:.1f}ms ({self.prefetch_batch_size} cells)\\n\"\n                prefetch_report += (\n                    f\"   Processing: {shuffle_time * 1000:.1f}ms shuffle\\n\"\n                )\n                prefetch_report += f\"   Total: {total_time * 1000:.1f}ms, {len(shuffled_chunks)} chunks, {total_cells_in_chunks} cells, {self._last_memory_mb:.1f} MB\"\n\n                print_prefetch(prefetch_report, self.verbose)\n\n            self.batch_id += 1\n\n            # Return the first chunk as a batch (we'll handle multiple chunks in the dataloader)\n            if shuffled_chunks:\n                first_chunk = shuffled_chunks[0]\n                return TileDBPrefetchBatch(\n                    batch_id=self.batch_id - 1,\n                    batch_df=first_chunk,\n                    cell_integer_ids=first_chunk[\"cell_integer_id\"].unique().to_list(),\n                    process_time=shuffle_time,\n                    memory_mb=self._last_memory_mb,\n                )\n            else:\n                # No data in this chunk, continue to next iteration\n                continue\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBBatchProcessor-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBBatchProcessor.__init__","title":"<code>__init__(tiledb_path: str, batch_size: int = 32, prefetch_batch_size: int = 100, seed: int = 42, n_epochs: int = 1, verbose: bool = True, log_metrics: bool = False, use_mixture_of_scanners: bool = True, n_readers: int = 50, n_scanners: int = 8)</code>","text":"<p>Initialize the TileDB batch processor with training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>tiledb_path</code> <code>str</code> <p>Path to the TileDB SOMA experiment directory.          Must contain a valid SOMA experiment with RNA measurement data.</p> required <code>batch_size</code> <code>int</code> <p>Number of cells per training batch. Larger batches use more        memory but may improve training efficiency. Range: 1-512, default: 32.</p> <code>32</code> <code>prefetch_batch_size</code> <code>int</code> <p>Number of cells to load per prefetch batch from TileDB.                Higher values improve throughput but use more memory.                Range: 10-10000, default: 100.</p> <code>100</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible shuffling and MoS sampling.   Used for consistent data ordering across runs. Default: 42.</p> <code>42</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs to run. The processor will automatically reset      after each epoch, enabling multi-epoch training. Default: 1.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed timing and progress information.     If False, suppress all internal prints for clean output. Default: True.</p> <code>True</code> <code>log_metrics</code> <code>bool</code> <p>If True, collect detailed timing metrics for performance analysis.         Metrics include loading time, shuffle time, and memory usage.         Default: False.</p> <code>False</code> <code>use_mixture_of_scanners</code> <code>bool</code> <p>If True, use MoS strategy for higher entropy by                    randomly sampling from multiple fragment generators.                    Provides better randomization for foundation model training.                    Default: True.</p> <code>True</code> <code>n_readers</code> <code>int</code> <p>Total number of fragment generators to create when using MoS.       Higher values provide better entropy but use more memory.       Range: 1-1000, default: 50.</p> <code>50</code> <code>n_scanners</code> <code>int</code> <p>Number of active scanners to sample from simultaneously when using MoS.        Higher values provide better entropy but use more memory.        Range: 1-100, default: 8.</p> <code>8</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If TileDB SOMA is not available.</p> <code>ValueError</code> <p>If MoS parameters are invalid (n_readers &lt; 1, n_scanners &lt; 1,        or n_scanners &gt; n_readers).</p> <code>RuntimeError</code> <p>If the TileDB experiment cannot be opened or is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization with default MoS strategy\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     batch_size=32,\n...     prefetch_batch_size=100\n... )\n&gt;&gt;&gt; print(f\"Total cells: {processor.total_cells}\")\nTotal cells: 50000\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False,\n...     batch_size=64\n... )\n&gt;&gt;&gt; print(f\"MoS enabled: {processor.use_mixture_of_scanners}\")\nMoS enabled: False\n</code></pre> <pre><code>&gt;&gt;&gt; # High-entropy MoS configuration\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     n_readers=100,\n...     n_scanners=16\n... )\n&gt;&gt;&gt; print(f\"MoS readers: {processor.n_readers}, scanners: {processor.n_scanners}\")\nMoS readers: 100, scanners: 16\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def __init__(\n    self,\n    tiledb_path: str,\n    batch_size: int = 32,\n    prefetch_batch_size: int = 100,\n    seed: int = 42,\n    n_epochs: int = 1,\n    verbose: bool = True,\n    log_metrics: bool = False,\n    use_mixture_of_scanners: bool = True,\n    n_readers: int = 50,\n    n_scanners: int = 8,\n):\n    \"\"\"\n    Initialize the TileDB batch processor with training configuration.\n\n    Args:\n        tiledb_path: Path to the TileDB SOMA experiment directory.\n                     Must contain a valid SOMA experiment with RNA measurement data.\n        batch_size: Number of cells per training batch. Larger batches use more\n                   memory but may improve training efficiency. Range: 1-512, default: 32.\n        prefetch_batch_size: Number of cells to load per prefetch batch from TileDB.\n                           Higher values improve throughput but use more memory.\n                           Range: 10-10000, default: 100.\n        seed: Random seed for reproducible shuffling and MoS sampling.\n              Used for consistent data ordering across runs. Default: 42.\n        n_epochs: Number of epochs to run. The processor will automatically reset\n                 after each epoch, enabling multi-epoch training. Default: 1.\n        verbose: If True, print detailed timing and progress information.\n                If False, suppress all internal prints for clean output. Default: True.\n        log_metrics: If True, collect detailed timing metrics for performance analysis.\n                    Metrics include loading time, shuffle time, and memory usage.\n                    Default: False.\n        use_mixture_of_scanners: If True, use MoS strategy for higher entropy by\n                               randomly sampling from multiple fragment generators.\n                               Provides better randomization for foundation model training.\n                               Default: True.\n        n_readers: Total number of fragment generators to create when using MoS.\n                  Higher values provide better entropy but use more memory.\n                  Range: 1-1000, default: 50.\n        n_scanners: Number of active scanners to sample from simultaneously when using MoS.\n                   Higher values provide better entropy but use more memory.\n                   Range: 1-100, default: 8.\n\n    Raises:\n        ImportError: If TileDB SOMA is not available.\n        ValueError: If MoS parameters are invalid (n_readers &lt; 1, n_scanners &lt; 1,\n                   or n_scanners &gt; n_readers).\n        RuntimeError: If the TileDB experiment cannot be opened or is invalid.\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization with default MoS strategy\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     batch_size=32,\n        ...     prefetch_batch_size=100\n        ... )\n        &gt;&gt;&gt; print(f\"Total cells: {processor.total_cells}\")\n        Total cells: 50000\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False,\n        ...     batch_size=64\n        ... )\n        &gt;&gt;&gt; print(f\"MoS enabled: {processor.use_mixture_of_scanners}\")\n        MoS enabled: False\n\n        &gt;&gt;&gt; # High-entropy MoS configuration\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_readers=100,\n        ...     n_scanners=16\n        ... )\n        &gt;&gt;&gt; print(f\"MoS readers: {processor.n_readers}, scanners: {processor.n_scanners}\")\n        MoS readers: 100, scanners: 16\n    \"\"\"\n    if not TILEDB_AVAILABLE:\n        raise ImportError(\"TileDB SOMA is required but not available\")\n\n    self.tiledb_path = tiledb_path\n    self.batch_size = batch_size\n    self.prefetch_batch_size = prefetch_batch_size\n    self.seed = seed\n    self.n_epochs = n_epochs\n    self.verbose = verbose\n    self.log_metrics = log_metrics\n    self.use_mixture_of_scanners = use_mixture_of_scanners\n    self.n_readers = n_readers\n    self.n_scanners = n_scanners\n\n    # Validate MoS parameters\n    if self.use_mixture_of_scanners:\n        if self.n_readers &lt; 1:\n            raise ValueError(\"n_readers must be at least 1\")\n        if self.n_scanners &lt; 1:\n            raise ValueError(\"n_scanners must be at least 1\")\n        if self.n_scanners &gt; self.n_readers:\n            raise ValueError(\"n_scanners cannot exceed n_readers\")\n\n    # Initialize state\n    self.batch_id = 0\n    self.current_epoch = 0\n    self.total_cells = 0\n\n    # Open TileDB experiment\n    self.experiment = tiledbsoma.Experiment.open(tiledb_path)\n    self.X = self.experiment.ms[\"RNA\"].X[\"data\"]\n\n    # Get total number of cells\n    self.total_cells = self.X.shape[0]\n\n    # Initialize shuffling strategy (similar to SLAF)\n    from slaf.ml.samplers import ShuffleType, create_shuffle\n\n    self.shuffle = create_shuffle(ShuffleType.RANDOM)\n\n    # Initialize MoS generators if enabled\n    if self.use_mixture_of_scanners:\n        self._initialize_mos_generators()\n\n    # Initialize timing metrics for benchmarking\n    self._timing_metrics: dict[str, list[float]] | None\n    if self.log_metrics:\n        self._timing_metrics = {\n            \"tiledb_loading\": [],\n            \"shuffle\": [],\n            \"total\": [],\n            \"cells_processed\": [],\n        }\n    else:\n        self._timing_metrics = None\n\n    # Initialize timing variables for consolidated reporting\n    self._last_load_time = 0.0\n    self._last_memory_mb = 0.0\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBBatchProcessor.reset_for_epoch","title":"<code>reset_for_epoch(epoch: int) -&gt; None</code>","text":"<p>Reset the processor for a new epoch.</p> <p>This method resets the batch processor state to start a new epoch, including resetting batch counters, MoS generator positions, and shuffling seeds. It is called automatically during multi-epoch training.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number to start (0-based indexing).   Must be 0 &lt;= epoch &lt; n_epochs.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If epoch is invalid (negative or &gt;= n_epochs).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Reset for epoch 1\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\", n_epochs=3)\n&gt;&gt;&gt; processor.reset_for_epoch(1)\n&gt;&gt;&gt; print(f\"Current epoch: {processor.current_epoch}\")\nCurrent epoch: 1\n</code></pre> <pre><code>&gt;&gt;&gt; # Invalid epoch raises error\n&gt;&gt;&gt; try:\n...     processor.reset_for_epoch(5)  # n_epochs=3\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Invalid epoch 5. Must be 0 &lt;= epoch &lt; 3\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def reset_for_epoch(self, epoch: int) -&gt; None:\n    \"\"\"\n    Reset the processor for a new epoch.\n\n    This method resets the batch processor state to start a new epoch,\n    including resetting batch counters, MoS generator positions, and\n    shuffling seeds. It is called automatically during multi-epoch training.\n\n    Args:\n        epoch: The epoch number to start (0-based indexing).\n              Must be 0 &lt;= epoch &lt; n_epochs.\n\n    Raises:\n        ValueError: If epoch is invalid (negative or &gt;= n_epochs).\n\n    Examples:\n        &gt;&gt;&gt; # Reset for epoch 1\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\", n_epochs=3)\n        &gt;&gt;&gt; processor.reset_for_epoch(1)\n        &gt;&gt;&gt; print(f\"Current epoch: {processor.current_epoch}\")\n        Current epoch: 1\n\n        &gt;&gt;&gt; # Invalid epoch raises error\n        &gt;&gt;&gt; try:\n        ...     processor.reset_for_epoch(5)  # n_epochs=3\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Invalid epoch 5. Must be 0 &lt;= epoch &lt; 3\n    \"\"\"\n    if epoch &lt; 0 or epoch &gt;= self.n_epochs:\n        raise ValueError(\n            f\"Invalid epoch {epoch}. Must be 0 &lt;= epoch &lt; {self.n_epochs}\"\n        )\n\n    self.current_epoch = epoch\n    self.batch_id = 0\n\n    # Reset MoS generators if enabled\n    if self.use_mixture_of_scanners:\n        for generator in self.generators:\n            generator[\"current_position\"] = generator[\"start_position\"]\n            generator[\"is_active\"] = True\n\n    if self.verbose:\n        print(f\"\ud83d\udd04 Reset TileDB processor for epoch {epoch}\")\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBBatchProcessor.load_prefetch_batch","title":"<code>load_prefetch_batch() -&gt; TileDBPrefetchBatch</code>","text":"<p>Load and process a chunk of TileDB data into batches using configured strategy.</p> <p>This method loads a batch of data from TileDB SOMA format, applies shuffling, and returns a processed batch ready for training. It supports both MoS and sequential loading strategies and handles epoch transitions automatically.</p> <p>The method performs the following steps: 1. Load data from TileDB using the configured strategy (MoS or sequential) 2. Convert Arrow data to Polars DataFrame 3. Apply shuffling strategy for data randomization 4. Return processed batch with metadata</p> <p>Returns:</p> Name Type Description <code>TileDBPrefetchBatch</code> <code>TileDBPrefetchBatch</code> <p>Processed batch containing: - batch_df: Polars DataFrame with cell-gene expression data - cell_integer_ids: List of unique cell IDs in the batch - process_time: Time taken to process the batch - memory_mb: Memory usage at batch creation time</p> <p>Raises:</p> Type Description <code>StopIteration</code> <p>When all epochs are completed and no more data is available.</p> <code>RuntimeError</code> <p>If TileDB data loading fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Load a batch with MoS strategy\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=True\n... )\n&gt;&gt;&gt; batch = processor.load_prefetch_batch()\n&gt;&gt;&gt; print(f\"Batch {batch.batch_id} has {len(batch.cell_integer_ids)} cells\")\nBatch 0 has 100 cells\n</code></pre> <pre><code>&gt;&gt;&gt; # Load a batch with sequential strategy\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False\n... )\n&gt;&gt;&gt; batch = processor.load_prefetch_batch()\n&gt;&gt;&gt; print(f\"Sequential batch shape: {batch.batch_df.shape}\")\nSequential batch shape: (100, 3)\n</code></pre> <pre><code>&gt;&gt;&gt; # Handle epoch completion\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\", n_epochs=1)\n&gt;&gt;&gt; try:\n...     while True:\n...         batch = processor.load_prefetch_batch()\n...         print(f\"Processed batch {batch.batch_id}\")\n... except StopIteration:\n...     print(\"All epochs completed\")\nAll epochs completed\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def load_prefetch_batch(self) -&gt; TileDBPrefetchBatch:\n    \"\"\"\n    Load and process a chunk of TileDB data into batches using configured strategy.\n\n    This method loads a batch of data from TileDB SOMA format, applies shuffling,\n    and returns a processed batch ready for training. It supports both MoS and\n    sequential loading strategies and handles epoch transitions automatically.\n\n    The method performs the following steps:\n    1. Load data from TileDB using the configured strategy (MoS or sequential)\n    2. Convert Arrow data to Polars DataFrame\n    3. Apply shuffling strategy for data randomization\n    4. Return processed batch with metadata\n\n    Returns:\n        TileDBPrefetchBatch: Processed batch containing:\n            - batch_df: Polars DataFrame with cell-gene expression data\n            - cell_integer_ids: List of unique cell IDs in the batch\n            - process_time: Time taken to process the batch\n            - memory_mb: Memory usage at batch creation time\n\n    Raises:\n        StopIteration: When all epochs are completed and no more data is available.\n        RuntimeError: If TileDB data loading fails.\n\n    Examples:\n        &gt;&gt;&gt; # Load a batch with MoS strategy\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=True\n        ... )\n        &gt;&gt;&gt; batch = processor.load_prefetch_batch()\n        &gt;&gt;&gt; print(f\"Batch {batch.batch_id} has {len(batch.cell_integer_ids)} cells\")\n        Batch 0 has 100 cells\n\n        &gt;&gt;&gt; # Load a batch with sequential strategy\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False\n        ... )\n        &gt;&gt;&gt; batch = processor.load_prefetch_batch()\n        &gt;&gt;&gt; print(f\"Sequential batch shape: {batch.batch_df.shape}\")\n        Sequential batch shape: (100, 3)\n\n        &gt;&gt;&gt; # Handle epoch completion\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\", n_epochs=1)\n        &gt;&gt;&gt; try:\n        ...     while True:\n        ...         batch = processor.load_prefetch_batch()\n        ...         print(f\"Processed batch {batch.batch_id}\")\n        ... except StopIteration:\n        ...     print(\"All epochs completed\")\n        All epochs completed\n    \"\"\"\n    # Iterative approach to handle epoch transitions\n    while True:\n        start_time = time.time()\n\n        if self.use_mixture_of_scanners:\n            # MoS approach: randomly sample from active generators\n            import numpy as np\n\n            # Get indices of currently active generators\n            active_generators = [g for g in self.generators if g[\"is_active\"]]\n\n            if not active_generators:\n                # Check if we should start a new epoch\n                if self.current_epoch + 1 &lt; self.n_epochs:\n                    if self.verbose:\n                        print(\n                            f\"\ud83d\udd04 Epoch {self.current_epoch} complete, starting epoch {self.current_epoch + 1}\"\n                        )\n                    self.reset_for_epoch(self.current_epoch + 1)\n                    continue\n                else:\n                    raise StopIteration(\"No more epochs available\") from None\n\n            # Randomly sample from active generators\n            n_to_sample = min(self.n_scanners, len(active_generators))\n            selected_generators = np.random.choice(\n                active_generators, size=n_to_sample, replace=False\n            )\n\n            if self.verbose and self.batch_id % 10 == 0:\n                print_prefetch(\n                    f\"TileDB MoS sampling: {len(active_generators)} active generators, \"\n                    f\"sampling from {n_to_sample} generators\",\n                    self.verbose,\n                )\n\n            # Load data from selected generators\n            load_start = time.time()\n            batch_dfs = []\n\n            for generator in selected_generators:\n                try:\n                    start_cell = generator[\"current_position\"]\n                    end_cell = min(\n                        start_cell + self.prefetch_batch_size,\n                        generator[\"end_position\"],\n                    )\n\n                    if start_cell &gt;= generator[\"end_position\"]:\n                        # Generator exhausted\n                        generator[\"is_active\"] = False\n                        continue\n\n                    # Read slice from TileDB\n                    arrow_data = (\n                        self.X.read((slice(start_cell, end_cell),))\n                        .tables()\n                        .concat()\n                    )\n\n                    # Convert Arrow table to Polars DataFrame\n                    df = pl.from_arrow(arrow_data)  # type: ignore[assignment]\n                    if not isinstance(df, pl.DataFrame):\n                        raise TypeError(\"Expected DataFrame from Arrow table\")\n\n                    # Rename SOMA columns to expected names\n                    df = df.rename(\n                        {\n                            \"soma_dim_0\": \"cell_integer_id\",\n                            \"soma_dim_1\": \"gene_integer_id\",\n                            \"soma_data\": \"value\",\n                        }\n                    )\n\n                    batch_dfs.append(df)\n\n                    # Update generator position\n                    generator[\"current_position\"] = end_cell\n\n                    # Mark as inactive if exhausted\n                    if generator[\"current_position\"] &gt;= generator[\"end_position\"]:\n                        generator[\"is_active\"] = False\n\n                except Exception as e:\n                    logger.error(\n                        f\"Error loading TileDB data from generator {generator['generator_id']}: {e}\"\n                    )\n                    generator[\"is_active\"] = False\n                    continue\n\n            if not batch_dfs:\n                # All selected generators are exhausted, continue to next iteration\n                continue\n\n            # Combine all batches\n            combined_df_mos = pl.concat(batch_dfs, how=\"vertical\")\n        else:\n            # Sequential approach (original implementation)\n            current_position = (\n                self.batch_id * self.prefetch_batch_size\n            ) % self.total_cells\n\n            # Only check for epoch transitions when we actually wrap around\n            if self.batch_id &gt; 0:\n                prev_position = (\n                    (self.batch_id - 1) * self.prefetch_batch_size\n                ) % self.total_cells\n                if current_position &lt; prev_position:  # We wrapped around\n                    if self.current_epoch + 1 &lt; self.n_epochs:\n                        if self.verbose:\n                            print(\n                                f\"\ud83d\udd04 Epoch {self.current_epoch} complete, starting epoch {self.current_epoch + 1}\"\n                            )\n                        self.reset_for_epoch(self.current_epoch + 1)\n                        continue\n                    else:\n                        raise StopIteration(\"No more epochs available\") from None\n\n            # Load data from TileDB\n            load_start = time.time()\n            try:\n                # Read slice from TileDB as Arrow table\n                arrow_data = (\n                    self.X.read(\n                        (\n                            slice(\n                                current_position,\n                                current_position + self.prefetch_batch_size,\n                            ),\n                        )\n                    )\n                    .tables()\n                    .concat()\n                )\n\n                # Convert Arrow table to Polars DataFrame\n                combined_df = pl.from_arrow(arrow_data)  # type: ignore[assignment]\n                if not isinstance(combined_df, pl.DataFrame):\n                    raise TypeError(\"Expected DataFrame from Arrow table\")\n\n                # Rename SOMA columns to expected names\n                combined_df = combined_df.rename(\n                    {\n                        \"soma_dim_0\": \"cell_integer_id\",\n                        \"soma_dim_1\": \"gene_integer_id\",\n                        \"soma_data\": \"value\",\n                    }\n                )\n\n            except Exception as e:\n                logger.error(f\"Error loading TileDB data: {e}\")\n                raise StopIteration(f\"Failed to load TileDB data: {e}\") from e\n\n        load_time = time.time() - load_start\n        self._record_timing(\"tiledb_loading\", load_time)\n\n        # Print detailed loading breakdown every 10 batches\n        if self.batch_id % 10 == 0:\n            import psutil\n\n            process = psutil.Process()\n            memory_mb = process.memory_info().rss / 1024 / 1024\n\n            # Store timing info for consolidated report\n            self._last_load_time = load_time\n            self._last_memory_mb = memory_mb\n\n        # Apply shuffling strategy\n        shuffle_start = time.time()\n\n        # Apply shuffling with chunking\n        if self.use_mixture_of_scanners:\n            shuffled_chunks = self.shuffle.apply(\n                combined_df_mos,  # type: ignore[arg-type]\n                self.seed + self.batch_id + self.current_epoch * 10000,\n                batch_size=self.batch_size,\n            )\n        else:\n            shuffled_chunks = self.shuffle.apply(\n                combined_df,  # type: ignore[arg-type]\n                self.seed + self.batch_id + self.current_epoch * 10000,\n                batch_size=self.batch_size,\n            )\n\n        shuffle_time = time.time() - shuffle_start\n        total_time = time.time() - start_time\n\n        # Record timing metrics\n        self._record_timing(\"shuffle\", shuffle_time)\n        self._record_timing(\"total\", total_time)\n\n        # Count total cells across all chunks\n        total_cells_in_chunks = sum(\n            len(chunk.get_column(\"cell_integer_id\").unique())\n            for chunk in shuffled_chunks\n            if isinstance(chunk, pl.DataFrame)\n        )\n\n        # Record cells processed\n        self._record_timing(\"cells_processed\", 0, total_cells_in_chunks)\n\n        # Print consolidated prefetch batch reporting\n        if self.batch_id % 10 == 0:\n            strategy_name = \"MoS\" if self.use_mixture_of_scanners else \"sequential\"\n            prefetch_report = f\"TileDB {strategy_name} prefetch batch {self.batch_id} (epoch {self.current_epoch}):\\n\"\n            prefetch_report += f\"   TileDB loading: {self._last_load_time * 1000:.1f}ms ({self.prefetch_batch_size} cells)\\n\"\n            prefetch_report += (\n                f\"   Processing: {shuffle_time * 1000:.1f}ms shuffle\\n\"\n            )\n            prefetch_report += f\"   Total: {total_time * 1000:.1f}ms, {len(shuffled_chunks)} chunks, {total_cells_in_chunks} cells, {self._last_memory_mb:.1f} MB\"\n\n            print_prefetch(prefetch_report, self.verbose)\n\n        self.batch_id += 1\n\n        # Return the first chunk as a batch (we'll handle multiple chunks in the dataloader)\n        if shuffled_chunks:\n            first_chunk = shuffled_chunks[0]\n            return TileDBPrefetchBatch(\n                batch_id=self.batch_id - 1,\n                batch_df=first_chunk,\n                cell_integer_ids=first_chunk[\"cell_integer_id\"].unique().to_list(),\n                process_time=shuffle_time,\n                memory_mb=self._last_memory_mb,\n            )\n        else:\n            # No data in this chunk, continue to next iteration\n            continue\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher","title":"<code>TileDBAsyncPrefetcher</code>","text":"<p>Asynchronous prefetcher for TileDB batch processing with background loading.</p> <p>TileDBAsyncPrefetcher provides background batch processing and prefetching for TileDB data to improve training throughput. It runs a separate worker thread that continuously loads and processes batches while the main training loop consumes pre-processed data.</p> Key Features <ul> <li>Background batch processing in separate worker thread</li> <li>Configurable queue size for memory management</li> <li>Comprehensive performance monitoring and statistics</li> <li>Automatic epoch transition handling</li> <li>Graceful shutdown and cleanup</li> <li>Real-time rate monitoring and reporting</li> <li>Error handling and recovery</li> </ul> <p>The prefetcher maintains a queue of pre-processed batches and provides statistics about loading rates, memory usage, and processing times.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create prefetcher with a batch processor\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor, max_queue_size=100)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Start background processing\n&gt;&gt;&gt; prefetcher.start()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get pre-processed batches\n&gt;&gt;&gt; batch = prefetcher.get_batch()\n&gt;&gt;&gt; if batch:\n...     print(f\"Got batch {batch.batch_id} with {len(batch.cell_integer_ids)} cells\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check performance statistics\n&gt;&gt;&gt; stats = prefetcher.get_stats()\n&gt;&gt;&gt; print(f\"Loading rate: {stats['cells_per_sec']:.1f} cells/sec\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Stop background processing\n&gt;&gt;&gt; prefetcher.stop()\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>class TileDBAsyncPrefetcher:\n    \"\"\"\n    Asynchronous prefetcher for TileDB batch processing with background loading.\n\n    TileDBAsyncPrefetcher provides background batch processing and prefetching\n    for TileDB data to improve training throughput. It runs a separate worker\n    thread that continuously loads and processes batches while the main training\n    loop consumes pre-processed data.\n\n    Key Features:\n        - Background batch processing in separate worker thread\n        - Configurable queue size for memory management\n        - Comprehensive performance monitoring and statistics\n        - Automatic epoch transition handling\n        - Graceful shutdown and cleanup\n        - Real-time rate monitoring and reporting\n        - Error handling and recovery\n\n    The prefetcher maintains a queue of pre-processed batches and provides\n    statistics about loading rates, memory usage, and processing times.\n\n    Examples:\n            &gt;&gt;&gt; # Create prefetcher with a batch processor\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor, max_queue_size=100)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Start background processing\n            &gt;&gt;&gt; prefetcher.start()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get pre-processed batches\n            &gt;&gt;&gt; batch = prefetcher.get_batch()\n            &gt;&gt;&gt; if batch:\n            ...     print(f\"Got batch {batch.batch_id} with {len(batch.cell_integer_ids)} cells\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Check performance statistics\n            &gt;&gt;&gt; stats = prefetcher.get_stats()\n            &gt;&gt;&gt; print(f\"Loading rate: {stats['cells_per_sec']:.1f} cells/sec\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Stop background processing\n            &gt;&gt;&gt; prefetcher.stop()\n    \"\"\"\n\n    def __init__(\n        self, batch_processor: TileDBBatchProcessor, max_queue_size: int = 500\n    ):\n        \"\"\"\n        Initialize the TileDB async prefetcher with background processing.\n\n        Args:\n            batch_processor: TileDBBatchProcessor instance to use for loading batches.\n                           Must be properly initialized with TileDB path and configuration.\n            max_queue_size: Maximum number of pre-processed batches to keep in queue.\n                          Higher values use more memory but provide better buffering.\n                          Range: 10-10000, default: 500.\n\n        Examples:\n            &gt;&gt;&gt; # Create prefetcher with default queue size\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n            &gt;&gt;&gt; print(f\"Max queue size: {prefetcher.max_queue_size}\")\n            Max queue size: 500\n\n            &gt;&gt;&gt; # Create prefetcher with custom queue size\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor, max_queue_size=1000)\n            &gt;&gt;&gt; print(f\"Custom queue size: {prefetcher.max_queue_size}\")\n            Custom queue size: 1000\n        \"\"\"\n        self.batch_processor = batch_processor\n        self.max_queue_size = max_queue_size\n        self.queue: Queue[TileDBPrefetchBatch] = Queue(maxsize=max_queue_size)\n        self.worker_thread = None\n        self.should_stop = False\n\n        # Monitoring stats\n        self.total_cells_added = 0\n        self.start_time = None\n        self.last_rate_print = 0\n        self.total_process_time = 0.0\n        self.process_count = 0\n        self.current_epoch = 0\n\n    def start(self):\n        \"\"\"\n        Start the prefetching worker thread for background batch processing.\n\n        This method starts a background worker thread that continuously loads\n        and processes batches from the TileDB batch processor. The worker thread\n        runs as a daemon thread and will automatically stop when the main\n        process exits.\n\n        The prefetcher will begin loading batches immediately after starting.\n        Use get_batch() to retrieve pre-processed batches from the queue.\n\n        Examples:\n            &gt;&gt;&gt; # Start background prefetching\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n            &gt;&gt;&gt; prefetcher.start()\n            &gt;&gt;&gt; print(\"Prefetcher started\")\n            Prefetcher started\n\n            &gt;&gt;&gt; # Check if prefetcher is ready\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; time.sleep(1)  # Wait for first batch\n            &gt;&gt;&gt; if prefetcher.has_batch():\n            ...     print(\"Prefetcher is ready with data\")\n            ... else:\n            ...     print(\"Prefetcher not ready yet\")\n            Prefetcher is ready with data\n        \"\"\"\n        if self.worker_thread is None or not self.worker_thread.is_alive():\n            self.should_stop = False\n            self.start_time = time.time()\n            self.worker_thread = threading.Thread(\n                target=self._prefetch_worker, daemon=True\n            )\n            self.worker_thread.start()\n\n    def stop(self):\n        \"\"\"\n        Stop the prefetching worker thread and clean up resources.\n\n        This method gracefully stops the background worker thread and waits\n        for it to finish processing the current batch. It sets the stop flag\n        and joins the thread with a timeout to prevent hanging.\n\n        After calling stop(), the prefetcher will no longer load new batches.\n        Any remaining batches in the queue can still be retrieved with get_batch().\n\n        Examples:\n            &gt;&gt;&gt; # Stop the prefetcher\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n            &gt;&gt;&gt; prefetcher.start()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Do some work...\n            &gt;&gt;&gt; batch = prefetcher.get_batch()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Stop when done\n            &gt;&gt;&gt; prefetcher.stop()\n            &gt;&gt;&gt; print(\"Prefetcher stopped\")\n            Prefetcher stopped\n        \"\"\"\n        self.should_stop = True\n        if self.worker_thread and self.worker_thread.is_alive():\n            self.worker_thread.join(timeout=1.0)\n\n    def _prefetch_worker(self):\n        \"\"\"Worker thread that loads batches in background\"\"\"\n        while not self.should_stop:\n            try:\n                # Load batch chunk\n                batch = self.batch_processor.load_prefetch_batch()\n\n                # Update monitoring stats\n                self.total_cells_added += len(batch.cell_integer_ids)\n                self.total_process_time += batch.process_time\n                self.process_count += 1\n                self.current_epoch = self.batch_processor.current_epoch\n\n                elapsed = time.time() - (self.start_time or 0)\n                rate = self.total_cells_added / elapsed if elapsed &gt; 0 else 0\n\n                # Print rate every 10 batches\n                if batch.batch_id % 10 == 0 and batch.batch_id &gt; self.last_rate_print:\n                    avg_process_ms = (\n                        self.total_process_time / self.process_count\n                    ) * 1000\n                    rate_report = f\"TileDB prefetch rate: {rate:.1f} cells/sec (epoch {self.current_epoch}, total: {self.total_cells_added} cells, avg process: {avg_process_ms:.1f}ms)\"\n                    print_prefetch(rate_report, self.batch_processor.verbose)\n                    self.last_rate_print = batch.batch_id\n\n                # Put in queue\n                try:\n                    self.queue.put_nowait(batch)\n                except queue.Full:\n                    # Queue is full, wait a bit\n                    time.sleep(0.1)\n\n            except StopIteration as e:\n                if \"No more epochs available\" in str(e):\n                    if self.batch_processor.verbose:\n                        print(\n                            f\"\u2705 All {self.batch_processor.n_epochs} epochs completed\"\n                        )\n                else:\n                    logger.info(\"Reached end of batches\")\n                break\n            except Exception as e:\n                logger.info(f\"Error loading TileDB batch: {e}\")\n                break\n\n    def get_batch(self) -&gt; TileDBPrefetchBatch | None:\n        \"\"\"\n        Get the next pre-processed batch from the queue.\n\n        This method retrieves a pre-processed batch from the internal queue.\n        If no batch is available, it returns None. The method has a timeout\n        to prevent blocking indefinitely.\n\n        Returns:\n            TileDBPrefetchBatch | None: The next available batch, or None if\n                                       no batch is available within the timeout.\n\n        Examples:\n            &gt;&gt;&gt; # Get a batch from the prefetcher\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n            &gt;&gt;&gt; prefetcher.start()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Wait for a batch\n            &gt;&gt;&gt; batch = prefetcher.get_batch()\n            &gt;&gt;&gt; if batch:\n            ...     print(f\"Got batch {batch.batch_id} with {len(batch.cell_integer_ids)} cells\")\n            ... else:\n            ...     print(\"No batch available\")\n            Got batch 0 with 100 cells\n\n            &gt;&gt;&gt; # Check for multiple batches\n            &gt;&gt;&gt; batches = []\n            &gt;&gt;&gt; for _ in range(3):\n            ...     batch = prefetcher.get_batch()\n            ...     if batch:\n            ...         batches.append(batch)\n            ...     else:\n            ...         break\n            &gt;&gt;&gt; print(f\"Retrieved {len(batches)} batches\")\n            Retrieved 3 batches\n        \"\"\"\n        try:\n            return self.queue.get(timeout=1.0)\n        except queue.Empty:\n            return None\n\n    def has_batch(self) -&gt; bool:\n        \"\"\"\n        Check if a pre-processed batch is available in the queue.\n\n        This method provides a non-blocking way to check if batches are\n        available for immediate retrieval. It returns True if the queue\n        contains at least one batch, False otherwise.\n\n        Returns:\n            bool: True if at least one batch is available, False otherwise.\n\n        Examples:\n            &gt;&gt;&gt; # Check for available batches\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n            &gt;&gt;&gt; prefetcher.start()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Check if batches are ready\n            &gt;&gt;&gt; if prefetcher.has_batch():\n            ...     batch = prefetcher.get_batch()\n            ...     print(f\"Processing batch {batch.batch_id}\")\n            ... else:\n            ...     print(\"No batches ready yet\")\n            No batches ready yet\n\n            &gt;&gt;&gt; # Wait and check again\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; time.sleep(2)\n            &gt;&gt;&gt; if prefetcher.has_batch():\n            ...     print(\"Batches are now available\")\n            ... else:\n            ...     print(\"Still waiting for batches\")\n            Batches are now available\n        \"\"\"\n        return not self.queue.empty()\n\n    def get_stats(self) -&gt; dict:\n        \"\"\"\n        Get comprehensive statistics about the prefetcher's performance.\n\n        This method returns detailed performance statistics including loading\n        rates, memory usage, queue status, and processing times. Useful for\n        monitoring and debugging the prefetcher's performance.\n\n        Returns:\n            dict: Performance statistics dictionary containing:\n                - total_cells: Total number of cells processed\n                - elapsed_time: Total time since prefetcher started (seconds)\n                - cells_per_sec: Average loading rate (cells per second)\n                - queue_size: Current number of batches in queue\n                - queue_full: Whether the queue is at maximum capacity\n                - total_process_time: Total time spent processing batches\n                - process_count: Number of batches processed\n                - avg_process_time_ms: Average processing time per batch (ms)\n                - current_epoch: Current epoch number\n                - n_epochs: Total number of epochs configured\n\n        Examples:\n            &gt;&gt;&gt; # Get performance statistics\n            &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n            &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n            &gt;&gt;&gt; prefetcher.start()\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Wait for some processing\n            &gt;&gt;&gt; import time\n            &gt;&gt;&gt; time.sleep(5)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Get and display stats\n            &gt;&gt;&gt; stats = prefetcher.get_stats()\n            &gt;&gt;&gt; print(f\"Loading rate: {stats['cells_per_sec']:.1f} cells/sec\")\n            &gt;&gt;&gt; print(f\"Queue size: {stats['queue_size']}/{prefetcher.max_queue_size}\")\n            &gt;&gt;&gt; print(f\"Current epoch: {stats['current_epoch']}/{stats['n_epochs']}\")\n            Loading rate: 1250.5 cells/sec\n            Queue size: 45/500\n            Current epoch: 0/1\n\n            &gt;&gt;&gt; # Monitor performance over time\n            &gt;&gt;&gt; for i in range(3):\n            ...     stats = prefetcher.get_stats()\n            ...     print(f\"Check {i+1}: {stats['cells_per_sec']:.1f} cells/sec\")\n            ...     time.sleep(2)\n            Check 1: 1200.3 cells/sec\n            Check 2: 1250.5 cells/sec\n            Check 3: 1180.7 cells/sec\n        \"\"\"\n        elapsed = time.time() - (self.start_time or 0)\n        rate = self.total_cells_added / elapsed if elapsed &gt; 0 else 0\n        avg_process_time = (\n            self.total_process_time / self.process_count\n            if self.process_count &gt; 0\n            else 0\n        )\n        return {\n            \"total_cells\": self.total_cells_added,\n            \"elapsed_time\": elapsed,\n            \"cells_per_sec\": rate,\n            \"queue_size\": self.queue.qsize(),\n            \"queue_full\": self.queue.full(),\n            \"total_process_time\": self.total_process_time,\n            \"process_count\": self.process_count,\n            \"avg_process_time_ms\": avg_process_time * 1000,\n            \"current_epoch\": self.current_epoch,\n            \"n_epochs\": self.batch_processor.n_epochs,\n        }\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher.__init__","title":"<code>__init__(batch_processor: TileDBBatchProcessor, max_queue_size: int = 500)</code>","text":"<p>Initialize the TileDB async prefetcher with background processing.</p> <p>Parameters:</p> Name Type Description Default <code>batch_processor</code> <code>TileDBBatchProcessor</code> <p>TileDBBatchProcessor instance to use for loading batches.            Must be properly initialized with TileDB path and configuration.</p> required <code>max_queue_size</code> <code>int</code> <p>Maximum number of pre-processed batches to keep in queue.           Higher values use more memory but provide better buffering.           Range: 10-10000, default: 500.</p> <code>500</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create prefetcher with default queue size\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n&gt;&gt;&gt; print(f\"Max queue size: {prefetcher.max_queue_size}\")\nMax queue size: 500\n</code></pre> <pre><code>&gt;&gt;&gt; # Create prefetcher with custom queue size\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor, max_queue_size=1000)\n&gt;&gt;&gt; print(f\"Custom queue size: {prefetcher.max_queue_size}\")\nCustom queue size: 1000\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def __init__(\n    self, batch_processor: TileDBBatchProcessor, max_queue_size: int = 500\n):\n    \"\"\"\n    Initialize the TileDB async prefetcher with background processing.\n\n    Args:\n        batch_processor: TileDBBatchProcessor instance to use for loading batches.\n                       Must be properly initialized with TileDB path and configuration.\n        max_queue_size: Maximum number of pre-processed batches to keep in queue.\n                      Higher values use more memory but provide better buffering.\n                      Range: 10-10000, default: 500.\n\n    Examples:\n        &gt;&gt;&gt; # Create prefetcher with default queue size\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n        &gt;&gt;&gt; print(f\"Max queue size: {prefetcher.max_queue_size}\")\n        Max queue size: 500\n\n        &gt;&gt;&gt; # Create prefetcher with custom queue size\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor, max_queue_size=1000)\n        &gt;&gt;&gt; print(f\"Custom queue size: {prefetcher.max_queue_size}\")\n        Custom queue size: 1000\n    \"\"\"\n    self.batch_processor = batch_processor\n    self.max_queue_size = max_queue_size\n    self.queue: Queue[TileDBPrefetchBatch] = Queue(maxsize=max_queue_size)\n    self.worker_thread = None\n    self.should_stop = False\n\n    # Monitoring stats\n    self.total_cells_added = 0\n    self.start_time = None\n    self.last_rate_print = 0\n    self.total_process_time = 0.0\n    self.process_count = 0\n    self.current_epoch = 0\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher.start","title":"<code>start()</code>","text":"<p>Start the prefetching worker thread for background batch processing.</p> <p>This method starts a background worker thread that continuously loads and processes batches from the TileDB batch processor. The worker thread runs as a daemon thread and will automatically stop when the main process exits.</p> <p>The prefetcher will begin loading batches immediately after starting. Use get_batch() to retrieve pre-processed batches from the queue.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Start background prefetching\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n&gt;&gt;&gt; prefetcher.start()\n&gt;&gt;&gt; print(\"Prefetcher started\")\nPrefetcher started\n</code></pre> <pre><code>&gt;&gt;&gt; # Check if prefetcher is ready\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.sleep(1)  # Wait for first batch\n&gt;&gt;&gt; if prefetcher.has_batch():\n...     print(\"Prefetcher is ready with data\")\n... else:\n...     print(\"Prefetcher not ready yet\")\nPrefetcher is ready with data\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def start(self):\n    \"\"\"\n    Start the prefetching worker thread for background batch processing.\n\n    This method starts a background worker thread that continuously loads\n    and processes batches from the TileDB batch processor. The worker thread\n    runs as a daemon thread and will automatically stop when the main\n    process exits.\n\n    The prefetcher will begin loading batches immediately after starting.\n    Use get_batch() to retrieve pre-processed batches from the queue.\n\n    Examples:\n        &gt;&gt;&gt; # Start background prefetching\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n        &gt;&gt;&gt; prefetcher.start()\n        &gt;&gt;&gt; print(\"Prefetcher started\")\n        Prefetcher started\n\n        &gt;&gt;&gt; # Check if prefetcher is ready\n        &gt;&gt;&gt; import time\n        &gt;&gt;&gt; time.sleep(1)  # Wait for first batch\n        &gt;&gt;&gt; if prefetcher.has_batch():\n        ...     print(\"Prefetcher is ready with data\")\n        ... else:\n        ...     print(\"Prefetcher not ready yet\")\n        Prefetcher is ready with data\n    \"\"\"\n    if self.worker_thread is None or not self.worker_thread.is_alive():\n        self.should_stop = False\n        self.start_time = time.time()\n        self.worker_thread = threading.Thread(\n            target=self._prefetch_worker, daemon=True\n        )\n        self.worker_thread.start()\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher.stop","title":"<code>stop()</code>","text":"<p>Stop the prefetching worker thread and clean up resources.</p> <p>This method gracefully stops the background worker thread and waits for it to finish processing the current batch. It sets the stop flag and joins the thread with a timeout to prevent hanging.</p> <p>After calling stop(), the prefetcher will no longer load new batches. Any remaining batches in the queue can still be retrieved with get_batch().</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Stop the prefetcher\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n&gt;&gt;&gt; prefetcher.start()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Do some work...\n&gt;&gt;&gt; batch = prefetcher.get_batch()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Stop when done\n&gt;&gt;&gt; prefetcher.stop()\n&gt;&gt;&gt; print(\"Prefetcher stopped\")\nPrefetcher stopped\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def stop(self):\n    \"\"\"\n    Stop the prefetching worker thread and clean up resources.\n\n    This method gracefully stops the background worker thread and waits\n    for it to finish processing the current batch. It sets the stop flag\n    and joins the thread with a timeout to prevent hanging.\n\n    After calling stop(), the prefetcher will no longer load new batches.\n    Any remaining batches in the queue can still be retrieved with get_batch().\n\n    Examples:\n        &gt;&gt;&gt; # Stop the prefetcher\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n        &gt;&gt;&gt; prefetcher.start()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Do some work...\n        &gt;&gt;&gt; batch = prefetcher.get_batch()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Stop when done\n        &gt;&gt;&gt; prefetcher.stop()\n        &gt;&gt;&gt; print(\"Prefetcher stopped\")\n        Prefetcher stopped\n    \"\"\"\n    self.should_stop = True\n    if self.worker_thread and self.worker_thread.is_alive():\n        self.worker_thread.join(timeout=1.0)\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher.get_batch","title":"<code>get_batch() -&gt; TileDBPrefetchBatch | None</code>","text":"<p>Get the next pre-processed batch from the queue.</p> <p>This method retrieves a pre-processed batch from the internal queue. If no batch is available, it returns None. The method has a timeout to prevent blocking indefinitely.</p> <p>Returns:</p> Type Description <code>TileDBPrefetchBatch | None</code> <p>TileDBPrefetchBatch | None: The next available batch, or None if                        no batch is available within the timeout.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get a batch from the prefetcher\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n&gt;&gt;&gt; prefetcher.start()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Wait for a batch\n&gt;&gt;&gt; batch = prefetcher.get_batch()\n&gt;&gt;&gt; if batch:\n...     print(f\"Got batch {batch.batch_id} with {len(batch.cell_integer_ids)} cells\")\n... else:\n...     print(\"No batch available\")\nGot batch 0 with 100 cells\n</code></pre> <pre><code>&gt;&gt;&gt; # Check for multiple batches\n&gt;&gt;&gt; batches = []\n&gt;&gt;&gt; for _ in range(3):\n...     batch = prefetcher.get_batch()\n...     if batch:\n...         batches.append(batch)\n...     else:\n...         break\n&gt;&gt;&gt; print(f\"Retrieved {len(batches)} batches\")\nRetrieved 3 batches\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def get_batch(self) -&gt; TileDBPrefetchBatch | None:\n    \"\"\"\n    Get the next pre-processed batch from the queue.\n\n    This method retrieves a pre-processed batch from the internal queue.\n    If no batch is available, it returns None. The method has a timeout\n    to prevent blocking indefinitely.\n\n    Returns:\n        TileDBPrefetchBatch | None: The next available batch, or None if\n                                   no batch is available within the timeout.\n\n    Examples:\n        &gt;&gt;&gt; # Get a batch from the prefetcher\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n        &gt;&gt;&gt; prefetcher.start()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Wait for a batch\n        &gt;&gt;&gt; batch = prefetcher.get_batch()\n        &gt;&gt;&gt; if batch:\n        ...     print(f\"Got batch {batch.batch_id} with {len(batch.cell_integer_ids)} cells\")\n        ... else:\n        ...     print(\"No batch available\")\n        Got batch 0 with 100 cells\n\n        &gt;&gt;&gt; # Check for multiple batches\n        &gt;&gt;&gt; batches = []\n        &gt;&gt;&gt; for _ in range(3):\n        ...     batch = prefetcher.get_batch()\n        ...     if batch:\n        ...         batches.append(batch)\n        ...     else:\n        ...         break\n        &gt;&gt;&gt; print(f\"Retrieved {len(batches)} batches\")\n        Retrieved 3 batches\n    \"\"\"\n    try:\n        return self.queue.get(timeout=1.0)\n    except queue.Empty:\n        return None\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher.has_batch","title":"<code>has_batch() -&gt; bool</code>","text":"<p>Check if a pre-processed batch is available in the queue.</p> <p>This method provides a non-blocking way to check if batches are available for immediate retrieval. It returns True if the queue contains at least one batch, False otherwise.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if at least one batch is available, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Check for available batches\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n&gt;&gt;&gt; prefetcher.start()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check if batches are ready\n&gt;&gt;&gt; if prefetcher.has_batch():\n...     batch = prefetcher.get_batch()\n...     print(f\"Processing batch {batch.batch_id}\")\n... else:\n...     print(\"No batches ready yet\")\nNo batches ready yet\n</code></pre> <pre><code>&gt;&gt;&gt; # Wait and check again\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.sleep(2)\n&gt;&gt;&gt; if prefetcher.has_batch():\n...     print(\"Batches are now available\")\n... else:\n...     print(\"Still waiting for batches\")\nBatches are now available\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def has_batch(self) -&gt; bool:\n    \"\"\"\n    Check if a pre-processed batch is available in the queue.\n\n    This method provides a non-blocking way to check if batches are\n    available for immediate retrieval. It returns True if the queue\n    contains at least one batch, False otherwise.\n\n    Returns:\n        bool: True if at least one batch is available, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; # Check for available batches\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n        &gt;&gt;&gt; prefetcher.start()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Check if batches are ready\n        &gt;&gt;&gt; if prefetcher.has_batch():\n        ...     batch = prefetcher.get_batch()\n        ...     print(f\"Processing batch {batch.batch_id}\")\n        ... else:\n        ...     print(\"No batches ready yet\")\n        No batches ready yet\n\n        &gt;&gt;&gt; # Wait and check again\n        &gt;&gt;&gt; import time\n        &gt;&gt;&gt; time.sleep(2)\n        &gt;&gt;&gt; if prefetcher.has_batch():\n        ...     print(\"Batches are now available\")\n        ... else:\n        ...     print(\"Still waiting for batches\")\n        Batches are now available\n    \"\"\"\n    return not self.queue.empty()\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBAsyncPrefetcher.get_stats","title":"<code>get_stats() -&gt; dict</code>","text":"<p>Get comprehensive statistics about the prefetcher's performance.</p> <p>This method returns detailed performance statistics including loading rates, memory usage, queue status, and processing times. Useful for monitoring and debugging the prefetcher's performance.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Performance statistics dictionary containing: - total_cells: Total number of cells processed - elapsed_time: Total time since prefetcher started (seconds) - cells_per_sec: Average loading rate (cells per second) - queue_size: Current number of batches in queue - queue_full: Whether the queue is at maximum capacity - total_process_time: Total time spent processing batches - process_count: Number of batches processed - avg_process_time_ms: Average processing time per batch (ms) - current_epoch: Current epoch number - n_epochs: Total number of epochs configured</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get performance statistics\n&gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n&gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n&gt;&gt;&gt; prefetcher.start()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Wait for some processing\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; time.sleep(5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get and display stats\n&gt;&gt;&gt; stats = prefetcher.get_stats()\n&gt;&gt;&gt; print(f\"Loading rate: {stats['cells_per_sec']:.1f} cells/sec\")\n&gt;&gt;&gt; print(f\"Queue size: {stats['queue_size']}/{prefetcher.max_queue_size}\")\n&gt;&gt;&gt; print(f\"Current epoch: {stats['current_epoch']}/{stats['n_epochs']}\")\nLoading rate: 1250.5 cells/sec\nQueue size: 45/500\nCurrent epoch: 0/1\n</code></pre> <pre><code>&gt;&gt;&gt; # Monitor performance over time\n&gt;&gt;&gt; for i in range(3):\n...     stats = prefetcher.get_stats()\n...     print(f\"Check {i+1}: {stats['cells_per_sec']:.1f} cells/sec\")\n...     time.sleep(2)\nCheck 1: 1200.3 cells/sec\nCheck 2: 1250.5 cells/sec\nCheck 3: 1180.7 cells/sec\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"\n    Get comprehensive statistics about the prefetcher's performance.\n\n    This method returns detailed performance statistics including loading\n    rates, memory usage, queue status, and processing times. Useful for\n    monitoring and debugging the prefetcher's performance.\n\n    Returns:\n        dict: Performance statistics dictionary containing:\n            - total_cells: Total number of cells processed\n            - elapsed_time: Total time since prefetcher started (seconds)\n            - cells_per_sec: Average loading rate (cells per second)\n            - queue_size: Current number of batches in queue\n            - queue_full: Whether the queue is at maximum capacity\n            - total_process_time: Total time spent processing batches\n            - process_count: Number of batches processed\n            - avg_process_time_ms: Average processing time per batch (ms)\n            - current_epoch: Current epoch number\n            - n_epochs: Total number of epochs configured\n\n    Examples:\n        &gt;&gt;&gt; # Get performance statistics\n        &gt;&gt;&gt; processor = TileDBBatchProcessor(\"path/to/experiment\")\n        &gt;&gt;&gt; prefetcher = TileDBAsyncPrefetcher(processor)\n        &gt;&gt;&gt; prefetcher.start()\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Wait for some processing\n        &gt;&gt;&gt; import time\n        &gt;&gt;&gt; time.sleep(5)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get and display stats\n        &gt;&gt;&gt; stats = prefetcher.get_stats()\n        &gt;&gt;&gt; print(f\"Loading rate: {stats['cells_per_sec']:.1f} cells/sec\")\n        &gt;&gt;&gt; print(f\"Queue size: {stats['queue_size']}/{prefetcher.max_queue_size}\")\n        &gt;&gt;&gt; print(f\"Current epoch: {stats['current_epoch']}/{stats['n_epochs']}\")\n        Loading rate: 1250.5 cells/sec\n        Queue size: 45/500\n        Current epoch: 0/1\n\n        &gt;&gt;&gt; # Monitor performance over time\n        &gt;&gt;&gt; for i in range(3):\n        ...     stats = prefetcher.get_stats()\n        ...     print(f\"Check {i+1}: {stats['cells_per_sec']:.1f} cells/sec\")\n        ...     time.sleep(2)\n        Check 1: 1200.3 cells/sec\n        Check 2: 1250.5 cells/sec\n        Check 3: 1180.7 cells/sec\n    \"\"\"\n    elapsed = time.time() - (self.start_time or 0)\n    rate = self.total_cells_added / elapsed if elapsed &gt; 0 else 0\n    avg_process_time = (\n        self.total_process_time / self.process_count\n        if self.process_count &gt; 0\n        else 0\n    )\n    return {\n        \"total_cells\": self.total_cells_added,\n        \"elapsed_time\": elapsed,\n        \"cells_per_sec\": rate,\n        \"queue_size\": self.queue.qsize(),\n        \"queue_full\": self.queue.full(),\n        \"total_process_time\": self.total_process_time,\n        \"process_count\": self.process_count,\n        \"avg_process_time_ms\": avg_process_time * 1000,\n        \"current_epoch\": self.current_epoch,\n        \"n_epochs\": self.batch_processor.n_epochs,\n    }\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBIterableDataset","title":"<code>TileDBIterableDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>PyTorch IterableDataset for streaming TileDB SOMA data with async prefetching.</p> <p>TileDBIterableDataset provides a PyTorch-compatible interface for streaming single-cell data from TileDB SOMA format. It combines the TileDBBatchProcessor and TileDBAsyncPrefetcher to provide efficient, asynchronous data loading for machine learning training.</p> Key Features <ul> <li>PyTorch IterableDataset compatibility</li> <li>Asynchronous background prefetching for improved throughput</li> <li>Multiple loading strategies (MoS and sequential)</li> <li>Multi-epoch training support</li> <li>Automatic epoch transition handling</li> <li>Memory-efficient streaming</li> <li>Comprehensive error handling</li> <li>Configurable batch and prefetch sizes</li> </ul> <p>The dataset automatically manages background prefetching and provides seamless iteration over batches of TileDB data. It handles epoch transitions and provides detailed timing information for performance monitoring.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create dataset with default MoS strategy\n&gt;&gt;&gt; dataset = TileDBIterableDataset(\n...     tiledb_path=\"path/to/experiment\",\n...     batch_size=32,\n...     prefetch_batch_size=100\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Iterate through batches\n&gt;&gt;&gt; for batch in dataset:\n...     print(f\"Batch keys: {list(batch.keys())}\")\n...     print(f\"Cell IDs: {batch['cell_ids']}\")\n...     break\n</code></pre> <pre><code>Batch keys: ['X', 'cell_ids']\nCell IDs: [0, 1, 2, ..., 29, 30, 31]\n\n&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; dataset = TileDBIterableDataset(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False,\n...     batch_size=64\n... )\n&gt;&gt;&gt; print(f\"MoS enabled: {dataset.use_mixture_of_scanners}\")\nMoS enabled: False\n\n&gt;&gt;&gt; # Multi-epoch training\n&gt;&gt;&gt; dataset = TileDBIterableDataset(\n...     tiledb_path=\"path/to/experiment\",\n...     n_epochs=3\n... )\n&gt;&gt;&gt; epochs_seen = set()\n&gt;&gt;&gt; for batch in dataset:\n...     if 'epoch' in batch:\n...         epochs_seen.add(batch['epoch'])\n...     if len(epochs_seen) &gt;= 3:\n...         break\n&gt;&gt;&gt; print(f\"Epochs completed: {sorted(epochs_seen)}\")\nEpochs completed: [0, 1, 2]\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>class TileDBIterableDataset(IterableDataset):\n    \"\"\"\n    PyTorch IterableDataset for streaming TileDB SOMA data with async prefetching.\n\n    TileDBIterableDataset provides a PyTorch-compatible interface for streaming\n    single-cell data from TileDB SOMA format. It combines the TileDBBatchProcessor\n    and TileDBAsyncPrefetcher to provide efficient, asynchronous data loading\n    for machine learning training.\n\n    Key Features:\n        - PyTorch IterableDataset compatibility\n        - Asynchronous background prefetching for improved throughput\n        - Multiple loading strategies (MoS and sequential)\n        - Multi-epoch training support\n        - Automatic epoch transition handling\n        - Memory-efficient streaming\n        - Comprehensive error handling\n        - Configurable batch and prefetch sizes\n\n    The dataset automatically manages background prefetching and provides\n    seamless iteration over batches of TileDB data. It handles epoch\n    transitions and provides detailed timing information for performance\n    monitoring.\n\n    Examples:\n            &gt;&gt;&gt; # Create dataset with default MoS strategy\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     batch_size=32,\n            ...     prefetch_batch_size=100\n            ... )\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Iterate through batches\n            &gt;&gt;&gt; for batch in dataset:\n            ...     print(f\"Batch keys: {list(batch.keys())}\")\n            ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n            ...     break\n        Batch keys: ['X', 'cell_ids']\n        Cell IDs: [0, 1, 2, ..., 29, 30, 31]\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; dataset = TileDBIterableDataset(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False,\n        ...     batch_size=64\n        ... )\n        &gt;&gt;&gt; print(f\"MoS enabled: {dataset.use_mixture_of_scanners}\")\n        MoS enabled: False\n\n        &gt;&gt;&gt; # Multi-epoch training\n        &gt;&gt;&gt; dataset = TileDBIterableDataset(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_epochs=3\n        ... )\n        &gt;&gt;&gt; epochs_seen = set()\n        &gt;&gt;&gt; for batch in dataset:\n        ...     if 'epoch' in batch:\n        ...         epochs_seen.add(batch['epoch'])\n        ...     if len(epochs_seen) &gt;= 3:\n        ...         break\n        &gt;&gt;&gt; print(f\"Epochs completed: {sorted(epochs_seen)}\")\n        Epochs completed: [0, 1, 2]\n    \"\"\"\n\n    def __init__(\n        self,\n        tiledb_path: str,\n        batch_size: int = 32,\n        prefetch_batch_size: int = 100,\n        seed: int = 42,\n        max_queue_size: int = 500,\n        n_epochs: int = 1,\n        verbose: bool = True,\n        use_mixture_of_scanners: bool = True,\n        n_readers: int = 50,\n        n_scanners: int = 8,\n    ):\n        \"\"\"\n        Initialize the TileDB IterableDataset with async prefetching.\n\n        Args:\n            tiledb_path: Path to the TileDB SOMA experiment directory.\n                         Must contain a valid SOMA experiment with RNA measurement data.\n            batch_size: Number of cells per training batch. Larger batches use more\n                       memory but may improve training efficiency. Range: 1-512, default: 32.\n            prefetch_batch_size: Number of cells to load per prefetch batch from TileDB.\n                               Higher values improve throughput but use more memory.\n                               Range: 10-10000, default: 100.\n            seed: Random seed for reproducible shuffling and MoS sampling.\n                  Used for consistent data ordering across runs. Default: 42.\n            max_queue_size: Maximum number of pre-processed batches to keep in queue.\n                          Higher values use more memory but provide better buffering.\n                          Range: 10-10000, default: 500.\n            n_epochs: Number of epochs to run. The dataset will automatically reset\n                     after each epoch, enabling multi-epoch training. Default: 1.\n            verbose: If True, print detailed timing and progress information.\n                    If False, suppress all internal prints for clean output. Default: True.\n            use_mixture_of_scanners: If True, use MoS strategy for higher entropy by\n                                   randomly sampling from multiple fragment generators.\n                                   Provides better randomization for foundation model training.\n                                   Default: True.\n            n_readers: Total number of fragment generators to create when using MoS.\n                      Higher values provide better entropy but use more memory.\n                      Range: 1-1000, default: 50.\n            n_scanners: Number of active scanners to sample from simultaneously when using MoS.\n                       Higher values provide better entropy but use more memory.\n                       Range: 1-100, default: 8.\n\n        Raises:\n            ImportError: If TileDB SOMA is not available.\n            ValueError: If MoS parameters are invalid.\n            RuntimeError: If the TileDB experiment cannot be opened or is invalid.\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization with default MoS strategy\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     batch_size=32,\n            ...     prefetch_batch_size=100\n            ... )\n            &gt;&gt;&gt; print(f\"MoS enabled: {dataset.use_mixture_of_scanners}\")\n            MoS enabled: True\n\n            &gt;&gt;&gt; # Sequential loading for maximum throughput\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     use_mixture_of_scanners=False,\n            ...     batch_size=64\n            ... )\n            &gt;&gt;&gt; print(f\"Sequential loading: {not dataset.use_mixture_of_scanners}\")\n            Sequential loading: True\n\n            &gt;&gt;&gt; # High-entropy MoS configuration\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     n_readers=100,\n            ...     n_scanners=16\n            ... )\n            &gt;&gt;&gt; print(f\"MoS readers: {dataset.n_readers}, scanners: {dataset.n_scanners}\")\n            MoS readers: 100, scanners: 16\n        \"\"\"\n        super().__init__()\n        self.tiledb_path = tiledb_path\n        self.batch_size = batch_size\n        self.prefetch_batch_size = prefetch_batch_size\n        self.seed = seed\n        self.max_queue_size = max_queue_size\n        self.n_epochs = n_epochs\n        self.verbose = verbose\n        self.use_mixture_of_scanners = use_mixture_of_scanners\n        self.n_readers = n_readers\n        self.n_scanners = n_scanners\n\n        # Initialize batch processor\n        self.batch_processor = TileDBBatchProcessor(\n            tiledb_path=tiledb_path,\n            batch_size=batch_size,\n            prefetch_batch_size=prefetch_batch_size,\n            seed=seed,\n            n_epochs=n_epochs,\n            verbose=verbose,\n            log_metrics=False,\n            use_mixture_of_scanners=use_mixture_of_scanners,\n            n_readers=n_readers,\n            n_scanners=n_scanners,\n        )\n\n        # Initialize async prefetcher\n        self.prefetcher = TileDBAsyncPrefetcher(\n            batch_processor=self.batch_processor,\n            max_queue_size=max_queue_size,\n        )\n\n        # Start async prefetching\n        self.prefetcher.start()\n\n        # Wait for prefetcher to initialize\n        self._wait_for_prefetcher_ready()\n\n    def _wait_for_prefetcher_ready(self, timeout: float = 10.0):\n        \"\"\"Wait for the prefetcher to be ready with data.\"\"\"\n        start_time = time.time()\n        while time.time() - start_time &lt; timeout:\n            if self.prefetcher.has_batch():\n                if self.verbose:\n                    print(\n                        f\"\u2705 TileDB prefetcher ready after {time.time() - start_time:.2f}s\"\n                    )\n                return\n            time.sleep(0.1)\n\n        if self.verbose:\n            print(\n                f\"\u26a0\ufe0f TileDB prefetcher not ready after {timeout}s, proceeding anyway...\"\n            )\n\n    def __iter__(self) -&gt; Iterator[dict]:\n        \"\"\"\n        Iterate through batches of TileDB data with async prefetching.\n\n        This method provides an iterator over batches of TileDB data, automatically\n        handling background prefetching, epoch transitions, and error recovery.\n        It yields dictionaries containing the batch data and metadata.\n\n        The iterator automatically manages:\n        - Background prefetching for improved throughput\n        - Epoch transitions for multi-epoch training\n        - Error handling and recovery\n        - Performance monitoring and reporting\n\n        Yields:\n            dict: Batch dictionary containing:\n                - X: Polars DataFrame with cell-gene expression data\n                - cell_ids: List of unique cell IDs in the batch\n                - epoch: Current epoch number (when n_epochs &gt; 1)\n\n        Examples:\n            &gt;&gt;&gt; # Basic iteration\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\"path/to/experiment\")\n            &gt;&gt;&gt; for batch in dataset:\n            ...     print(f\"Batch keys: {list(batch.keys())}\")\n            ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n            ...     break\n            Batch keys: ['X', 'cell_ids']\n            Cell IDs: [0, 1, 2, ..., 29, 30, 31]\n\n            &gt;&gt;&gt; # Multi-epoch training\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\"path/to/experiment\", n_epochs=3)\n            &gt;&gt;&gt; epochs_seen = set()\n            &gt;&gt;&gt; for batch in dataset:\n            ...     if 'epoch' in batch:\n            ...         epochs_seen.add(batch['epoch'])\n            ...     if len(epochs_seen) &gt;= 3:\n            ...         break\n            &gt;&gt;&gt; print(f\"Epochs completed: {sorted(epochs_seen)}\")\n            Epochs completed: [0, 1, 2]\n\n            &gt;&gt;&gt; # Training loop with error handling\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\"path/to/experiment\")\n            &gt;&gt;&gt; for batch_idx, batch in enumerate(dataset):\n            ...     try:\n            ...         x = batch[\"X\"]\n            ...         cell_ids = batch[\"cell_ids\"]\n            ...         print(f\"Processed batch {batch_idx} with {len(cell_ids)} cells\")\n            ...     except Exception as e:\n            ...         print(f\"Error in batch {batch_idx}: {e}\")\n            ...         continue\n            ...     if batch_idx &gt;= 2:  # Just first few batches\n            ...         break\n            Processed batch 0 with 32 cells\n            Processed batch 1 with 32 cells\n            Processed batch 2 with 32 cells\n        \"\"\"\n        batches_yielded = 0\n        current_epoch = 0\n        last_epoch = -1\n\n        while True:\n            # Get data from prefetcher\n            data_start = time.time()\n            data = self.prefetcher.get_batch()\n            data_time = time.time() - data_start\n\n            if data is None:\n                # Check if prefetcher has finished all epochs\n                stats = self.prefetcher.get_stats()\n                if stats[\"current_epoch\"] &gt;= stats[\"n_epochs\"]:\n                    if self.verbose:\n                        print(\n                            f\"\u2705 Dataset iteration complete: all {stats['n_epochs']} epochs finished\"\n                        )\n                    break\n\n                # Wait for more data with timeout\n                wait_start = time.time()\n                while not self.prefetcher.has_batch():\n                    time.sleep(0.1)\n                    # Timeout after 5 seconds to avoid infinite wait\n                    if time.time() - wait_start &gt; 5.0:\n                        if self.verbose:\n                            print(\"\u26a0\ufe0f Timeout waiting for prefetcher data\")\n                        break\n\n                data = self.prefetcher.get_batch()\n                if data is None:\n                    # Double-check if prefetcher is done\n                    stats = self.prefetcher.get_stats()\n                    if stats[\"current_epoch\"] &gt;= stats[\"n_epochs\"]:\n                        if self.verbose:\n                            print(\n                                f\"\u2705 Dataset iteration complete: all {stats['n_epochs']} epochs finished\"\n                            )\n                        break\n                    else:\n                        if self.verbose:\n                            print(\"\u26a0\ufe0f No data available from prefetcher\")\n                        break\n\n            # Track epoch transitions\n            current_epoch = self.batch_processor.current_epoch\n            if current_epoch != last_epoch:\n                if self.verbose:\n                    print(\n                        f\"\ud83d\udd04 Epoch transition detected: {last_epoch} -&gt; {current_epoch}\"\n                    )\n                last_epoch = current_epoch\n\n            # Process batch data\n            batch_df = data.batch_df\n\n            # Time the overall batch processing\n            batch_start_time = time.time()\n\n            # Get unique cell IDs in this batch\n            batch_cell_ids = batch_df[\"cell_integer_id\"].unique().to_list()\n\n            # Calculate total batch processing time\n            total_batch_time = time.time() - batch_start_time\n\n            # Create batch dictionary\n            batch_dict = {\n                \"X\": batch_df,  # Polars DataFrame with CSR-like structure\n                \"cell_ids\": batch_cell_ids,\n            }\n\n            # Add epoch info if multi-epoch training\n            if self.n_epochs &gt; 1:\n                batch_dict[\"epoch\"] = current_epoch\n\n            batches_yielded += 1\n\n            # Print detailed timing every 100 batches\n            if batches_yielded % 100 == 0:\n                # Consolidate training batch reporting\n                training_report = f\"  TileDB training batch {batches_yielded} (epoch {current_epoch}) processing:\\n\"\n                training_report += f\"     Data retrieval: {data_time * 1000:.1f}ms\\n\"\n                training_report += (\n                    f\"     Total batch time: {total_batch_time * 1000:.1f}ms\\n\"\n                )\n                training_report += \"     Raw data (polars DataFrame)\"\n\n                print_training(training_report, self.verbose)\n\n            yield batch_dict\n\n    def __del__(self):\n        \"\"\"\n        Cleanup when dataset is destroyed.\n\n        This method is called when the dataset object is garbage collected.\n        It ensures that the underlying prefetcher is properly stopped to\n        prevent resource leaks and background thread issues.\n\n        Examples:\n            &gt;&gt;&gt; # Dataset cleanup happens automatically\n            &gt;&gt;&gt; dataset = TileDBIterableDataset(\"path/to/experiment\")\n            &gt;&gt;&gt; print(\"Dataset created\")\n            Dataset created\n            &gt;&gt;&gt; # When dataset goes out of scope, __del__ is called automatically\n            &gt;&gt;&gt; del dataset\n            &gt;&gt;&gt; print(\"Dataset destroyed and cleaned up\")\n            Dataset destroyed and cleaned up\n        \"\"\"\n        self.prefetcher.stop()\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBIterableDataset-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBIterableDataset.__init__","title":"<code>__init__(tiledb_path: str, batch_size: int = 32, prefetch_batch_size: int = 100, seed: int = 42, max_queue_size: int = 500, n_epochs: int = 1, verbose: bool = True, use_mixture_of_scanners: bool = True, n_readers: int = 50, n_scanners: int = 8)</code>","text":"<p>Initialize the TileDB IterableDataset with async prefetching.</p> <p>Parameters:</p> Name Type Description Default <code>tiledb_path</code> <code>str</code> <p>Path to the TileDB SOMA experiment directory.          Must contain a valid SOMA experiment with RNA measurement data.</p> required <code>batch_size</code> <code>int</code> <p>Number of cells per training batch. Larger batches use more        memory but may improve training efficiency. Range: 1-512, default: 32.</p> <code>32</code> <code>prefetch_batch_size</code> <code>int</code> <p>Number of cells to load per prefetch batch from TileDB.                Higher values improve throughput but use more memory.                Range: 10-10000, default: 100.</p> <code>100</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible shuffling and MoS sampling.   Used for consistent data ordering across runs. Default: 42.</p> <code>42</code> <code>max_queue_size</code> <code>int</code> <p>Maximum number of pre-processed batches to keep in queue.           Higher values use more memory but provide better buffering.           Range: 10-10000, default: 500.</p> <code>500</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs to run. The dataset will automatically reset      after each epoch, enabling multi-epoch training. Default: 1.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed timing and progress information.     If False, suppress all internal prints for clean output. Default: True.</p> <code>True</code> <code>use_mixture_of_scanners</code> <code>bool</code> <p>If True, use MoS strategy for higher entropy by                    randomly sampling from multiple fragment generators.                    Provides better randomization for foundation model training.                    Default: True.</p> <code>True</code> <code>n_readers</code> <code>int</code> <p>Total number of fragment generators to create when using MoS.       Higher values provide better entropy but use more memory.       Range: 1-1000, default: 50.</p> <code>50</code> <code>n_scanners</code> <code>int</code> <p>Number of active scanners to sample from simultaneously when using MoS.        Higher values provide better entropy but use more memory.        Range: 1-100, default: 8.</p> <code>8</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If TileDB SOMA is not available.</p> <code>ValueError</code> <p>If MoS parameters are invalid.</p> <code>RuntimeError</code> <p>If the TileDB experiment cannot be opened or is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization with default MoS strategy\n&gt;&gt;&gt; dataset = TileDBIterableDataset(\n...     tiledb_path=\"path/to/experiment\",\n...     batch_size=32,\n...     prefetch_batch_size=100\n... )\n&gt;&gt;&gt; print(f\"MoS enabled: {dataset.use_mixture_of_scanners}\")\nMoS enabled: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; dataset = TileDBIterableDataset(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False,\n...     batch_size=64\n... )\n&gt;&gt;&gt; print(f\"Sequential loading: {not dataset.use_mixture_of_scanners}\")\nSequential loading: True\n</code></pre> <pre><code>&gt;&gt;&gt; # High-entropy MoS configuration\n&gt;&gt;&gt; dataset = TileDBIterableDataset(\n...     tiledb_path=\"path/to/experiment\",\n...     n_readers=100,\n...     n_scanners=16\n... )\n&gt;&gt;&gt; print(f\"MoS readers: {dataset.n_readers}, scanners: {dataset.n_scanners}\")\nMoS readers: 100, scanners: 16\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def __init__(\n    self,\n    tiledb_path: str,\n    batch_size: int = 32,\n    prefetch_batch_size: int = 100,\n    seed: int = 42,\n    max_queue_size: int = 500,\n    n_epochs: int = 1,\n    verbose: bool = True,\n    use_mixture_of_scanners: bool = True,\n    n_readers: int = 50,\n    n_scanners: int = 8,\n):\n    \"\"\"\n    Initialize the TileDB IterableDataset with async prefetching.\n\n    Args:\n        tiledb_path: Path to the TileDB SOMA experiment directory.\n                     Must contain a valid SOMA experiment with RNA measurement data.\n        batch_size: Number of cells per training batch. Larger batches use more\n                   memory but may improve training efficiency. Range: 1-512, default: 32.\n        prefetch_batch_size: Number of cells to load per prefetch batch from TileDB.\n                           Higher values improve throughput but use more memory.\n                           Range: 10-10000, default: 100.\n        seed: Random seed for reproducible shuffling and MoS sampling.\n              Used for consistent data ordering across runs. Default: 42.\n        max_queue_size: Maximum number of pre-processed batches to keep in queue.\n                      Higher values use more memory but provide better buffering.\n                      Range: 10-10000, default: 500.\n        n_epochs: Number of epochs to run. The dataset will automatically reset\n                 after each epoch, enabling multi-epoch training. Default: 1.\n        verbose: If True, print detailed timing and progress information.\n                If False, suppress all internal prints for clean output. Default: True.\n        use_mixture_of_scanners: If True, use MoS strategy for higher entropy by\n                               randomly sampling from multiple fragment generators.\n                               Provides better randomization for foundation model training.\n                               Default: True.\n        n_readers: Total number of fragment generators to create when using MoS.\n                  Higher values provide better entropy but use more memory.\n                  Range: 1-1000, default: 50.\n        n_scanners: Number of active scanners to sample from simultaneously when using MoS.\n                   Higher values provide better entropy but use more memory.\n                   Range: 1-100, default: 8.\n\n    Raises:\n        ImportError: If TileDB SOMA is not available.\n        ValueError: If MoS parameters are invalid.\n        RuntimeError: If the TileDB experiment cannot be opened or is invalid.\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization with default MoS strategy\n        &gt;&gt;&gt; dataset = TileDBIterableDataset(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     batch_size=32,\n        ...     prefetch_batch_size=100\n        ... )\n        &gt;&gt;&gt; print(f\"MoS enabled: {dataset.use_mixture_of_scanners}\")\n        MoS enabled: True\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; dataset = TileDBIterableDataset(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False,\n        ...     batch_size=64\n        ... )\n        &gt;&gt;&gt; print(f\"Sequential loading: {not dataset.use_mixture_of_scanners}\")\n        Sequential loading: True\n\n        &gt;&gt;&gt; # High-entropy MoS configuration\n        &gt;&gt;&gt; dataset = TileDBIterableDataset(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_readers=100,\n        ...     n_scanners=16\n        ... )\n        &gt;&gt;&gt; print(f\"MoS readers: {dataset.n_readers}, scanners: {dataset.n_scanners}\")\n        MoS readers: 100, scanners: 16\n    \"\"\"\n    super().__init__()\n    self.tiledb_path = tiledb_path\n    self.batch_size = batch_size\n    self.prefetch_batch_size = prefetch_batch_size\n    self.seed = seed\n    self.max_queue_size = max_queue_size\n    self.n_epochs = n_epochs\n    self.verbose = verbose\n    self.use_mixture_of_scanners = use_mixture_of_scanners\n    self.n_readers = n_readers\n    self.n_scanners = n_scanners\n\n    # Initialize batch processor\n    self.batch_processor = TileDBBatchProcessor(\n        tiledb_path=tiledb_path,\n        batch_size=batch_size,\n        prefetch_batch_size=prefetch_batch_size,\n        seed=seed,\n        n_epochs=n_epochs,\n        verbose=verbose,\n        log_metrics=False,\n        use_mixture_of_scanners=use_mixture_of_scanners,\n        n_readers=n_readers,\n        n_scanners=n_scanners,\n    )\n\n    # Initialize async prefetcher\n    self.prefetcher = TileDBAsyncPrefetcher(\n        batch_processor=self.batch_processor,\n        max_queue_size=max_queue_size,\n    )\n\n    # Start async prefetching\n    self.prefetcher.start()\n\n    # Wait for prefetcher to initialize\n    self._wait_for_prefetcher_ready()\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBDataLoader","title":"<code>TileDBDataLoader</code>","text":"<p>High-performance DataLoader for TileDB SOMA data optimized for ML training.</p> <p>TileDBDataLoader provides efficient streaming of single-cell data from TileDB SOMA format for machine learning applications. It uses async batch processing and provides multiple loading strategies for different use cases.</p> Key Features <ul> <li>Multiple loading strategies for different entropy requirements:<ul> <li>Mixture of Scanners (MoS): Maximum entropy, best randomization (default)</li> <li>Sequential loading: Fastest, lowest entropy</li> </ul> </li> <li>Asynchronous background prefetching for improved throughput</li> <li>Multi-epoch training support with automatic epoch transitions</li> <li>Memory-efficient streaming with configurable batch sizes</li> <li>Comprehensive error handling and validation</li> <li>Performance monitoring and statistics</li> <li>PyTorch IterableDataset compatibility</li> </ul> Loading Strategies <ol> <li>Mixture of Scanners (default): Randomly samples from multiple generators    for maximum entropy and randomization</li> <li>Sequential: Loads contiguous data chunks for maximum throughput</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage with default MoS strategy\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     batch_size=32,\n...     prefetch_batch_size=100\n... )\n&gt;&gt;&gt; for batch in dataloader:\n...     print(f\"Batch keys: {list(batch.keys())}\")\n...     print(f\"Cell IDs: {batch['cell_ids']}\")\n...     break\nBatch keys: ['X', 'cell_ids']\nCell IDs: [0, 1, 2, ..., 29, 30, 31]\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False,\n...     batch_size=64\n... )\n&gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\nMoS enabled: False\n</code></pre> <pre><code>&gt;&gt;&gt; # Multi-epoch training\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     n_epochs=3\n... )\n&gt;&gt;&gt; print(f\"Number of epochs: {dataloader.n_epochs}\")\nNumber of epochs: 3\n</code></pre> <pre><code>&gt;&gt;&gt; # Custom MoS configuration\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     n_readers=100,\n...     n_scanners=16\n... )\n&gt;&gt;&gt; print(f\"MoS readers: {dataloader.n_readers}, scanners: {dataloader.n_scanners}\")\nMoS readers: 100, scanners: 16\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>class TileDBDataLoader:\n    \"\"\"\n    High-performance DataLoader for TileDB SOMA data optimized for ML training.\n\n    TileDBDataLoader provides efficient streaming of single-cell data from TileDB\n    SOMA format for machine learning applications. It uses async batch processing\n    and provides multiple loading strategies for different use cases.\n\n    Key Features:\n        - Multiple loading strategies for different entropy requirements:\n            * Mixture of Scanners (MoS): Maximum entropy, best randomization (default)\n            * Sequential loading: Fastest, lowest entropy\n        - Asynchronous background prefetching for improved throughput\n        - Multi-epoch training support with automatic epoch transitions\n        - Memory-efficient streaming with configurable batch sizes\n        - Comprehensive error handling and validation\n        - Performance monitoring and statistics\n        - PyTorch IterableDataset compatibility\n\n    Loading Strategies:\n        1. Mixture of Scanners (default): Randomly samples from multiple generators\n           for maximum entropy and randomization\n        2. Sequential: Loads contiguous data chunks for maximum throughput\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage with default MoS strategy\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     batch_size=32,\n        ...     prefetch_batch_size=100\n        ... )\n        &gt;&gt;&gt; for batch in dataloader:\n        ...     print(f\"Batch keys: {list(batch.keys())}\")\n        ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n        ...     break\n        Batch keys: ['X', 'cell_ids']\n        Cell IDs: [0, 1, 2, ..., 29, 30, 31]\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False,\n        ...     batch_size=64\n        ... )\n        &gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\n        MoS enabled: False\n\n        &gt;&gt;&gt; # Multi-epoch training\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_epochs=3\n        ... )\n        &gt;&gt;&gt; print(f\"Number of epochs: {dataloader.n_epochs}\")\n        Number of epochs: 3\n\n        &gt;&gt;&gt; # Custom MoS configuration\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_readers=100,\n        ...     n_scanners=16\n        ... )\n        &gt;&gt;&gt; print(f\"MoS readers: {dataloader.n_readers}, scanners: {dataloader.n_scanners}\")\n        MoS readers: 100, scanners: 16\n    \"\"\"\n\n    def __init__(\n        self,\n        tiledb_path: str,\n        batch_size: int = 32,\n        prefetch_batch_size: int = 100,\n        seed: int = 42,\n        n_epochs: int = 1,\n        verbose: bool = True,\n        max_queue_size: int = 500,\n        use_mixture_of_scanners: bool = True,\n        n_readers: int = 50,\n        n_scanners: int = 8,\n    ):\n        \"\"\"\n        Initialize the TileDB DataLoader with training configuration.\n\n        Args:\n            tiledb_path: Path to the TileDB SOMA experiment directory.\n                         Must contain a valid SOMA experiment with RNA measurement data.\n            batch_size: Number of cells per training batch. Larger batches use more\n                       memory but may improve training efficiency. Range: 1-512, default: 32.\n            prefetch_batch_size: Number of cells to prefetch from TileDB per batch.\n                               Higher values improve throughput but use more memory.\n                               Range: 10-10000, default: 100.\n            seed: Random seed for reproducible shuffling and MoS sampling.\n                  Used for consistent data ordering across runs. Default: 42.\n            n_epochs: Number of epochs to run. The dataloader will automatically reset\n                     after each epoch, enabling multi-epoch training. Default: 1.\n            verbose: If True, print detailed timing and progress information.\n                    If False, suppress all internal prints for clean output. Default: True.\n            max_queue_size: Maximum number of pre-processed batches to keep in queue.\n                          Higher values use more memory but provide better buffering.\n                          Range: 10-10000, default: 500.\n            use_mixture_of_scanners: If True, use MoS strategy for higher entropy by\n                                   randomly sampling from multiple fragment generators.\n                                   Provides better randomization for foundation model training.\n                                   Default: True.\n            n_readers: Total number of fragment generators to create when using MoS.\n                      Higher values provide better entropy but use more memory.\n                      Range: 1-1000, default: 50.\n            n_scanners: Number of active scanners to sample from simultaneously when using MoS.\n                       Higher values provide better entropy but use more memory.\n                       Range: 1-100, default: 8.\n\n        Raises:\n            ImportError: If TileDB SOMA is not available.\n            ValueError: If MoS parameters are invalid.\n            RuntimeError: If the TileDB experiment cannot be opened or is invalid.\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization with default MoS strategy\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     batch_size=32,\n            ...     prefetch_batch_size=100\n            ... )\n            &gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\n            MoS enabled: True\n\n            &gt;&gt;&gt; # Sequential loading for maximum throughput\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     use_mixture_of_scanners=False,\n            ...     batch_size=64\n            ... )\n            &gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\n            Sequential loading: True\n\n            &gt;&gt;&gt; # High-entropy MoS configuration\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\n            ...     tiledb_path=\"path/to/experiment\",\n            ...     n_readers=100,\n            ...     n_scanners=16\n            ... )\n            &gt;&gt;&gt; print(f\"MoS readers: {dataloader.n_readers}, scanners: {dataloader.n_scanners}\")\n            MoS readers: 100, scanners: 16\n        \"\"\"\n        self.tiledb_path = tiledb_path\n        self.batch_size = batch_size\n        self.prefetch_batch_size = prefetch_batch_size\n        self.seed = seed\n        self.n_epochs = n_epochs\n        self.verbose = verbose\n        self.max_queue_size = max_queue_size\n        self.use_mixture_of_scanners = use_mixture_of_scanners\n        self.n_readers = n_readers\n        self.n_scanners = n_scanners\n\n        # Check that required modules are available\n        if not TILEDB_AVAILABLE:\n            raise ImportError(\"TileDB SOMA is required but not available\")\n\n        # Use IterableDataset\n        self._dataset = TileDBIterableDataset(\n            tiledb_path=tiledb_path,\n            batch_size=batch_size,\n            prefetch_batch_size=prefetch_batch_size,\n            seed=seed,\n            max_queue_size=max_queue_size,\n            n_epochs=n_epochs,\n            verbose=verbose,\n            use_mixture_of_scanners=use_mixture_of_scanners,\n            n_readers=n_readers,\n            n_scanners=n_scanners,\n        )\n\n    def __iter__(self):\n        \"\"\"\n        Iterate through batches of TileDB data with async prefetching.\n\n        This method provides an iterator over batches of TileDB data, automatically\n        handling background prefetching, epoch transitions, and error recovery.\n        It yields dictionaries containing the batch data and metadata.\n\n        The iterator automatically manages:\n        - Background prefetching for improved throughput\n        - Epoch transitions for multi-epoch training\n        - Error handling and recovery\n        - Performance monitoring and reporting\n\n        Yields:\n            dict: Batch dictionary containing:\n                - X: Polars DataFrame with cell-gene expression data\n                - cell_ids: List of unique cell IDs in the batch\n                - epoch: Current epoch number (when n_epochs &gt; 1)\n\n        Examples:\n            &gt;&gt;&gt; # Basic iteration\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\"path/to/experiment\")\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     print(f\"Batch keys: {list(batch.keys())}\")\n            ...     print(f\"Cell IDs: {batch['cell_ids']}\")\n            ...     break\n            Batch keys: ['X', 'cell_ids']\n            Cell IDs: [0, 1, 2, ..., 29, 30, 31]\n\n            &gt;&gt;&gt; # Multi-epoch training\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\"path/to/experiment\", n_epochs=3)\n            &gt;&gt;&gt; epochs_seen = set()\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     if 'epoch' in batch:\n            ...         epochs_seen.add(batch['epoch'])\n            ...     if len(epochs_seen) &gt;= 3:\n            ...         break\n            &gt;&gt;&gt; print(f\"Epochs completed: {sorted(epochs_seen)}\")\n            Epochs completed: [0, 1, 2]\n\n            &gt;&gt;&gt; # Training loop with error handling\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\"path/to/experiment\")\n            &gt;&gt;&gt; for batch_idx, batch in enumerate(dataloader):\n            ...     try:\n            ...         x = batch[\"X\"]\n            ...         cell_ids = batch[\"cell_ids\"]\n            ...         print(f\"Processed batch {batch_idx} with {len(cell_ids)} cells\")\n            ...     except Exception as e:\n            ...         print(f\"Error in batch {batch_idx}: {e}\")\n            ...         continue\n            ...     if batch_idx &gt;= 2:  # Just first few batches\n            ...         break\n            Processed batch 0 with 32 cells\n            Processed batch 1 with 32 cells\n            Processed batch 2 with 32 cells\n        \"\"\"\n        yield from self._dataset\n\n    def __len__(self):\n        \"\"\"\n        Return the number of batches in the dataset.\n\n        Note: Since TileDBDataLoader uses an IterableDataset that streams data,\n        the exact number of batches is not known in advance. This method\n        returns 0 to indicate an unknown length for streaming datasets.\n\n        Returns:\n            int: Always returns 0 to indicate unknown length for streaming datasets.\n\n        Examples:\n            &gt;&gt;&gt; # Check dataset length\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\"path/to/experiment\")\n            &gt;&gt;&gt; print(f\"Dataset length: {len(dataloader)}\")\n            Dataset length: 0\n\n            &gt;&gt;&gt; # IterableDataset behavior\n            &gt;&gt;&gt; batch_count = 0\n            &gt;&gt;&gt; for batch in dataloader:\n            ...     batch_count += 1\n            ...     if batch_count &gt;= 5:  # Just count first 5 batches\n            ...         break\n            &gt;&gt;&gt; print(f\"Actually processed {batch_count} batches\")\n            Actually processed 5 batches\n\n            &gt;&gt;&gt; # Length is consistent\n            &gt;&gt;&gt; print(f\"Length check: {len(dataloader)}\")\n            Length check: 0\n        \"\"\"\n        return 0  # Indicates unknown length for streaming datasets\n\n    def __del__(self):\n        \"\"\"\n        Cleanup method to stop async prefetching.\n\n        This method is called when the DataLoader object is garbage collected.\n        It ensures that the underlying dataset's prefetcher is properly cleaned up\n        to prevent resource leaks.\n\n        Examples:\n            &gt;&gt;&gt; # DataLoader cleanup happens automatically\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\"path/to/experiment\")\n            &gt;&gt;&gt; print(\"DataLoader created\")\n            DataLoader created\n            &gt;&gt;&gt; # When dataloader goes out of scope, __del__ is called automatically\n            &gt;&gt;&gt; del dataloader\n            &gt;&gt;&gt; print(\"DataLoader destroyed and cleaned up\")\n            DataLoader destroyed and cleaned up\n\n            &gt;&gt;&gt; # Manual cleanup (not usually needed)\n            &gt;&gt;&gt; dataloader = TileDBDataLoader(\"path/to/experiment\")\n            &gt;&gt;&gt; dataloader.__del__()\n            &gt;&gt;&gt; print(\"Manual cleanup completed\")\n            Manual cleanup completed\n        \"\"\"\n        if hasattr(self, \"_dataset\"):\n            # The TileDBIterableDataset doesn't have a stop method,\n            # so we just let it finish its current epoch.\n            pass\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBDataLoader-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.TileDBDataLoader.__init__","title":"<code>__init__(tiledb_path: str, batch_size: int = 32, prefetch_batch_size: int = 100, seed: int = 42, n_epochs: int = 1, verbose: bool = True, max_queue_size: int = 500, use_mixture_of_scanners: bool = True, n_readers: int = 50, n_scanners: int = 8)</code>","text":"<p>Initialize the TileDB DataLoader with training configuration.</p> <p>Parameters:</p> Name Type Description Default <code>tiledb_path</code> <code>str</code> <p>Path to the TileDB SOMA experiment directory.          Must contain a valid SOMA experiment with RNA measurement data.</p> required <code>batch_size</code> <code>int</code> <p>Number of cells per training batch. Larger batches use more        memory but may improve training efficiency. Range: 1-512, default: 32.</p> <code>32</code> <code>prefetch_batch_size</code> <code>int</code> <p>Number of cells to prefetch from TileDB per batch.                Higher values improve throughput but use more memory.                Range: 10-10000, default: 100.</p> <code>100</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible shuffling and MoS sampling.   Used for consistent data ordering across runs. Default: 42.</p> <code>42</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs to run. The dataloader will automatically reset      after each epoch, enabling multi-epoch training. Default: 1.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>If True, print detailed timing and progress information.     If False, suppress all internal prints for clean output. Default: True.</p> <code>True</code> <code>max_queue_size</code> <code>int</code> <p>Maximum number of pre-processed batches to keep in queue.           Higher values use more memory but provide better buffering.           Range: 10-10000, default: 500.</p> <code>500</code> <code>use_mixture_of_scanners</code> <code>bool</code> <p>If True, use MoS strategy for higher entropy by                    randomly sampling from multiple fragment generators.                    Provides better randomization for foundation model training.                    Default: True.</p> <code>True</code> <code>n_readers</code> <code>int</code> <p>Total number of fragment generators to create when using MoS.       Higher values provide better entropy but use more memory.       Range: 1-1000, default: 50.</p> <code>50</code> <code>n_scanners</code> <code>int</code> <p>Number of active scanners to sample from simultaneously when using MoS.        Higher values provide better entropy but use more memory.        Range: 1-100, default: 8.</p> <code>8</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If TileDB SOMA is not available.</p> <code>ValueError</code> <p>If MoS parameters are invalid.</p> <code>RuntimeError</code> <p>If the TileDB experiment cannot be opened or is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization with default MoS strategy\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     batch_size=32,\n...     prefetch_batch_size=100\n... )\n&gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\nMoS enabled: True\n</code></pre> <pre><code>&gt;&gt;&gt; # Sequential loading for maximum throughput\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     use_mixture_of_scanners=False,\n...     batch_size=64\n... )\n&gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\nSequential loading: True\n</code></pre> <pre><code>&gt;&gt;&gt; # High-entropy MoS configuration\n&gt;&gt;&gt; dataloader = TileDBDataLoader(\n...     tiledb_path=\"path/to/experiment\",\n...     n_readers=100,\n...     n_scanners=16\n... )\n&gt;&gt;&gt; print(f\"MoS readers: {dataloader.n_readers}, scanners: {dataloader.n_scanners}\")\nMoS readers: 100, scanners: 16\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def __init__(\n    self,\n    tiledb_path: str,\n    batch_size: int = 32,\n    prefetch_batch_size: int = 100,\n    seed: int = 42,\n    n_epochs: int = 1,\n    verbose: bool = True,\n    max_queue_size: int = 500,\n    use_mixture_of_scanners: bool = True,\n    n_readers: int = 50,\n    n_scanners: int = 8,\n):\n    \"\"\"\n    Initialize the TileDB DataLoader with training configuration.\n\n    Args:\n        tiledb_path: Path to the TileDB SOMA experiment directory.\n                     Must contain a valid SOMA experiment with RNA measurement data.\n        batch_size: Number of cells per training batch. Larger batches use more\n                   memory but may improve training efficiency. Range: 1-512, default: 32.\n        prefetch_batch_size: Number of cells to prefetch from TileDB per batch.\n                           Higher values improve throughput but use more memory.\n                           Range: 10-10000, default: 100.\n        seed: Random seed for reproducible shuffling and MoS sampling.\n              Used for consistent data ordering across runs. Default: 42.\n        n_epochs: Number of epochs to run. The dataloader will automatically reset\n                 after each epoch, enabling multi-epoch training. Default: 1.\n        verbose: If True, print detailed timing and progress information.\n                If False, suppress all internal prints for clean output. Default: True.\n        max_queue_size: Maximum number of pre-processed batches to keep in queue.\n                      Higher values use more memory but provide better buffering.\n                      Range: 10-10000, default: 500.\n        use_mixture_of_scanners: If True, use MoS strategy for higher entropy by\n                               randomly sampling from multiple fragment generators.\n                               Provides better randomization for foundation model training.\n                               Default: True.\n        n_readers: Total number of fragment generators to create when using MoS.\n                  Higher values provide better entropy but use more memory.\n                  Range: 1-1000, default: 50.\n        n_scanners: Number of active scanners to sample from simultaneously when using MoS.\n                   Higher values provide better entropy but use more memory.\n                   Range: 1-100, default: 8.\n\n    Raises:\n        ImportError: If TileDB SOMA is not available.\n        ValueError: If MoS parameters are invalid.\n        RuntimeError: If the TileDB experiment cannot be opened or is invalid.\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization with default MoS strategy\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     batch_size=32,\n        ...     prefetch_batch_size=100\n        ... )\n        &gt;&gt;&gt; print(f\"MoS enabled: {dataloader.use_mixture_of_scanners}\")\n        MoS enabled: True\n\n        &gt;&gt;&gt; # Sequential loading for maximum throughput\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     use_mixture_of_scanners=False,\n        ...     batch_size=64\n        ... )\n        &gt;&gt;&gt; print(f\"Sequential loading: {not dataloader.use_mixture_of_scanners}\")\n        Sequential loading: True\n\n        &gt;&gt;&gt; # High-entropy MoS configuration\n        &gt;&gt;&gt; dataloader = TileDBDataLoader(\n        ...     tiledb_path=\"path/to/experiment\",\n        ...     n_readers=100,\n        ...     n_scanners=16\n        ... )\n        &gt;&gt;&gt; print(f\"MoS readers: {dataloader.n_readers}, scanners: {dataloader.n_scanners}\")\n        MoS readers: 100, scanners: 16\n    \"\"\"\n    self.tiledb_path = tiledb_path\n    self.batch_size = batch_size\n    self.prefetch_batch_size = prefetch_batch_size\n    self.seed = seed\n    self.n_epochs = n_epochs\n    self.verbose = verbose\n    self.max_queue_size = max_queue_size\n    self.use_mixture_of_scanners = use_mixture_of_scanners\n    self.n_readers = n_readers\n    self.n_scanners = n_scanners\n\n    # Check that required modules are available\n    if not TILEDB_AVAILABLE:\n        raise ImportError(\"TileDB SOMA is required but not available\")\n\n    # Use IterableDataset\n    self._dataset = TileDBIterableDataset(\n        tiledb_path=tiledb_path,\n        batch_size=batch_size,\n        prefetch_batch_size=prefetch_batch_size,\n        seed=seed,\n        max_queue_size=max_queue_size,\n        n_epochs=n_epochs,\n        verbose=verbose,\n        use_mixture_of_scanners=use_mixture_of_scanners,\n        n_readers=n_readers,\n        n_scanners=n_scanners,\n    )\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.print_prefetch","title":"<code>print_prefetch(message: str, verbose: bool = True)</code>","text":"<p>Print prefetch-related messages with colored formatting.</p> <p>This function prints prefetch-related messages using rich console formatting when available, or falls back to loguru logging. Messages are displayed in cyan-colored panels for better visual distinction during training.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to print.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the message. If False, suppress output.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Print a prefetch message\n&gt;&gt;&gt; print_prefetch(\"Loading batch 1 of 100\")\n&gt;&gt;&gt; # Suppress output\n&gt;&gt;&gt; print_prefetch(\"Loading batch 1 of 100\", verbose=False)\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def print_prefetch(message: str, verbose: bool = True):\n    \"\"\"\n    Print prefetch-related messages with colored formatting.\n\n    This function prints prefetch-related messages using rich console formatting\n    when available, or falls back to loguru logging. Messages are displayed in\n    cyan-colored panels for better visual distinction during training.\n\n    Args:\n        message: The message to print.\n        verbose: If True, print the message. If False, suppress output.\n\n    Examples:\n        &gt;&gt;&gt; # Print a prefetch message\n        &gt;&gt;&gt; print_prefetch(\"Loading batch 1 of 100\")\n        &gt;&gt;&gt; # Suppress output\n        &gt;&gt;&gt; print_prefetch(\"Loading batch 1 of 100\", verbose=False)\n    \"\"\"\n    if not verbose:\n        return\n\n    if RICH_AVAILABLE and console is not None:\n        console.print(Panel(message, border_style=\"cyan\"))\n    else:\n        logger.info(f\"\ud83d\udd0d {message}\")\n</code></pre>"},{"location":"api/ml/#slaf.ml.tiledb_dataloaders.print_training","title":"<code>print_training(message: str, verbose: bool = True)</code>","text":"<p>Print training-related messages with colored formatting.</p> <p>This function prints training-related messages using rich console formatting when available, or falls back to loguru logging. Messages are displayed in green-colored panels for better visual distinction during training.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>The message to print.</p> required <code>verbose</code> <code>bool</code> <p>If True, print the message. If False, suppress output.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Print a training message\n&gt;&gt;&gt; print_training(\"Processing batch with 32 cells\")\n&gt;&gt;&gt; # Suppress output\n&gt;&gt;&gt; print_training(\"Processing batch with 32 cells\", verbose=False)\n</code></pre> Source code in <code>slaf/ml/tiledb_dataloaders.py</code> <pre><code>def print_training(message: str, verbose: bool = True):\n    \"\"\"\n    Print training-related messages with colored formatting.\n\n    This function prints training-related messages using rich console formatting\n    when available, or falls back to loguru logging. Messages are displayed in\n    green-colored panels for better visual distinction during training.\n\n    Args:\n        message: The message to print.\n        verbose: If True, print the message. If False, suppress output.\n\n    Examples:\n        &gt;&gt;&gt; # Print a training message\n        &gt;&gt;&gt; print_training(\"Processing batch with 32 cells\")\n        &gt;&gt;&gt; # Suppress output\n        &gt;&gt;&gt; print_training(\"Processing batch with 32 cells\", verbose=False)\n    \"\"\"\n    if not verbose:\n        return\n\n    if RICH_AVAILABLE and console is not None:\n        console.print(Panel(message, border_style=\"green\"))\n    else:\n        logger.info(f\"\ud83d\udcca {message}\")\n</code></pre>"},{"location":"api/ml/#tokenizers","title":"Tokenizers","text":""},{"location":"api/ml/#slaf.ml.tokenizers","title":"<code>slaf.ml.tokenizers</code>","text":""},{"location":"api/ml/#slaf.ml.tokenizers-classes","title":"Classes","text":""},{"location":"api/ml/#slaf.ml.tokenizers.TokenizerType","title":"<code>TokenizerType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Tokenizer types</p> Source code in <code>slaf/ml/tokenizers.py</code> <pre><code>class TokenizerType(str, Enum):\n    \"\"\"Tokenizer types\"\"\"\n\n    GENEFORMER = \"geneformer\"\n    SCPGPT = \"scgpt\"\n</code></pre>"},{"location":"api/ml/#slaf.ml.tokenizers.SLAFTokenizer","title":"<code>SLAFTokenizer</code>","text":"<p>Tokenizer for single-cell RNA-seq data in SLAF format.</p> <p>SLAFTokenizer converts single-cell gene expression data into token sequences suitable for machine learning models. It supports multiple tokenization strategies including GeneFormer and scGPT formats with optimized vectorized operations.</p> Key Features <ul> <li>Multiple tokenization strategies (GeneFormer, scGPT)</li> <li>Vectorized tokenization for high performance</li> <li>Expression binning for scGPT format</li> <li>Device-agnostic CPU tensor output</li> <li>Memory-efficient processing</li> <li>Comprehensive vocabulary management</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic usage with GeneFormer\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"geneformer\")\n&gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences)\n&gt;&gt;&gt; print(f\"Input shape: {input_ids.shape}\")\nInput shape: torch.Size([2, 2048])\n</code></pre> <pre><code>&gt;&gt;&gt; # scGPT with expression sequences\n&gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"scgpt\")\n&gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; expr_sequences = [[0.5, 0.8, 0.2], [0.9, 0.1, 0.7]]\n&gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(\n...     gene_sequences, expr_sequences\n... )\n&gt;&gt;&gt; print(f\"Input shape: {input_ids.shape}\")\nInput shape: torch.Size([2, 2050])\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid tokenizer type\n&gt;&gt;&gt; try:\n...     tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"invalid\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Unsupported tokenizer type: invalid. Supported types: ['geneformer', 'scgpt']\n</code></pre> <pre><code>&gt;&gt;&gt; # Vocabulary information\n&gt;&gt;&gt; vocab_info = tokenizer.get_vocab_info()\n&gt;&gt;&gt; print(f\"Vocabulary size: {vocab_info['vocab_size']}\")\nVocabulary size: 50000\n</code></pre> Source code in <code>slaf/ml/tokenizers.py</code> <pre><code>class SLAFTokenizer:\n    \"\"\"\n    Tokenizer for single-cell RNA-seq data in SLAF format.\n\n    SLAFTokenizer converts single-cell gene expression data into token sequences\n    suitable for machine learning models. It supports multiple tokenization strategies\n    including GeneFormer and scGPT formats with optimized vectorized operations.\n\n    Key Features:\n        - Multiple tokenization strategies (GeneFormer, scGPT)\n        - Vectorized tokenization for high performance\n        - Expression binning for scGPT format\n        - Device-agnostic CPU tensor output\n        - Memory-efficient processing\n        - Comprehensive vocabulary management\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage with GeneFormer\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"geneformer\")\n        &gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n        &gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences)\n        &gt;&gt;&gt; print(f\"Input shape: {input_ids.shape}\")\n        Input shape: torch.Size([2, 2048])\n\n        &gt;&gt;&gt; # scGPT with expression sequences\n        &gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"scgpt\")\n        &gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n        &gt;&gt;&gt; expr_sequences = [[0.5, 0.8, 0.2], [0.9, 0.1, 0.7]]\n        &gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(\n        ...     gene_sequences, expr_sequences\n        ... )\n        &gt;&gt;&gt; print(f\"Input shape: {input_ids.shape}\")\n        Input shape: torch.Size([2, 2050])\n\n        &gt;&gt;&gt; # Error handling for invalid tokenizer type\n        &gt;&gt;&gt; try:\n        ...     tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"invalid\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Unsupported tokenizer type: invalid. Supported types: ['geneformer', 'scgpt']\n\n        &gt;&gt;&gt; # Vocabulary information\n        &gt;&gt;&gt; vocab_info = tokenizer.get_vocab_info()\n        &gt;&gt;&gt; print(f\"Vocabulary size: {vocab_info['vocab_size']}\")\n        Vocabulary size: 50000\n    \"\"\"\n\n    def __init__(\n        self,\n        slaf_array: SLAFArray,\n        tokenizer_type: TokenizerType | str = TokenizerType.GENEFORMER,\n        vocab_size: int = 50000,\n        n_expression_bins: int = 10,\n    ):\n        \"\"\"\n        Initialize SLAFTokenizer with SLAF array and vocabulary settings.\n\n        Args:\n            slaf_array: Initialized SLAFArray instance containing the single-cell data.\n                       Used to build the gene vocabulary and access expression data.\n                       Must be a valid SLAFArray with proper var DataFrame.\n            tokenizer_type: Type of tokenizer to use. Options: \"geneformer\", \"scgpt\".\n                          Can be passed as string or TokenizerType enum.\n            vocab_size: Maximum size of gene vocabulary. Genes beyond this limit\n                       are excluded from tokenization. Higher values use more memory.\n            n_expression_bins: Number of expression bins for scGPT tokenization.\n                             Higher values provide finer expression resolution.\n                             Range: 1-1000, default: 10.\n\n        Raises:\n            ValueError: If tokenizer_type is not supported or vocab_size is invalid.\n            RuntimeError: If SLAF array is not properly initialized.\n            TypeError: If slaf_array is not a valid SLAFArray instance.\n\n        Examples:\n            &gt;&gt;&gt; # Basic initialization\n            &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n            &gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array)\n            &gt;&gt;&gt; print(f\"Tokenizer type: {tokenizer.tokenizer_type}\")\n            Tokenizer type: TokenizerType.GENEFORMER\n\n            &gt;&gt;&gt; # scGPT with custom settings\n            &gt;&gt;&gt; tokenizer = SLAFTokenizer(\n            ...     slaf_array=slaf_array,\n            ...     tokenizer_type=\"scgpt\",\n            ...     vocab_size=30000,\n            ...     n_expression_bins=20\n            ... )\n            &gt;&gt;&gt; print(f\"Expression bins: {tokenizer.n_expression_bins}\")\n            Expression bins: 20\n\n            &gt;&gt;&gt; # Error handling for invalid tokenizer type\n            &gt;&gt;&gt; try:\n            ...     tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"invalid\")\n            ... except ValueError as e:\n            ...     print(f\"Error: {e}\")\n            Error: Unsupported tokenizer type: invalid. Supported types: ['geneformer', 'scgpt']\n\n            &gt;&gt;&gt; # Error handling for invalid SLAF array\n            &gt;&gt;&gt; try:\n            ...     tokenizer = SLAFTokenizer(None)\n            ... except TypeError as e:\n            ...     print(f\"Error: {e}\")\n            Error: slaf_array must be a valid SLAFArray instance\n        \"\"\"\n        self.slaf_array = slaf_array\n        self.vocab_size = vocab_size\n        self.n_expression_bins = n_expression_bins\n\n        # Convert string to enum if needed\n        if isinstance(tokenizer_type, str):\n            try:\n                self.tokenizer_type = TokenizerType(tokenizer_type.lower())\n            except ValueError as err:\n                raise ValueError(\n                    f\"Unsupported tokenizer type: {tokenizer_type}. \"\n                    f\"Supported types: {[t.value for t in TokenizerType]}\"\n                ) from err\n        else:\n            self.tokenizer_type = tokenizer_type\n\n        # Build vocabulary and special tokens\n        self._build_gene_vocabulary()\n        self._setup_special_tokens()\n\n    def _build_gene_vocabulary(self):\n        \"\"\"Build gene vocabulary from SLAF var DataFrame or genes Lance table.\"\"\"\n        try:\n            # Try to use metadata if available\n            if self.slaf_array.is_metadata_ready():\n                var_df = self.slaf_array.var.reset_index()\n\n                # Check if we have a real SLAF array or a Mock object\n                if (\n                    hasattr(var_df, \"columns\")\n                    and \"gene_integer_id\" in var_df.columns\n                    and \"gene_id\" in var_df.columns\n                ):\n                    # Real SLAF array - build vocabulary from gene data\n                    gene_vocab = {}\n\n                    # Use Polars native iteration\n                    for row in var_df.iter_rows(named=True):\n                        gene_id = row[\"gene_id\"]\n                        gene_integer_id = row[\"gene_integer_id\"]\n\n                        # Only include genes within vocab size limit\n                        if gene_integer_id &lt; self.vocab_size:\n                            gene_vocab[gene_id] = gene_integer_id\n\n                    self.gene_vocab = gene_vocab\n                    # Account for the +4 offset used in tokenization\n                    self.token_to_gene = {v + 4: k for k, v in self.gene_vocab.items()}\n\n                    # Pre-build vectorized mapping array for fast lookup\n                    max_gene_id = (\n                        max(\n                            int(k) if isinstance(k, str) else k\n                            for k in self.gene_vocab.keys()\n                        )\n                        if self.gene_vocab\n                        else 0\n                    )\n                    self.vocab_mapping = np.full(max_gene_id + 1, -1, dtype=int)\n\n                    # Fill the mapping array once\n                    for gene_id, token_id in self.gene_vocab.items():\n                        try:\n                            gene_id_int = (\n                                int(gene_id) if isinstance(gene_id, str) else gene_id\n                            )\n                            self.vocab_mapping[gene_id_int] = token_id\n                        except (ValueError, TypeError):\n                            continue\n                    return\n\n            # If metadata not available, read directly from genes Lance table\n            # This is more efficient for cloud datasets where metadata loading is skipped\n            import polars as pl\n\n            genes_table = self.slaf_array.genes.to_table()\n            genes_df = pl.from_arrow(genes_table)\n\n            # Check if we have the required columns\n            if \"gene_integer_id\" in genes_df.columns and \"gene_id\" in genes_df.columns:\n                gene_vocab = {}\n\n                # Use Polars native iteration\n                for row in genes_df.iter_rows(named=True):\n                    gene_id = row[\"gene_id\"]\n                    gene_integer_id = row[\"gene_integer_id\"]\n\n                    # Only include genes within vocab size limit\n                    if gene_integer_id &lt; self.vocab_size:\n                        gene_vocab[gene_id] = gene_integer_id\n\n                self.gene_vocab = gene_vocab\n                # Account for the +4 offset used in tokenization\n                self.token_to_gene = {v + 4: k for k, v in self.gene_vocab.items()}\n\n                # Pre-build vectorized mapping array for fast lookup\n                max_gene_id = (\n                    max(\n                        int(k) if isinstance(k, str) else k\n                        for k in self.gene_vocab.keys()\n                    )\n                    if self.gene_vocab\n                    else 0\n                )\n                self.vocab_mapping = np.full(max_gene_id + 1, -1, dtype=int)\n\n                # Fill the mapping array once\n                for gene_id, token_id in self.gene_vocab.items():\n                    try:\n                        gene_id_int = (\n                            int(gene_id) if isinstance(gene_id, str) else gene_id\n                        )\n                        self.vocab_mapping[gene_id_int] = token_id\n                    except (ValueError, TypeError):\n                        continue\n                return\n\n            # Fallback: Mock object or missing columns\n            self.gene_vocab = {f\"gene_{i}\": i for i in range(1000)}\n            # Account for the +4 offset used in tokenization\n            self.token_to_gene = {v + 4: k for k, v in self.gene_vocab.items()}\n            # No mapping array needed for mock objects\n\n        except Exception:\n            # Fallback for testing or error cases\n            self.gene_vocab = {f\"gene_{i}\": i for i in range(1000)}\n            # Account for the +4 offset used in tokenization\n            self.token_to_gene = {v + 4: k for k, v in self.gene_vocab.items()}\n            # No mapping array needed for fallback\n\n    def _setup_special_tokens(self):\n        \"\"\"Setup special tokens for tokenization.\"\"\"\n        # Special token IDs\n        self.special_tokens = {\n            \"PAD\": 0,\n            \"CLS\": 1,\n            \"SEP\": 2,\n            \"MASK\": 3,\n        }\n\n        # Expression binning setup for scGPT\n        self.expr_bin_start = self.vocab_size\n        self.expr_bin_size = 1.0 / self.n_expression_bins\n\n    def _expression_to_bin(self, expression_value: float) -&gt; int:\n        \"\"\"Convert expression value to bin token ID\"\"\"\n        if expression_value &lt;= 0:\n            return self.special_tokens[\"PAD\"]\n\n        # Bin the expression value\n        bin_id = min(\n            int(expression_value / self.expr_bin_size), self.n_expression_bins - 1\n        )\n        return self.expr_bin_start + bin_id\n\n    def _expression_to_bin_vectorized(\n        self, expression_values: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"Vectorized version of expression binning\"\"\"\n        # Handle edge cases\n        if len(expression_values) == 0:\n            return np.array([], dtype=int)\n\n        # Create bins\n        bins = np.clip(\n            (expression_values / self.expr_bin_size).astype(int),\n            0,\n            self.n_expression_bins - 1,\n        )\n\n        # Convert to token IDs\n        result = np.where(\n            expression_values &gt; 0,\n            self.expr_bin_start + bins,\n            self.special_tokens[\"PAD\"],\n        )\n\n        return result.astype(int)\n\n    def _map_gene_ids_to_tokens_vectorized(self, gene_ids) -&gt; np.ndarray:\n        \"\"\"Vectorized mapping of gene IDs to token IDs\"\"\"\n        # Handle edge cases\n        if hasattr(gene_ids, \"is_empty\") and gene_ids.is_empty():\n            return np.array([], dtype=int)\n        elif hasattr(gene_ids, \"__len__\") and len(gene_ids) == 0:\n            return np.array([], dtype=int)\n\n        # Convert to numpy array for vectorized operations\n        gene_ids_array = np.array(gene_ids, dtype=int)\n\n        # Check if we have a real SLAF array or a Mock object\n        try:\n            # Try to access the DataFrame properly\n            var_df = self.slaf_array.var.reset_index()\n\n            # Check if it's actually a DataFrame with the expected columns\n            if (\n                hasattr(var_df, \"columns\")\n                and \"gene_integer_id\" in var_df.columns\n                and \"gene_id\" in var_df.columns\n            ):\n                # Real SLAF array - use pre-built vectorized mapping\n                # Vectorized lookup using pre-built mapping array\n                tokens = self.vocab_mapping[gene_ids_array]\n\n                # Filter out missing genes (-1 values)\n                valid_mask = tokens != -1\n                return tokens[valid_mask]\n            else:\n                # Mock object - direct mapping (same as original test)\n                return gene_ids_array + 4  # Simple offset like original test\n        except Exception:\n            # Fallback for testing - direct mapping with offset\n            return gene_ids_array + 4  # Simple offset like original test\n\n    def tokenize(\n        self,\n        gene_sequences: list[list[int] | list[tuple[int, float]]],\n        expr_sequences: list[list[float]] | None = None,\n        max_genes: int | None = None,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Tokenize gene expression sequences into model-ready tensors.\n\n        This method converts gene and expression sequences into tokenized tensors\n        suitable for machine learning models. It supports both GeneFormer and scGPT\n        tokenization strategies with optimized vectorized operations.\n\n        Args:\n            gene_sequences: List of gene ID sequences for each cell\n            expr_sequences: List of expression value sequences for each cell (required for scGPT)\n            max_genes: Maximum number of genes per cell (defaults based on tokenizer type)\n\n        Returns:\n            tuple: (input_ids, attention_mask) tensors\n                - input_ids: Tokenized sequences with padding\n                - attention_mask: Boolean mask indicating valid tokens\n\n        Raises:\n            ValueError: If gene_sequences is empty\n\n        Examples:\n            &gt;&gt;&gt; # GeneFormer tokenization\n            &gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n            &gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences)\n            &gt;&gt;&gt; print(f\"Shape: {input_ids.shape}\")\n            Shape: torch.Size([2, 2048])\n\n            &gt;&gt;&gt; # scGPT tokenization\n            &gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n            &gt;&gt;&gt; expr_sequences = [[0.5, 0.8, 0.2], [0.9, 0.1, 0.7]]\n            &gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences, expr_sequences)\n            &gt;&gt;&gt; print(f\"Shape: {input_ids.shape}\")\n            Shape: torch.Size([2, 2050])\n        \"\"\"\n        if not gene_sequences:\n            raise ValueError(\"Gene sequences cannot be empty\")\n\n        # Set default max_genes based on tokenizer type\n        if max_genes is None:\n            if self.tokenizer_type == TokenizerType.GENEFORMER:\n                max_genes = 2048\n            else:\n                # For scGPT: CLS + (gene,expr)*n + SEP = 2*n + 2\n                # So if we want max_genes total tokens, n = (max_genes - 2) / 2\n                max_genes = 1024  # This is the number of gene-expression pairs\n\n        # Always define max_sequence_length based on tokenizer type\n        if self.tokenizer_type == TokenizerType.GENEFORMER:\n            max_sequence_length = max_genes  # For Geneformer, same as max_genes\n        else:\n            # For scGPT: CLS + (gene,expr)*n + SEP = 2*n + 2\n            max_sequence_length = 2 * max_genes + 2  # Total sequence length\n\n        # For scGPT, gene_sequences now contains struct pairs [(gene, expr), ...]\n        # so we don't need separate expr_sequences validation\n\n        batch_size = len(gene_sequences)\n\n        # Use fast numpy-based approach (same as original test)\n        import numpy as np\n\n        # Pre-allocate numpy array with correct dimensions\n        if self.tokenizer_type == TokenizerType.SCPGPT:\n            # For scGPT: use max_sequence_length (2*max_genes+2)\n            array_width = max_sequence_length\n        else:\n            # For Geneformer: use max_genes\n            array_width = max_genes\n\n        token_array = np.full(\n            (batch_size, array_width), self.special_tokens[\"PAD\"], dtype=np.int64\n        )\n\n        if self.tokenizer_type == TokenizerType.SCPGPT:\n            # scGPT format: [CLS] gene1 expr1 gene2 expr2 ... [SEP]\n            for i, (gene_sequence, expr_sequence) in enumerate(\n                zip(gene_sequences, expr_sequences or [], strict=False)\n            ):\n                # For scGPT, we now use separate gene_sequence and expr_sequence columns\n                if gene_sequence and len(gene_sequence) &gt; 0:\n                    # Fast path: separate gene_sequence and expr_sequence columns\n                    genes = gene_sequence\n                    exprs = expr_sequence if expr_sequence else []\n\n                    # Vectorized operations - use simple +4 for performance\n                    gene_tokens = np.array(genes, dtype=np.int64) + 4\n\n                    # Handle expression tokens - don't bin if already binned by window function\n                    if len(exprs) &gt; 0 and isinstance(exprs[0], int | np.integer):\n                        # Already binned by window function - just convert to tokens\n                        expr_tokens = (\n                            np.array(exprs, dtype=np.int64) + self.expr_bin_start\n                        )\n                    else:\n                        # Raw values - need to bin them\n                        expr_tokens = self._expression_to_bin_vectorized(\n                            np.array(exprs, dtype=np.float32)\n                        )\n\n                    # Vectorized interleaving (much faster than Python loop)\n                    if len(gene_tokens) &gt; 0:\n                        # Pre-allocate full sequence: CLS + (gene,expr)*n + SEP\n                        sequence_length = 1 + 2 * len(gene_tokens) + 1\n                        tokens = np.full(\n                            sequence_length, self.special_tokens[\"PAD\"], dtype=np.int64\n                        )\n\n                        # Set CLS token\n                        tokens[0] = self.special_tokens[\"CLS\"]\n\n                        # Vectorized interleaving\n                        tokens[1::2][: len(gene_tokens)] = gene_tokens  # type: ignore[assignment]\n                        tokens[2::2][: len(expr_tokens)] = expr_tokens  # type: ignore[assignment]\n\n                        tokens[1 + 2 * len(gene_tokens)] = self.special_tokens[\"SEP\"]\n                    else:\n                        # Empty sequence case\n                        tokens = np.array(\n                            [self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]],\n                            dtype=np.int64,\n                        )  # type: ignore[assignment]\n                else:\n                    # Empty sequence case\n                    tokens = np.array(\n                        [self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]],\n                        dtype=np.int64,\n                    )  # type: ignore[assignment]\n\n                # Pad/truncate to correct sequence length\n                if self.tokenizer_type == TokenizerType.SCPGPT:\n                    # For scGPT: use max_sequence_length (2*max_genes+2)\n                    target_length = max_sequence_length\n                else:\n                    # For Geneformer: use max_genes\n                    target_length = max_genes\n\n                tokens = tokens[:target_length]  # type: ignore[assignment]\n                if len(tokens) &lt; target_length:\n                    padding = np.full(\n                        target_length - len(tokens),\n                        self.special_tokens[\"PAD\"],\n                        dtype=np.int64,\n                    )\n                    tokens = np.concatenate([tokens, padding])  # type: ignore[assignment]\n\n                # Fill array\n                token_array[i, :] = tokens  # type: ignore[assignment]\n\n        else:\n            # Geneformer format: [CLS] gene1 gene2 gene3 ... [SEP]\n            for i, gene_sequence in enumerate(gene_sequences):\n                # Convert gene IDs to tokens (fast mapping)\n                gene_tokens = np.array(gene_sequence, dtype=np.int64) + 4\n\n                # Vectorized sequence building: use concatenation for speed\n                if len(gene_tokens) &gt; 0:\n                    # Use concatenation: CLS + genes + SEP\n                    tokens = np.concatenate(\n                        [\n                            [self.special_tokens[\"CLS\"]],\n                            gene_tokens,\n                            [self.special_tokens[\"SEP\"]],\n                        ]\n                    )  # type: ignore[assignment]\n                else:\n                    # Empty sequence case\n                    tokens = np.array(\n                        [self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]],\n                        dtype=np.int64,\n                    )  # type: ignore[assignment]\n\n                # Pad/truncate to max_genes\n                tokens = tokens[:max_genes]  # type: ignore[assignment]\n                if len(tokens) &lt; max_genes:\n                    padding = np.full(\n                        max_genes - len(tokens),\n                        self.special_tokens[\"PAD\"],\n                        dtype=np.int64,\n                    )\n                    tokens = np.concatenate([tokens, padding])  # type: ignore[assignment]\n\n                # Fill array\n                token_array[i, :] = tokens  # type: ignore[assignment]\n\n        # Convert to tensors in one operation\n        input_ids = torch.from_numpy(token_array)\n        attention_mask = input_ids != self.special_tokens[\"PAD\"]\n\n        return input_ids, attention_mask\n\n    def get_vocab_info(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Get vocabulary information for debugging and analysis.\n\n        Returns:\n            dict: Vocabulary information including size, special tokens, etc.\n\n        Examples:\n            &gt;&gt;&gt; vocab_info = tokenizer.get_vocab_info()\n            &gt;&gt;&gt; print(f\"Vocabulary size: {vocab_info['vocab_size']}\")\n            &gt;&gt;&gt; print(f\"Special tokens: {vocab_info['special_tokens']}\")\n            Vocabulary size: 50000\n            Special tokens: {'PAD': 0, 'CLS': 1, 'SEP': 2, 'MASK': 3}\n        \"\"\"\n        return {\n            \"vocab_size\": self.vocab_size,\n            \"tokenizer_type\": self.tokenizer_type.value,\n            \"special_tokens\": self.special_tokens,\n            \"n_expression_bins\": self.n_expression_bins,\n            \"gene_vocab_size\": len(self.gene_vocab),\n        }\n\n    def decode_tokens(self, tokens: list[int]) -&gt; dict[str, Any]:\n        \"\"\"\n        Decode token sequence back to gene information.\n\n        Args:\n            tokens: List of token IDs to decode\n\n        Returns:\n            dict: Decoded information including genes, expressions, etc.\n\n        Examples:\n            &gt;&gt;&gt; # Decode a token sequence\n            &gt;&gt;&gt; tokens = [1, 100, 50050, 200, 50060, 2]  # CLS, gene1, expr1, gene2, expr2, SEP\n            &gt;&gt;&gt; decoded = tokenizer.decode_tokens(tokens)\n            &gt;&gt;&gt; print(f\"Genes: {decoded['genes']}\")\n            &gt;&gt;&gt; print(f\"Expressions: {decoded['expressions']}\")\n            Genes: ['gene_100', 'gene_200']\n            Expressions: [0.5, 0.6]\n        \"\"\"\n        if not tokens:\n            return {\"genes\": [], \"expressions\": [], \"special_tokens\": []}\n\n        genes = []\n        expressions = []\n        special_tokens = []\n\n        i = 0\n        while i &lt; len(tokens):\n            token = tokens[i]\n\n            if token == self.special_tokens[\"CLS\"]:\n                special_tokens.append(\"CLS\")\n                i += 1\n            elif token == self.special_tokens[\"SEP\"]:\n                special_tokens.append(\"SEP\")\n                i += 1\n            elif token == self.special_tokens[\"PAD\"]:\n                special_tokens.append(\"PAD\")\n                i += 1\n            elif token == self.special_tokens[\"MASK\"]:\n                special_tokens.append(\"MASK\")\n                i += 1\n            elif (\n                self.tokenizer_type == TokenizerType.SCPGPT\n                and token &gt;= self.expr_bin_start\n            ):\n                # Expression token\n                bin_id = token - self.expr_bin_start\n                expr_value = bin_id * self.expr_bin_size\n                expressions.append(expr_value)\n                i += 1\n            else:\n                # Gene token\n                if token in self.token_to_gene:\n                    genes.append(self.token_to_gene[token])\n                else:\n                    genes.append(f\"unknown_gene_{token}\")\n                i += 1\n\n        return {\n            \"genes\": genes,\n            \"expressions\": expressions,\n            \"special_tokens\": special_tokens,\n        }\n</code></pre>"},{"location":"api/ml/#slaf.ml.tokenizers.SLAFTokenizer-functions","title":"Functions","text":""},{"location":"api/ml/#slaf.ml.tokenizers.SLAFTokenizer.__init__","title":"<code>__init__(slaf_array: SLAFArray, tokenizer_type: TokenizerType | str = TokenizerType.GENEFORMER, vocab_size: int = 50000, n_expression_bins: int = 10)</code>","text":"<p>Initialize SLAFTokenizer with SLAF array and vocabulary settings.</p> <p>Parameters:</p> Name Type Description Default <code>slaf_array</code> <code>SLAFArray</code> <p>Initialized SLAFArray instance containing the single-cell data.        Used to build the gene vocabulary and access expression data.        Must be a valid SLAFArray with proper var DataFrame.</p> required <code>tokenizer_type</code> <code>TokenizerType | str</code> <p>Type of tokenizer to use. Options: \"geneformer\", \"scgpt\".           Can be passed as string or TokenizerType enum.</p> <code>GENEFORMER</code> <code>vocab_size</code> <code>int</code> <p>Maximum size of gene vocabulary. Genes beyond this limit        are excluded from tokenization. Higher values use more memory.</p> <code>50000</code> <code>n_expression_bins</code> <code>int</code> <p>Number of expression bins for scGPT tokenization.              Higher values provide finer expression resolution.              Range: 1-1000, default: 10.</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If tokenizer_type is not supported or vocab_size is invalid.</p> <code>RuntimeError</code> <p>If SLAF array is not properly initialized.</p> <code>TypeError</code> <p>If slaf_array is not a valid SLAFArray instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Basic initialization\n&gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n&gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array)\n&gt;&gt;&gt; print(f\"Tokenizer type: {tokenizer.tokenizer_type}\")\nTokenizer type: TokenizerType.GENEFORMER\n</code></pre> <pre><code>&gt;&gt;&gt; # scGPT with custom settings\n&gt;&gt;&gt; tokenizer = SLAFTokenizer(\n...     slaf_array=slaf_array,\n...     tokenizer_type=\"scgpt\",\n...     vocab_size=30000,\n...     n_expression_bins=20\n... )\n&gt;&gt;&gt; print(f\"Expression bins: {tokenizer.n_expression_bins}\")\nExpression bins: 20\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid tokenizer type\n&gt;&gt;&gt; try:\n...     tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"invalid\")\n... except ValueError as e:\n...     print(f\"Error: {e}\")\nError: Unsupported tokenizer type: invalid. Supported types: ['geneformer', 'scgpt']\n</code></pre> <pre><code>&gt;&gt;&gt; # Error handling for invalid SLAF array\n&gt;&gt;&gt; try:\n...     tokenizer = SLAFTokenizer(None)\n... except TypeError as e:\n...     print(f\"Error: {e}\")\nError: slaf_array must be a valid SLAFArray instance\n</code></pre> Source code in <code>slaf/ml/tokenizers.py</code> <pre><code>def __init__(\n    self,\n    slaf_array: SLAFArray,\n    tokenizer_type: TokenizerType | str = TokenizerType.GENEFORMER,\n    vocab_size: int = 50000,\n    n_expression_bins: int = 10,\n):\n    \"\"\"\n    Initialize SLAFTokenizer with SLAF array and vocabulary settings.\n\n    Args:\n        slaf_array: Initialized SLAFArray instance containing the single-cell data.\n                   Used to build the gene vocabulary and access expression data.\n                   Must be a valid SLAFArray with proper var DataFrame.\n        tokenizer_type: Type of tokenizer to use. Options: \"geneformer\", \"scgpt\".\n                      Can be passed as string or TokenizerType enum.\n        vocab_size: Maximum size of gene vocabulary. Genes beyond this limit\n                   are excluded from tokenization. Higher values use more memory.\n        n_expression_bins: Number of expression bins for scGPT tokenization.\n                         Higher values provide finer expression resolution.\n                         Range: 1-1000, default: 10.\n\n    Raises:\n        ValueError: If tokenizer_type is not supported or vocab_size is invalid.\n        RuntimeError: If SLAF array is not properly initialized.\n        TypeError: If slaf_array is not a valid SLAFArray instance.\n\n    Examples:\n        &gt;&gt;&gt; # Basic initialization\n        &gt;&gt;&gt; slaf_array = SLAFArray(\"path/to/data.slaf\")\n        &gt;&gt;&gt; tokenizer = SLAFTokenizer(slaf_array)\n        &gt;&gt;&gt; print(f\"Tokenizer type: {tokenizer.tokenizer_type}\")\n        Tokenizer type: TokenizerType.GENEFORMER\n\n        &gt;&gt;&gt; # scGPT with custom settings\n        &gt;&gt;&gt; tokenizer = SLAFTokenizer(\n        ...     slaf_array=slaf_array,\n        ...     tokenizer_type=\"scgpt\",\n        ...     vocab_size=30000,\n        ...     n_expression_bins=20\n        ... )\n        &gt;&gt;&gt; print(f\"Expression bins: {tokenizer.n_expression_bins}\")\n        Expression bins: 20\n\n        &gt;&gt;&gt; # Error handling for invalid tokenizer type\n        &gt;&gt;&gt; try:\n        ...     tokenizer = SLAFTokenizer(slaf_array, tokenizer_type=\"invalid\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Unsupported tokenizer type: invalid. Supported types: ['geneformer', 'scgpt']\n\n        &gt;&gt;&gt; # Error handling for invalid SLAF array\n        &gt;&gt;&gt; try:\n        ...     tokenizer = SLAFTokenizer(None)\n        ... except TypeError as e:\n        ...     print(f\"Error: {e}\")\n        Error: slaf_array must be a valid SLAFArray instance\n    \"\"\"\n    self.slaf_array = slaf_array\n    self.vocab_size = vocab_size\n    self.n_expression_bins = n_expression_bins\n\n    # Convert string to enum if needed\n    if isinstance(tokenizer_type, str):\n        try:\n            self.tokenizer_type = TokenizerType(tokenizer_type.lower())\n        except ValueError as err:\n            raise ValueError(\n                f\"Unsupported tokenizer type: {tokenizer_type}. \"\n                f\"Supported types: {[t.value for t in TokenizerType]}\"\n            ) from err\n    else:\n        self.tokenizer_type = tokenizer_type\n\n    # Build vocabulary and special tokens\n    self._build_gene_vocabulary()\n    self._setup_special_tokens()\n</code></pre>"},{"location":"api/ml/#slaf.ml.tokenizers.SLAFTokenizer.tokenize","title":"<code>tokenize(gene_sequences: list[list[int] | list[tuple[int, float]]], expr_sequences: list[list[float]] | None = None, max_genes: int | None = None) -&gt; tuple[torch.Tensor, torch.Tensor]</code>","text":"<p>Tokenize gene expression sequences into model-ready tensors.</p> <p>This method converts gene and expression sequences into tokenized tensors suitable for machine learning models. It supports both GeneFormer and scGPT tokenization strategies with optimized vectorized operations.</p> <p>Parameters:</p> Name Type Description Default <code>gene_sequences</code> <code>list[list[int] | list[tuple[int, float]]]</code> <p>List of gene ID sequences for each cell</p> required <code>expr_sequences</code> <code>list[list[float]] | None</code> <p>List of expression value sequences for each cell (required for scGPT)</p> <code>None</code> <code>max_genes</code> <code>int | None</code> <p>Maximum number of genes per cell (defaults based on tokenizer type)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Tensor, Tensor]</code> <p>(input_ids, attention_mask) tensors - input_ids: Tokenized sequences with padding - attention_mask: Boolean mask indicating valid tokens</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If gene_sequences is empty</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # GeneFormer tokenization\n&gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences)\n&gt;&gt;&gt; print(f\"Shape: {input_ids.shape}\")\nShape: torch.Size([2, 2048])\n</code></pre> <pre><code>&gt;&gt;&gt; # scGPT tokenization\n&gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n&gt;&gt;&gt; expr_sequences = [[0.5, 0.8, 0.2], [0.9, 0.1, 0.7]]\n&gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences, expr_sequences)\n&gt;&gt;&gt; print(f\"Shape: {input_ids.shape}\")\nShape: torch.Size([2, 2050])\n</code></pre> Source code in <code>slaf/ml/tokenizers.py</code> <pre><code>def tokenize(\n    self,\n    gene_sequences: list[list[int] | list[tuple[int, float]]],\n    expr_sequences: list[list[float]] | None = None,\n    max_genes: int | None = None,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Tokenize gene expression sequences into model-ready tensors.\n\n    This method converts gene and expression sequences into tokenized tensors\n    suitable for machine learning models. It supports both GeneFormer and scGPT\n    tokenization strategies with optimized vectorized operations.\n\n    Args:\n        gene_sequences: List of gene ID sequences for each cell\n        expr_sequences: List of expression value sequences for each cell (required for scGPT)\n        max_genes: Maximum number of genes per cell (defaults based on tokenizer type)\n\n    Returns:\n        tuple: (input_ids, attention_mask) tensors\n            - input_ids: Tokenized sequences with padding\n            - attention_mask: Boolean mask indicating valid tokens\n\n    Raises:\n        ValueError: If gene_sequences is empty\n\n    Examples:\n        &gt;&gt;&gt; # GeneFormer tokenization\n        &gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n        &gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences)\n        &gt;&gt;&gt; print(f\"Shape: {input_ids.shape}\")\n        Shape: torch.Size([2, 2048])\n\n        &gt;&gt;&gt; # scGPT tokenization\n        &gt;&gt;&gt; gene_sequences = [[1, 2, 3], [4, 5, 6]]\n        &gt;&gt;&gt; expr_sequences = [[0.5, 0.8, 0.2], [0.9, 0.1, 0.7]]\n        &gt;&gt;&gt; input_ids, attention_mask = tokenizer.tokenize(gene_sequences, expr_sequences)\n        &gt;&gt;&gt; print(f\"Shape: {input_ids.shape}\")\n        Shape: torch.Size([2, 2050])\n    \"\"\"\n    if not gene_sequences:\n        raise ValueError(\"Gene sequences cannot be empty\")\n\n    # Set default max_genes based on tokenizer type\n    if max_genes is None:\n        if self.tokenizer_type == TokenizerType.GENEFORMER:\n            max_genes = 2048\n        else:\n            # For scGPT: CLS + (gene,expr)*n + SEP = 2*n + 2\n            # So if we want max_genes total tokens, n = (max_genes - 2) / 2\n            max_genes = 1024  # This is the number of gene-expression pairs\n\n    # Always define max_sequence_length based on tokenizer type\n    if self.tokenizer_type == TokenizerType.GENEFORMER:\n        max_sequence_length = max_genes  # For Geneformer, same as max_genes\n    else:\n        # For scGPT: CLS + (gene,expr)*n + SEP = 2*n + 2\n        max_sequence_length = 2 * max_genes + 2  # Total sequence length\n\n    # For scGPT, gene_sequences now contains struct pairs [(gene, expr), ...]\n    # so we don't need separate expr_sequences validation\n\n    batch_size = len(gene_sequences)\n\n    # Use fast numpy-based approach (same as original test)\n    import numpy as np\n\n    # Pre-allocate numpy array with correct dimensions\n    if self.tokenizer_type == TokenizerType.SCPGPT:\n        # For scGPT: use max_sequence_length (2*max_genes+2)\n        array_width = max_sequence_length\n    else:\n        # For Geneformer: use max_genes\n        array_width = max_genes\n\n    token_array = np.full(\n        (batch_size, array_width), self.special_tokens[\"PAD\"], dtype=np.int64\n    )\n\n    if self.tokenizer_type == TokenizerType.SCPGPT:\n        # scGPT format: [CLS] gene1 expr1 gene2 expr2 ... [SEP]\n        for i, (gene_sequence, expr_sequence) in enumerate(\n            zip(gene_sequences, expr_sequences or [], strict=False)\n        ):\n            # For scGPT, we now use separate gene_sequence and expr_sequence columns\n            if gene_sequence and len(gene_sequence) &gt; 0:\n                # Fast path: separate gene_sequence and expr_sequence columns\n                genes = gene_sequence\n                exprs = expr_sequence if expr_sequence else []\n\n                # Vectorized operations - use simple +4 for performance\n                gene_tokens = np.array(genes, dtype=np.int64) + 4\n\n                # Handle expression tokens - don't bin if already binned by window function\n                if len(exprs) &gt; 0 and isinstance(exprs[0], int | np.integer):\n                    # Already binned by window function - just convert to tokens\n                    expr_tokens = (\n                        np.array(exprs, dtype=np.int64) + self.expr_bin_start\n                    )\n                else:\n                    # Raw values - need to bin them\n                    expr_tokens = self._expression_to_bin_vectorized(\n                        np.array(exprs, dtype=np.float32)\n                    )\n\n                # Vectorized interleaving (much faster than Python loop)\n                if len(gene_tokens) &gt; 0:\n                    # Pre-allocate full sequence: CLS + (gene,expr)*n + SEP\n                    sequence_length = 1 + 2 * len(gene_tokens) + 1\n                    tokens = np.full(\n                        sequence_length, self.special_tokens[\"PAD\"], dtype=np.int64\n                    )\n\n                    # Set CLS token\n                    tokens[0] = self.special_tokens[\"CLS\"]\n\n                    # Vectorized interleaving\n                    tokens[1::2][: len(gene_tokens)] = gene_tokens  # type: ignore[assignment]\n                    tokens[2::2][: len(expr_tokens)] = expr_tokens  # type: ignore[assignment]\n\n                    tokens[1 + 2 * len(gene_tokens)] = self.special_tokens[\"SEP\"]\n                else:\n                    # Empty sequence case\n                    tokens = np.array(\n                        [self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]],\n                        dtype=np.int64,\n                    )  # type: ignore[assignment]\n            else:\n                # Empty sequence case\n                tokens = np.array(\n                    [self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]],\n                    dtype=np.int64,\n                )  # type: ignore[assignment]\n\n            # Pad/truncate to correct sequence length\n            if self.tokenizer_type == TokenizerType.SCPGPT:\n                # For scGPT: use max_sequence_length (2*max_genes+2)\n                target_length = max_sequence_length\n            else:\n                # For Geneformer: use max_genes\n                target_length = max_genes\n\n            tokens = tokens[:target_length]  # type: ignore[assignment]\n            if len(tokens) &lt; target_length:\n                padding = np.full(\n                    target_length - len(tokens),\n                    self.special_tokens[\"PAD\"],\n                    dtype=np.int64,\n                )\n                tokens = np.concatenate([tokens, padding])  # type: ignore[assignment]\n\n            # Fill array\n            token_array[i, :] = tokens  # type: ignore[assignment]\n\n    else:\n        # Geneformer format: [CLS] gene1 gene2 gene3 ... [SEP]\n        for i, gene_sequence in enumerate(gene_sequences):\n            # Convert gene IDs to tokens (fast mapping)\n            gene_tokens = np.array(gene_sequence, dtype=np.int64) + 4\n\n            # Vectorized sequence building: use concatenation for speed\n            if len(gene_tokens) &gt; 0:\n                # Use concatenation: CLS + genes + SEP\n                tokens = np.concatenate(\n                    [\n                        [self.special_tokens[\"CLS\"]],\n                        gene_tokens,\n                        [self.special_tokens[\"SEP\"]],\n                    ]\n                )  # type: ignore[assignment]\n            else:\n                # Empty sequence case\n                tokens = np.array(\n                    [self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]],\n                    dtype=np.int64,\n                )  # type: ignore[assignment]\n\n            # Pad/truncate to max_genes\n            tokens = tokens[:max_genes]  # type: ignore[assignment]\n            if len(tokens) &lt; max_genes:\n                padding = np.full(\n                    max_genes - len(tokens),\n                    self.special_tokens[\"PAD\"],\n                    dtype=np.int64,\n                )\n                tokens = np.concatenate([tokens, padding])  # type: ignore[assignment]\n\n            # Fill array\n            token_array[i, :] = tokens  # type: ignore[assignment]\n\n    # Convert to tensors in one operation\n    input_ids = torch.from_numpy(token_array)\n    attention_mask = input_ids != self.special_tokens[\"PAD\"]\n\n    return input_ids, attention_mask\n</code></pre>"},{"location":"api/ml/#slaf.ml.tokenizers.SLAFTokenizer.get_vocab_info","title":"<code>get_vocab_info() -&gt; dict[str, Any]</code>","text":"<p>Get vocabulary information for debugging and analysis.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Vocabulary information including size, special tokens, etc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; vocab_info = tokenizer.get_vocab_info()\n&gt;&gt;&gt; print(f\"Vocabulary size: {vocab_info['vocab_size']}\")\n&gt;&gt;&gt; print(f\"Special tokens: {vocab_info['special_tokens']}\")\nVocabulary size: 50000\nSpecial tokens: {'PAD': 0, 'CLS': 1, 'SEP': 2, 'MASK': 3}\n</code></pre> Source code in <code>slaf/ml/tokenizers.py</code> <pre><code>def get_vocab_info(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Get vocabulary information for debugging and analysis.\n\n    Returns:\n        dict: Vocabulary information including size, special tokens, etc.\n\n    Examples:\n        &gt;&gt;&gt; vocab_info = tokenizer.get_vocab_info()\n        &gt;&gt;&gt; print(f\"Vocabulary size: {vocab_info['vocab_size']}\")\n        &gt;&gt;&gt; print(f\"Special tokens: {vocab_info['special_tokens']}\")\n        Vocabulary size: 50000\n        Special tokens: {'PAD': 0, 'CLS': 1, 'SEP': 2, 'MASK': 3}\n    \"\"\"\n    return {\n        \"vocab_size\": self.vocab_size,\n        \"tokenizer_type\": self.tokenizer_type.value,\n        \"special_tokens\": self.special_tokens,\n        \"n_expression_bins\": self.n_expression_bins,\n        \"gene_vocab_size\": len(self.gene_vocab),\n    }\n</code></pre>"},{"location":"api/ml/#slaf.ml.tokenizers.SLAFTokenizer.decode_tokens","title":"<code>decode_tokens(tokens: list[int]) -&gt; dict[str, Any]</code>","text":"<p>Decode token sequence back to gene information.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[int]</code> <p>List of token IDs to decode</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Decoded information including genes, expressions, etc.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Decode a token sequence\n&gt;&gt;&gt; tokens = [1, 100, 50050, 200, 50060, 2]  # CLS, gene1, expr1, gene2, expr2, SEP\n&gt;&gt;&gt; decoded = tokenizer.decode_tokens(tokens)\n&gt;&gt;&gt; print(f\"Genes: {decoded['genes']}\")\n&gt;&gt;&gt; print(f\"Expressions: {decoded['expressions']}\")\nGenes: ['gene_100', 'gene_200']\nExpressions: [0.5, 0.6]\n</code></pre> Source code in <code>slaf/ml/tokenizers.py</code> <pre><code>def decode_tokens(self, tokens: list[int]) -&gt; dict[str, Any]:\n    \"\"\"\n    Decode token sequence back to gene information.\n\n    Args:\n        tokens: List of token IDs to decode\n\n    Returns:\n        dict: Decoded information including genes, expressions, etc.\n\n    Examples:\n        &gt;&gt;&gt; # Decode a token sequence\n        &gt;&gt;&gt; tokens = [1, 100, 50050, 200, 50060, 2]  # CLS, gene1, expr1, gene2, expr2, SEP\n        &gt;&gt;&gt; decoded = tokenizer.decode_tokens(tokens)\n        &gt;&gt;&gt; print(f\"Genes: {decoded['genes']}\")\n        &gt;&gt;&gt; print(f\"Expressions: {decoded['expressions']}\")\n        Genes: ['gene_100', 'gene_200']\n        Expressions: [0.5, 0.6]\n    \"\"\"\n    if not tokens:\n        return {\"genes\": [], \"expressions\": [], \"special_tokens\": []}\n\n    genes = []\n    expressions = []\n    special_tokens = []\n\n    i = 0\n    while i &lt; len(tokens):\n        token = tokens[i]\n\n        if token == self.special_tokens[\"CLS\"]:\n            special_tokens.append(\"CLS\")\n            i += 1\n        elif token == self.special_tokens[\"SEP\"]:\n            special_tokens.append(\"SEP\")\n            i += 1\n        elif token == self.special_tokens[\"PAD\"]:\n            special_tokens.append(\"PAD\")\n            i += 1\n        elif token == self.special_tokens[\"MASK\"]:\n            special_tokens.append(\"MASK\")\n            i += 1\n        elif (\n            self.tokenizer_type == TokenizerType.SCPGPT\n            and token &gt;= self.expr_bin_start\n        ):\n            # Expression token\n            bin_id = token - self.expr_bin_start\n            expr_value = bin_id * self.expr_bin_size\n            expressions.append(expr_value)\n            i += 1\n        else:\n            # Gene token\n            if token in self.token_to_gene:\n                genes.append(self.token_to_gene[token])\n            else:\n                genes.append(f\"unknown_gene_{token}\")\n            i += 1\n\n    return {\n        \"genes\": genes,\n        \"expressions\": expressions,\n        \"special_tokens\": special_tokens,\n    }\n</code></pre>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>This section provides comprehensive performance benchmarks comparing SLAF against traditional and modern single-cell data formats and dataloaders.</p>"},{"location":"benchmarks/#comprehensive-benchmarks","title":"Comprehensive Benchmarks","text":"<ul> <li>For Bioinformaticians - Detailed benchmarks for common bioinformatics operations including cell/gene filtering, expression queries, and preprocessing pipelines</li> <li>For ML Engineers - Performance analysis for machine learning workflows, dataloader throughput, and GPU training preparation</li> </ul>"},{"location":"benchmarks/#format-comparisons","title":"Format Comparisons","text":"<ul> <li>SLAF vs h5ad - Performance comparison against the traditional AnnData format</li> <li>SLAF vs TileDB - Comparison with modern cloud-native storage alternatives.</li> </ul> <p>Ready to see SLAF in action? Start with the bioinformatics benchmarks for analysis workflows or ML benchmarks for training pipelines.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/","title":"Bioinformatics Benchmarks: SLAF vs Traditional Formats","text":"<p>SLAF provides dramatic performance improvements over traditional single-cell data formats for common bioinformatics operations. This document presents comprehensive benchmarks comparing SLAF against h5ad (AnnData) and TileDB SOMA across realistic bioinformatics workflows.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#overview","title":"Overview","text":"<p>These benchmarks demonstrate SLAF's performance advantages in three key areas:</p> <ol> <li>Metadata Filtering - Cell and gene filtering operations</li> <li>Expression Queries - Retrieving expression data for specific cells/genes</li> <li>Preprocessing Pipelines - Scanpy-based preprocessing workflows</li> </ol>"},{"location":"benchmarks/bioinformatics_benchmarks/#test-dataset-synthetic_50k_processed","title":"Test Dataset: synthetic_50k_processed","text":"<ul> <li>Cells: 49,955 cells</li> <li>Genes: 25,000 genes</li> <li>Input File Size: ~722MB h5ad file</li> </ul>"},{"location":"benchmarks/bioinformatics_benchmarks/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li>Machine: Apple MacBook Pro with M1 Max</li> <li>Memory: 32 GB RAM</li> <li>Storage: 1 TB NVMe SSD (local disk)</li> <li>OS: macOS 13.6.1</li> <li>Python: 3.12.0</li> </ul>"},{"location":"benchmarks/bioinformatics_benchmarks/#performance-summary-across-all-categories","title":"Performance Summary Across All Categories","text":"<p>SLAF vs h5ad Speedup:</p> <pre><code>Speedup (x faster)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCell Filtering     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 92.3x\nGene Filtering     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 17.3x\nExpression Queries \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 9.5x\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    0   20   40   60   80  100  120\n</code></pre> <p>SLAF vs TileDB Speedup:</p> <pre><code>Speedup (x faster)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nCell Filtering     \u2588\u2588\u2588\u2588 9.4x\nGene Filtering     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 8.6x\nExpression Queries \u2588 0.9x (competitive)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    0    2    4    6    8   10   12\n</code></pre>"},{"location":"benchmarks/bioinformatics_benchmarks/#cell-filtering-benchmarks","title":"Cell Filtering Benchmarks","text":"<p>Cell filtering is a fundamental operation in single-cell analysis, used for quality control, cell type selection, and data subsetting.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#performance-results","title":"Performance Results","text":"Scenario h5ad Total (ms) SLAF Total (ms) TileDB Total (ms) SLAF vs h5ad SLAF vs TileDB Description S1 530.0 2.9 20.9 183.6x 7.3x Cells with &gt;=500 genes S2 169.7 2.0 21.3 83.3x 10.5x High UMI count (total_counts &gt; 2000) S3 170.7 1.9 21.3 92.2x 11.5x Mitochondrial fraction &lt; 0.1 S4 177.1 2.0 18.6 86.7x 9.1x Complex multi-condition filter S5 186.6 2.8 18.1 67.2x 6.5x Cell type annotation filter S6 171.4 2.0 20.9 86.3x 10.5x Cells from batch_1 S7 207.0 2.3 23.8 89.4x 10.3x Cells in clusters 0,1 from batch_1 S8 170.9 2.1 23.0 79.6x 10.7x High-quality cells (&gt;=1000 genes, &lt;=10% mt) S9 172.1 2.5 19.3 70.2x 7.9x Cells with 800-2000 total counts S10 173.5 2.1 20.8 84.6x 10.1x Cells with 200-1500 genes <p>Average Performance:</p> <ul> <li>SLAF vs h5ad: 92.3x faster</li> <li>SLAF vs TileDB: 9.4x faster</li> <li>Memory Usage: SLAF uses 115.7x less memory than h5ad</li> </ul> <p>Cell Filtering Performance Summary:</p> <pre><code>Speedup (x faster)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSLAF vs h5ad        \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 92.3x\nSLAF vs TileDB      \u2588\u2588\u2588\u2588 9.4x\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    0   20   40   60   80  100  120\n</code></pre>"},{"location":"benchmarks/bioinformatics_benchmarks/#key-insights","title":"Key Insights","text":"<p>Dramatic Performance Advantage</p> <p>SLAF achieves 92.3x average speedup over h5ad for cell filtering operations, demonstrating the massive performance benefits of modern columnar storage and optimized querying.</p> <p>Columnar Format Efficiency</p> <p>Both SLAF and TileDB (Arrow-interoperable formats) significantly outperform h5ad, with SLAF providing an additional 9.4x advantage over TileDB through its optimized streaming architecture.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#gene-filtering-benchmarks","title":"Gene Filtering Benchmarks","text":"<p>Gene filtering operations are essential for feature selection, quality control, and differential expression in single-cell analysis.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#performance-results_1","title":"Performance Results","text":"Scenario h5ad Total (ms) SLAF Total (ms) TileDB Total (ms) SLAF vs h5ad SLAF vs TileDB Description S1 43.4 3.0 22.3 14.6x 7.5x Genes expressed in &gt;=10 cells S2 32.3 1.7 19.5 19.4x 11.7x Genes with &gt;=100 total counts S3 32.1 1.8 9.3 17.4x 5.0x Genes with mean expression &gt;=0.1 S4 31.1 1.6 15.8 19.9x 10.1x Exclude mitochondrial genes S5 32.7 1.7 16.9 19.7x 10.2x Highly variable genes S6 31.7 2.1 15.8 15.4x 7.7x Non-highly variable genes S7 31.5 2.0 18.2 15.8x 9.1x Genes in &gt;=50 cells with &gt;=500 total counts S8 31.7 1.9 19.8 17.0x 10.6x Genes with 100-10000 total counts S9 33.2 2.0 11.3 16.4x 5.6x Genes in 5-1000 cells <p>Average Performance:</p> <ul> <li>SLAF vs h5ad: 17.3x faster</li> <li>SLAF vs TileDB: 8.6x faster</li> <li>Memory Usage: SLAF uses 2.2x less memory than h5ad</li> </ul> <p>Gene Filtering Performance Summary:</p> <pre><code>Speedup (x faster)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSLAF vs h5ad        \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 17.3x\nSLAF vs TileDB      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 8.6x\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    0    5   10   15   20   25   30\n</code></pre>"},{"location":"benchmarks/bioinformatics_benchmarks/#key-insights_1","title":"Key Insights","text":"<p>Consistent High Performance</p> <p>SLAF maintains consistent 14x+ speedups across all gene filtering scenarios, demonstrating robust optimization of Polars operations and modern storage formats.</p> <p>Memory Efficiency</p> <p>Gene filtering operations show moderate memory efficiency gains, with SLAF using 2.2x less memory than h5ad for equivalent operations.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#expression-queries-benchmarks","title":"Expression Queries Benchmarks","text":"<p>Expression queries retrieve specific expression data for cells or genes, supporting analysis workflows that require targeted data access.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#performance-results_2","title":"Performance Results","text":"Scenario h5ad Total (ms) SLAF Total (ms) TileDB Total (ms) SLAF vs h5ad SLAF vs TileDB Description S1 484.5 16.1 63.8 30.1x 4.0x Single cell expression S2 251.3 13.9 19.3 18.1x 1.4x Another single cell S3 328.3 14.2 18.1 23.1x 1.3x Two cells S4 233.2 15.5 19.0 15.1x 1.2x Three cells S5 232.7 523.7 150.8 0.4x 0.3x Single gene across all cells S6 203.4 442.6 84.2 0.5x 0.2x Another single gene S7 256.1 303.0 97.7 0.8x 0.3x Two genes S8 212.0 655.9 83.3 0.3x 0.1x Three genes S9 221.4 22.5 9.9 9.9x 0.4x 100x50 submatrix S10 168.3 61.9 12.1 2.7x 0.2x 500x100 submatrix S11 212.2 63.2 10.8 3.4x 0.2x 500x500 submatrix <p>Average Performance:</p> <ul> <li>SLAF vs h5ad: 9.5x faster</li> <li>SLAF vs TileDB: 0.9x faster</li> <li>Memory Usage: SLAF uses 154.6x less memory than h5ad</li> </ul> <p>Expression Queries Performance Summary:</p> <pre><code>Speedup (x faster)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nSLAF vs h5ad        \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 9.5x\nSLAF vs TileDB      \u2588 0.9x (competitive)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                    0    2    4    6    8   10   12\n</code></pre>"},{"location":"benchmarks/bioinformatics_benchmarks/#key-insights_2","title":"Key Insights","text":"<p>Query Optimization</p> <p>SLAF's expression query performance demonstrates efficient sparse matrix operations and optimized data access patterns, achieving 9.5x average speedup over h5ad.</p> <p>TileDB's Expression Query Strengths</p> <p>TileDB demonstrates impressive performance for gene expression queries and submatrix operations, often outperforming both SLAF and h5ad. For single gene queries across all cells (S5-S8), TileDB shows 2.5-10x speedup over SLAF, highlighting its optimized columnar access patterns for gene-centric operations.</p> <p>SLAF's Cell-Centric Advantages</p> <p>SLAF maintains strong performance for cell-centric queries (S1-S4, S9-S11), achieving 15-30x speedup over h5ad for single cell and submatrix operations, while TileDB shows competitive or superior performance for larger submatrices.</p> <p>Mixed Performance Profile</p> <p>The benchmarks reveal a nuanced performance landscape: SLAF excels at cell-centric operations and metadata filtering, while TileDB demonstrates superior performance for gene-centric expression queries and large submatrix operations. This suggests different systems may be optimal for different analysis workflows.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#lazy-computation-preprocessing-pipelines","title":"Lazy Computation (Preprocessing Pipelines)","text":"<p>SLAF enables lazy computation graphs that build complex preprocessing pipelines and only execute them on the slice of interest, similar to Dask's delayed computation patterns.</p>"},{"location":"benchmarks/bioinformatics_benchmarks/#traditional-approach-eager-processing","title":"Traditional Approach (Eager Processing)","text":"<pre><code># Each step loads data into memory\nadata = sc.read_h5ad(\"data.h5ad\", backed=\"r\")\n\n# QC metrics calculation\nsc.pp.calculate_qc_metrics(adata, inplace=True)\n\n# Cell filtering with min_counts and min_genes\nsc.pp.filter_cells(adata, min_counts=500, min_genes=200, inplace=True)\n\n# Gene filtering\nsc.pp.filter_genes(adata, min_counts=10, min_cells=5, inplace=True)\n\n# Normalization\nsc.pp.normalize_total(adata, target_sum=1e4, inplace=True)\n\n# Log transformation\nsc.pp.log1p(adata)\n\n# Each operation processes the entire dataset\nexpression = adata.X[cell_ids, gene_ids]\n</code></pre>"},{"location":"benchmarks/bioinformatics_benchmarks/#slaf-approach-lazy-computation-graph","title":"SLAF Approach (Lazy Computation Graph)","text":"<pre><code># Build lazy computation graph\nadata = LazyAnnData(\"data.slaf\")  # LazyAnnData object\n\n# QC metrics calculation (lazy)\npp.calculate_qc_metrics(adata, inplace=True)\n\n# Cell filtering (lazy)\npp.filter_cells(adata, min_counts=500, min_genes=200, inplace=True)\n\n# Gene filtering (lazy)\npp.filter_genes(adata, min_counts=10, min_cells=5, inplace=True)\n\n# Normalization (lazy)\npp.normalize_total(adata, target_sum=1e4, inplace=True)\n\n# Log transformation (lazy)\npp.log1p(adata)\n\n# Only execute the computation on the slice of interest\nexpression = adata.X[cell_ids, gene_ids].compute()  # LazyExpressionMatrix.compute()\n</code></pre>"},{"location":"benchmarks/bioinformatics_benchmarks/#performance-results_3","title":"Performance Results","text":"Operation Traditional Total (ms) SLAF Total (ms) Total Speedup Memory Efficiency Description S1 1150.4 2577.0 0.4x 148.1x Calculate QC metrics S2 941.0 2045.1 0.5x 1.5x Filter cells (min_counts=500, min_genes=200) S3 929.7 2058.9 0.5x 1.5x Filter cells (min_counts=100, min_genes=50) S4 862.1 2184.9 0.4x 1.5x Filter cells (max_counts=10000, max_genes=3000) S5 1208.9 2586.7 0.5x 1.5x Filter genes (min_counts=10, min_cells=5) S6 1183.5 2516.4 0.5x 1.5x Filter genes (min_counts=20, min_cells=5) S7 436.2 3675.7 0.1x 1.5x Normalize total (target_sum=1e4) S8 410.9 2843.9 0.1x 1.5x Normalize total (target_sum=1e6) S9 628.2 2083.7 0.3x 0.0x Log1p transformation S10 653.9 1660.6 0.4x 98.9x Find highly variable genes S11 695.6 1642.6 0.4x 98.9x Find top 2000 highly variable genes S12 2359.9 6418.5 0.4x 1.5x QC metrics + cell filtering + gene filtering S13 338.8 3417.2 0.1x 2.0x Normalize total + slice 100x50 submatrix (lazy) S14 617.9 2174.4 0.3x 2.0x Log1p + slice 200x100 submatrix (lazy) S15 627.9 3510.9 0.2x 2.0x Normalize + Log1p + slice 500x250 submatrix (lazy) S16 922.6 3061.3 0.3x 1.9x Normalize + Log1p + mean per gene (lazy) S17 620.4 3174.6 0.2x 1.8x Normalize + Log1p + variance per cell (lazy) <p>Key Insight: Lazy computation enables complex preprocessing pipelines that would cause memory explosions with traditional tools. The computation cost is paid when materializing results, but the memory efficiency enables workflows impossible with eager processing. This is similar to Dask's delayed computation patterns.</p> <p>For detailed migration guides, see SLAF vs h5ad Benchmarks and SLAF vs TileDB Benchmarks.</p>"},{"location":"benchmarks/ml_benchmarks/","title":"ML Benchmarks: SLAF vs State-of-the-Art Dataloaders","text":"<p>SLAF provides state-of-the-art (SOTA) performance in data loading throughput for machine learning workflows, reaching 2.6x speedups relative to current SOTA, particularly for training transformer-based single-cell foundation models. What follows are comprehensive benchmarks comparing SLAF against state-of-the-art dataloaders including scDataset, BioNeMo SCDL, AnnDataLoader, AnnLoader, and TileDB DataLoader.</p>"},{"location":"benchmarks/ml_benchmarks/#motivation","title":"Motivation","text":"<p>The goal of these benchmarks is to demonstrate that SLAF can stream tokens to modern GPUs at a rate sufficient to prevent idle time between training loops. For a 1B parameter model like scGPT, fast enough means delivering training batches within 50 ms to keep the GPU utilization high. This benchmark establishes SLAF's ability to meet the throughput requirements for efficient foundation model training on massive single-cell datasets.</p>"},{"location":"benchmarks/ml_benchmarks/#dataset-and-hardware","title":"Dataset and Hardware","text":""},{"location":"benchmarks/ml_benchmarks/#dataset-tahoe-100m","title":"Dataset: Tahoe-100M","text":"<p>We downloaded one of the 7 h5ad files comprising the Tahoe-100M dataset made accessible by ARC Institute. This slice of the dataset contains 5,481,420 cells and 62,710 genes, with approximately 8B non-zero expression values. All benchmarks reported below used this dataset except for the TileDB dataloader since we couldn't successfully convert a 5M-cell dataset to the Tile DB SOMA Experiment format with 32G RAM. For the TileDB DataLoader alone, we report numbers on a smaller 50k-cell synthetic dataset.</p>"},{"location":"benchmarks/ml_benchmarks/#conversion-and-optimization","title":"Conversion and Optimization","text":"<p>We used the SLAF converter (see Migrating to SLAF) to convert the h5ad file to SLAF format. The Lance table fragments (Lance's term for partitions) were optimized for compression/query tradeoffs, with 5-10M non-zeros (rows) per fragment in the expression table. While inherently parallelizable, conversion is currently single process, and took about 10 minutes for this dataset.</p>"},{"location":"benchmarks/ml_benchmarks/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li>Machine: Apple MacBook Pro with M1 Max</li> <li>Memory: 32 GB RAM</li> <li>Storage: 1 TB NVMe SSD</li> <li>OS: macOS 13.6.1</li> </ul> <p>Note</p> <p>These benchmarks represent performance on a high-end laptop. Production deployments on dedicated servers with faster storage may show different performance characteristics. Likewise, performance from object storage to non-colocated compute might be worse.</p>"},{"location":"benchmarks/ml_benchmarks/#internal-benchmarks","title":"Internal Benchmarks","text":""},{"location":"benchmarks/ml_benchmarks/#methodology","title":"Methodology","text":"<p>We used a batch size of 32 with an enhanced warmup and measurement procedure to ensure accurate and consistent results, especially for the Mixture of Scanners (MoS) strategy:</p> <ul> <li>Initial Warmup: 15 batches to initialize the dataloader</li> <li>Extended Warmup: 10 seconds to allow MoS to fully stabilize all fragment generators</li> <li>Measurement Period: 40 seconds of pure performance measurement (excluding warmup time)</li> <li>Total Runtime: 50 seconds per benchmark (10s warmup + 40s measurement)</li> </ul> <p>This methodology ensures that all dataloader strategies reach steady-state performance before measurement begins, eliminating variance from incomplete initialization.</p>"},{"location":"benchmarks/ml_benchmarks/#tokenization-strategy-comparison","title":"Tokenization Strategy Comparison","text":"<p>We benchmarked different tokenization strategies to understand the performance impact of various preprocessing options:</p> Tokenization Strategy Throughput (cells/sec) Throughput (tokens/sec) scGPT with binning 5,350 10,967,687 scGPT without binning 5,157 10,572,411 Geneformer with percentile filtering 6,999 14,335,137 Geneformer without percentile filtering 6,494 13,300,337 Raw mode (no tokenization) 22,323 N/A <p>Tokenization Strategy Comparison:</p> <pre><code>Throughput (cells/sec)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nRaw mode              \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 22,323\nGeneformer (filter)   \u2588\u2588\u2588\u2588\u2588\u2588\u2588 6,999\nGeneformer (no filt)  \u2588\u2588\u2588\u2588\u2588\u2588 6,494\nscGPT (binning)       \u2588\u2588\u2588\u2588\u2588 5,350\nscGPT (no binning)    \u2588\u2588\u2588\u2588\u2588 5,157\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                      0    5K   10K   15K   20K   25K\n</code></pre> <p>Strategy Insights</p> <ul> <li>Geneformer strategies show ~30% higher throughput than scGPT strategies</li> <li>Binning and filtering have minimal performance impact (~7% difference)</li> <li>Raw mode provides 3.4x higher throughput than tokenized modes, demonstrating the tokenization overhead</li> </ul>"},{"location":"benchmarks/ml_benchmarks/#raw-mode-performance-scaling","title":"Raw Mode Performance Scaling","text":"<p>Raw mode bypasses tokenization and returns Polars DataFrames that have the exact schema as sparse CSR tensors, demonstrating SLAF's base data loading performance.</p> Batch Size Throughput (cells/sec) Total Cells Measurement Time (s) 32 23,783 713,577 30.0 64 25,259 765,957 30.3 128 28,079 842,394 30.0 256 28,169 850,146 30.2 <p>Raw Mode Batch Size Scaling:</p> <pre><code>Throughput (cells/sec)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nBatch 256    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 28,169\nBatch 128    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 28,079\nBatch 64     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 25,259\nBatch 32     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 23,783\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             0    5K   10K   15K   20K   25K   30K\n</code></pre> <p>Optimization Validation</p> <p>Raw mode throughput shows 1.2x improvement from batch size 32 to 256, demonstrating that SLAF's data loading pipeline scales efficiently with larger batch sizes while maintaining high performance.</p>"},{"location":"benchmarks/ml_benchmarks/#fragment-vs-batch-loading-comparison","title":"Fragment vs Batch Loading Comparison","text":"<p>SLAF supports two loading strategies: fragment-based and batch-based loading. Fragment-based loading processes entire Lance fragments at once, while batch-based loading processes multiple Lance batches sequentially.</p> Strategy Throughput (cells/sec) Total Cells Total Batches Fragment-Based Loading 22,472 229,669 7,180 Batch-Based Loading 24,354 243,554 8,038 <p>Fragment Strategy Performance</p> <p>Batch-based loading shows modestly higher throughput than fragment-based loading in this benchmark, but test-retest repeatability shows high variance. The performance difference should not be overinterpreted as it may vary significantly across different runs and hardware configurations.</p> <p>Strategy Selection</p> <p>Mixture of Scanners (MoS) is the default strategy in SLAF for foundation model training, providing 88% of random entropy with only 3.2% throughput penalty. Sequential loading is available for maximum throughput by setting <code>use_mixture_of_scanners=False, by_fragment=False</code> to the SLAFDataLoader for users who prioritize speed over entropy.</p>"},{"location":"benchmarks/ml_benchmarks/#tokenized-mode-tokenssec-scaling","title":"Tokenized Mode: Tokens/sec Scaling","text":"<p>Tokenized mode provides pre-tokenized sequences ready for GPU training, demonstrating SLAF's end-to-end pipeline performance.</p> Batch Size Throughput (cells/sec) Throughput (tokens/sec) Total Cells Measurement Time (s) 32 7,141 14,624,846 215,990 30.2 64 7,147 14,637,356 223,872 31.3 128 7,309 14,969,420 224,663 30.7 256 7,269 14,885,945 224,511 30.9 <p>Tokenized Mode Throughput Scaling:</p> <pre><code>Throughput (tokens/sec, millions)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nBatch 128    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 14.97M\nBatch 256   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 14.89M\nBatch 64    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 14.64M\nBatch 32    \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 14.62M\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n             0    3M    6M    9M   12M   15M   18M\n</code></pre> <p>Tokenization Efficiency</p> <p>Token throughput remains remarkably constant across batch sizes (1.0x scaling), demonstrating that SLAF's tokenization pipeline is well-optimized and not the bottleneck. This validates that tokens/sec is the meaningful metric for GPU training workloads.</p>"},{"location":"benchmarks/ml_benchmarks/#entropy-measurement-training-batch-randomness","title":"Entropy Measurement: Training Batch Randomness","text":"<p>To ensure models don't converge to local minima due to biased and highly correlated training batches, we want to make training batches as random as possible. However, random reads are more expensive than sequential reads, so we need to balance randomness with performance.</p> <p>To address this challenge, we developed a novel dataloader strategy called the Mixture of Scanners (MoS) approach, which randomly tasks a small randomized group of scanners to populate a queue of training batches by reading from different starting points of the dataset. A deeper dive into our approach to optimize dataloaders is available here and a more detailed write up of the MoS dataloader is here.</p> <p>To measure entropy without using metadata, we simulate random cell IDs and measure L1 distance between pairs of cell IDs both within and across adjacent training batches for our different dataloaders to show how each dataloader strategy performs relative to a purely sequential (lowerbound) vs a truly random approach (upperbound).</p> <p>We ran a test on 10,000 batches with a batch_size of 32 from a 5.4M cell dataset and found these results:</p> <p>Entropy Measurement Results:</p> Strategy Within-Batch L1 Across-Batch L1 sequential 94.1 104.5 fragment 1,643.5 1,672.6 mos 1,608,648.2 1,642,829.9 random 1,828,595.2 1,824,468.9 <p>Normalized Entropy Scores [0=Sequential, 1=Random]:</p> Strategy Within-Batch L1 Across-Batch L1 sequential 0.000 0.000 fragment 0.001 0.001 mos 0.880 0.900 <p>Throughput Performance Results:</p> Strategy Throughput (cells/sec) Total Cells Total Batches sequential 23,728 711,990 23,509 fragment 26,769 803,072 25,216 mos 22,972 689,234 21,546 <p>Entropy Strategy Performance</p> <ul> <li>Sequential loading provides the lowest entropy (0.000), with contiguous cell IDs from Lance batches</li> <li>Fragment-based loading shows minimal improvement (0.001), processing complete Lance fragments for slightly higher entropy</li> <li>Mixture of Scanners (MoS) achieves near-random entropy (0.88+), demonstrating effective randomization while maintaining high throughput</li> <li>MoS approach provides 88% of the entropy of truly random sampling while maintaining the performance benefits of structured data access</li> </ul> <p>Throughput Performance Analysis</p> <ul> <li>Fragment-based loading achieves the highest throughput (26,769 cells/sec), showing 12.8% improvement over sequential loading</li> <li>MoS approach maintains competitive throughput (22,972 cells/sec), only 3.2% slower than sequential loading despite providing 88% random entropy</li> <li>Performance-entropy trade-off: MoS successfully balances high entropy (0.88) with minimal throughput penalty (3.2% vs sequential)</li> <li>All strategies maintain excellent throughput (&gt;22K cells/sec), demonstrating SLAF's efficient data loading architecture</li> </ul> <p>Entropy Interpretation Guide</p> <ul> <li>Within-Batch: How random are the cells within each batch</li> <li>Across-Batch: How much batch composition changes between batches</li> <li>L1 Distance: Mean absolute difference between cell ID pairs</li> <li>Scores closer to 0 = more sequential, closer to 1 = more random</li> </ul> <p>MoS Implementation Benefits</p> <p>The Mixture of Scanners approach successfully balances the competing demands of training batch randomness and data loading performance. By using multiple scanners reading from different dataset locations, MoS achieves 88% of the entropy of truly random sampling without creating pre-randomized copies of datasets. The approach maintains 96.8% of sequential loading throughput while providing near-random batch composition, making it ideal for training foundation models that require both high throughput and effective batch randomization.</p>"},{"location":"benchmarks/ml_benchmarks/#external-benchmarks","title":"External Benchmarks","text":""},{"location":"benchmarks/ml_benchmarks/#alternate-dataloaders","title":"Alternate Dataloaders","text":"<p>We compared SLAF against six state-of-the-art dataloaders:</p> <ol> <li>annbatch - High-performance data loader for minibatching on-disk AnnData, co-developed by lamin and scverse</li> <li>AnnLoader - Experimental PyTorch DataLoader for AnnData objects from <code>anndata.experimental</code></li> <li>AnnDataLoader - From scvi-tools, designed for training variational autoencoder (VAE)-style models</li> <li>scDataset - Recently released high-performance dataloader with multiprocessing support</li> <li>TileDB DataLoader - An internal custom PyTorch DataLoader for TileDB SOMA experiments</li> <li>BioNeMo SCDL - NVIDIA's single-cell data loading framework for scalable training of foundation models</li> </ol>"},{"location":"benchmarks/ml_benchmarks/#methodology_1","title":"Methodology","text":"<p>To match the benchmarks from the scDataset paper as closely as possible, we used a <code>batch_size=64</code> across all comparisons. For scDataset itself, we used the optimal parameters in our hardware (<code>block_size=8</code>, <code>fetch_factor=64</code>, which were different from the ones found to be optimal in the paper). However, we couldn't use <code>num_workers=12</code> out of the box because h5ad datasets aren't pickle-able and PyTorch DataLoaders expect this since they use multiprocessing.</p> <p>Enhanced Measurement Procedure: All external benchmarks now use the same enhanced measurement procedure as internal benchmarks for fair comparison:</p> <ul> <li>Initial Warmup: 15 batches to initialize each dataloader</li> <li>Extended Warmup: 10 seconds to allow all systems to reach steady state</li> <li>Measurement Period: 30 seconds of pure performance measurement (excluding warmup time)</li> <li>Total Runtime: 40 seconds per benchmark (10s warmup + 30s measurement)</li> </ul> <p>This ensures fair and consistent performance comparisons across all dataloader systems.</p>"},{"location":"benchmarks/ml_benchmarks/#tier-1-raw-data-loading-comparison","title":"Tier 1: Raw Data Loading Comparison","text":"<p>Raw data loading performance measures the base throughput of each system without any tokenization overhead. All benchmarks use <code>batch_size=64</code> for consistent comparison.</p> System Throughput (cells/sec) annbatch 68,867 SLAF 22,399 BioNeMo SCDL 2,976 scDataset 2,550 TileDB DataLoader (MoS) 601 AnnDataLoader 411 AnnLoader 251 <p>Throughput Comparison Chart:</p> <pre><code>Throughput (cells/sec)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nannbatch          \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 68,867\nSLAF              \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 22,399\nBioNeMo SCDL      \u2588\u2588 2,976\nscDataset         \u2588\u2588 2,550\nTileDB (MoS)      \u2588 601\nAnnDataLoader      \u2588 411\nAnnLoader          \u2588 251\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                  0   10K   20K   30K   40K   50K   60K   70K\n</code></pre> <p>SOTA Performance</p> <p>SLAF achieves 8.8x higher throughput than scDataset, 7.5x higher throughput than BioNeMo SCDL, 37.3x higher throughput than TileDB DataLoader, 54.5x higher throughput than AnnDataLoader, and 89.2x higher throughput than AnnLoader in raw data loading.</p> <p>annbatch Performance Analysis</p> <p>annbatch demonstrates exceptional raw data loading performance, achieving 68,867 cells/sec\u20143.1x higher than SLAF's throughput. This performance advantage stems from fundamental storage format differences: annbatch uses CSR (Compressed Sparse Row) format via zarr, which is optimized specifically for row-wise batch loading operations common in ML training workflows.</p> <p>SLAF, in contrast, uses COO (Coordinate) format, which provides superior flexibility across multiple use cases. This design choice reflects SLAF's broader mission: to serve as a single unified format that efficiently supports (1) low-latency cell and gene queries, (2) batch processing operations, and (3) ML training workloads\u2014all from the same stored representation without data duplication. The COO format enables SLAF to maintain a \"store once, query in place\" philosophy across these diverse workloads, trading some raw loading throughput for greater versatility and query performance.</p> <p>For users whose primary use case is high-throughput ML training on pre-shuffled datasets, annbatch's CSR-based approach provides excellent performance. For users requiring a single format that supports both training and analytical queries, SLAF's COO-based architecture offers a more balanced solution.</p> <p>scDataset Performance Analysis</p> <p>Our comprehensive benchmarks reveal that scDataset achieves 2,550 cells/sec with optimized parameters (<code>block_size=8</code>, <code>fetch_factor=64</code>). This performance is consistent with the system's design, though lower than our initial expectations. Note that these benchmarks use different hardware (M1 Max) than the scDataset paper's reported results (NVIDIA DGX CPU), which may account for some performance differences.</p> <p>However, we found significant limitations with multiprocessing due to pickling issues with h5py-backed AnnData objects. See our detailed scDataset benchmarks for complete analysis including parameter scaling and multiprocessing limitations.</p> <p>Parameter Scaling Validation</p> <p>Our parameter sweeps confirm scDataset's strong scaling behavior: 23.1x improvement from worst to best configuration. The <code>fetch_factor</code> parameter shows the strongest scaling (20x+ improvement), while <code>block_size</code> shows more moderate effects. This validates the design approach described in their paper, though optimal parameters may vary by hardware.</p> <p>Multiprocessing Limitations</p> <p>We were unable to test <code>num_workers &gt; 0</code> due to pickling errors with h5py objects. We're still working with the scDataset team to figure out implementation differences.</p>"},{"location":"benchmarks/ml_benchmarks/#tier-2-gpu-ready-output-comparison","title":"Tier 2: GPU-Ready Output Comparison","text":"<p>Raw data loading benchmarks are great, provided that we intend to train on gene expression counts directly. However, for modern foundation models like Geneformer, scGPT, Transcriptformer, or STATE, cell sentences are constructed using tokens that represent gene identity and expression bins. A lot of these workflows require dataframe-friendly operations like sorting, windowing, ranking, and filtering. Our view is that it much better to situate these computations within the (typically) CPU-bound dataloader, rather than expect the GPU in the training loop to do the heavy lifting. Accordingly, SLAF dataloaders take a tokenizer and transform raw data into training-ready token sequences.</p> <p>This GPU-ready throughput measures end-to-end performance including tokenization (that includes windowing, ranking, vocabulary mapping and padding), which is critical for training workflows involving models that turn cells into sentences.</p> <p>Even though SLAF's tokenizing dataloaders do more work (tokenization), we find that their throughput remains competitive with scDataset's raw-data dataloader, achieving comparable performance despite the additional processing overhead.</p> System Throughput (cells/sec) Throughput (tokens/sec) SLAF 7,487 15,332,896 <p>GPU-Ready Cell Sentences</p> <p>SLAF dataloaders provide the only GPU-ready input among the available alternatives.</p>"},{"location":"benchmarks/ml_benchmarks/#some-discrepancies","title":"Some Discrepancies","text":"<p>Tokens/sec is better than Cells/sec</p> <p>Cells/sec can be a misleading metric for GPU training workloads. A better measure of throughput to minimize GPU idle time is tokens/sec, since pre-made token sequences are ready for matrix multiplication on the GPU. SLAF's tokenized mode demonstrates this principle: while cells/sec decreases due to tokenization overhead, relative to the raw mode, the constant tokens/sec across batch sizes shows that the tokenization pipeline is well-optimized across scales.</p> <p>Scaling behaviors reveal hidden optimization opportunities</p> <p>SLAF's constant scaling with batch size suggests that the loading and processing are impedance matched: loading more data per batch does not slow down throughput. Constant dataloader throughput for larger batch sizes implies that the bottleneck to batch size is not dataloading but GPU memory. In contrast, we observed that scDataset's throughput scales linearly with batch size (not shown in these results), suggesting that it is doing more work than needed at small batch sizes, and could achieve better performance with optimizations like async prefetching.</p> <p>In-memory formats matter</p> <p>The performance difference between AnnDataLoader (422 cells/sec) and scDataset (9,550 cells/sec) is dramatic. While scDataset is smarter at batching and randomization, since our benchmark tests them on loading from h5ad, it's important to compare apples to apples dataloader outputs. AnnDataLoader and AnnLoader return <code>torch.sparse_csr</code> tensors whereas scDataset returns <code>scipy.sparse.csr_matrix</code>, and these format inter-conversions represent non-zero overhead.</p> <p>In our work, we noticed different overheads for conversion from polars dataframe (SLAF's preferred format for raw data) to torch and scipy sparse formats, and ultimately decided to keep raw outputs in polars. The performance of AnnLoader and AnnDataLoader relative to scDataset is almost certainly due to the overhead of conversion from scipy sparse arrays to torch arrays and worth benchmarking more carefully to identify low-hanging fruit for optimizations in both AnnLoader and AnnDataLoader.</p>"},{"location":"benchmarks/ml_benchmarks/#conclusion","title":"Conclusion","text":""},{"location":"benchmarks/ml_benchmarks/#cloud-native-architecture","title":"Cloud-Native Architecture","text":"<p>While these benchmarks use local SSD, the Lance format is native to cloud storage. Early tests suggest that latency between S3 and EC2 in the same region is not appreciably different from local storage. This opens up a cloud-native, store-once, query-multiple-times zero-copy architecture that eliminates data duplication.</p>"},{"location":"benchmarks/ml_benchmarks/#gpu-throughput-requirements","title":"GPU Throughput Requirements","text":"<p>What's a good enough cells/sec rate to keep an 8\u00d7H100 node at $2/hr busy? Assuming 50 ms per training loop for a model like scGPT:</p> <ul> <li>8 GPUs \u00d7 32 cells/batch \u00d7 20 batches/sec = 5,120 cells/sec would maximize GPU utilization</li> <li>Tahoe-100M training: 100M cells \u00f7 5,120 cells/sec = ~5.4 hours per epoch ~ $86 / epoch</li> <li>Anything faster than 5,120 cells/sec opens up multi-node training possibilities, trading off cost and wall clock time.</li> </ul> <p>This raises the question: can we build towards a $100 scGPT model through efficient multi-node training enabled by high-throughput data loading? More on this soon!</p>"},{"location":"benchmarks/ml_benchmarks/#high-concurrency-and-multi-user-training","title":"High Concurrency and Multi-User Training","text":"<p>The Lance format's high concurrency, optimized for production multimodal data lakes with high QPS, enables not only multi-node training but multiple users training multiple models simultaneously without their own copies of the dataset. This contrasts with h5ad, which requires:</p> <ol> <li>Local storage: The dataset must be local to the CPU instance loading it for attached GPUs</li> <li>Non-concurrent access: One copy of the dataset per user</li> </ol> <p>SLAF, with Lance under the hood, enables a truly scalable architecture for foundation model training on massive single-cell datasets.</p> <p>These benchmarks demonstrate SLAF's position as the leading solution for high-performance single-cell data loading, enabling efficient training of foundation models on massive datasets with minimal resource requirements and maximum scalability.</p> <p>SLAF is a young project with a bus factor of 1. You can help improve that by using it and contributing to it. Read about the SLAF vision in this blog post and contribute at github.com/slaf-project/slaf.</p>"},{"location":"benchmarks/scdataset_benchmarks/","title":"scDataset Benchmarks","text":"<p>This document reports comprehensive benchmark results for scDataset, a high-performance dataloader for single-cell data. These benchmarks were conducted using the Tahoe-100M dataset to evaluate scDataset's performance characteristics and scaling behavior.</p>"},{"location":"benchmarks/scdataset_benchmarks/#dataset-and-hardware","title":"Dataset and Hardware","text":""},{"location":"benchmarks/scdataset_benchmarks/#dataset-tahoe-100m","title":"Dataset: Tahoe-100M","text":"<ul> <li>Cells: 5,481,420 cells</li> <li>Genes: 62,710 genes</li> <li>Format: h5ad (backed mode)</li> <li>Batch Size: 64 cells per batch</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#hardware-configuration","title":"Hardware Configuration","text":"<ul> <li>Machine: Apple MacBook Pro with M1 Max</li> <li>Memory: 32 GB RAM</li> <li>Storage: 1 TB NVMe SSD</li> <li>OS: macOS 13.6.1</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#parameter-scaling-benchmarks","title":"Parameter Scaling Benchmarks","text":""},{"location":"benchmarks/scdataset_benchmarks/#methodology","title":"Methodology","text":"<p>We tested scDataset performance across different combinations of <code>block_size</code> and <code>fetch_factor</code> parameters, which are key to scDataset's performance according to their paper. All tests used:</p> <ul> <li>Single worker (<code>num_workers=0</code>) to isolate parameter effects</li> <li>10-second measurement duration</li> <li>Module-level callback to avoid pickling issues</li> <li>Backed AnnData to match real-world usage</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#results-summary","title":"Results Summary","text":"Parameter Range Best Performance Best Configuration Scaling Factor <code>block_size</code>: [1, 2, 4, 8, 16, 32, 64] 10,976 cells/sec <code>block_size=8, fetch_factor=64</code> 23.1x <code>fetch_factor</code>: [1, 2, 4, 8, 16, 32, 64]"},{"location":"benchmarks/scdataset_benchmarks/#key-findings","title":"Key Findings","text":""},{"location":"benchmarks/scdataset_benchmarks/#1-strong-scaling-with-fetch_factor","title":"1. Strong Scaling with fetch_factor","text":"<ul> <li>Minimal fetch_factor (1): ~500 cells/sec</li> <li>Optimal fetch_factor (64): ~10,000+ cells/sec</li> <li>Scaling factor: 20x+ improvement with higher fetch_factor</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#2-moderate-scaling-with-block_size","title":"2. Moderate Scaling with block_size","text":"<ul> <li>Small block_size (1-4): Good performance with high fetch_factor</li> <li>Medium block_size (8-16): Optimal performance</li> <li>Large block_size (32-64): Slightly reduced performance</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#detailed-results-table","title":"Detailed Results Table","text":"Block Size Fetch Factor Throughput (cells/sec) 1 1 474 1 64 10,093 4 16 5,103 8 64 10,976 16 64 10,484 32 64 9,382 64 64 10,159 <p>Parameter Optimization</p> <p>The optimal configuration for scDataset on our hardware is <code>block_size=8, fetch_factor=64</code>, achieving 10,976 cells/sec.</p>"},{"location":"benchmarks/scdataset_benchmarks/#multiprocessing-benchmarks","title":"Multiprocessing Benchmarks","text":""},{"location":"benchmarks/scdataset_benchmarks/#methodology_1","title":"Methodology","text":"<p>We tested scDataset's multiprocessing capabilities using the optimal parameters (<code>block_size=4, fetch_factor=16</code>) and varying <code>num_workers</code> values.</p>"},{"location":"benchmarks/scdataset_benchmarks/#results","title":"Results","text":"Num Workers Throughput (cells/sec) Status 0 5,363 \u2705 Success 1 N/A \u274c Pickling Error 2 N/A \u274c Pickling Error 4 N/A \u274c Pickling Error 8 N/A \u274c Pickling Error"},{"location":"benchmarks/scdataset_benchmarks/#key-findings_1","title":"Key Findings","text":""},{"location":"benchmarks/scdataset_benchmarks/#1-multiprocessing-limitations","title":"1. Multiprocessing Limitations","text":"<ul> <li>Single worker only: scDataset works reliably with <code>num_workers=0</code></li> <li>Pickling errors: All multiprocessing attempts failed with \"h5py objects cannot be pickled\"</li> <li>Callback issues: Module-level callbacks didn't resolve the pickling problem</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#2-performance-comparison","title":"2. Performance Comparison","text":"<ul> <li>Single worker: 5,363 cells/sec with optimal parameters</li> <li>No multiprocessing scaling: Unable to test due to pickling limitations</li> </ul> <p>Multiprocessing Limitation</p> <p>scDataset's multiprocessing capabilities are limited by pickling issues with h5py-backed AnnData objects. This prevents the scaling benefits reported in their paper.</p>"},{"location":"benchmarks/scdataset_benchmarks/#comparison-with-paper-results","title":"Comparison with Paper Results","text":""},{"location":"benchmarks/scdataset_benchmarks/#reported-vs-observed-performance","title":"Reported vs Observed Performance","text":"Metric Paper Claim Our Results Difference Best throughput ~2,000 cells/sec 10,976 cells/sec 5.5x higher Parameter scaling Significant 23.1x scaling Matches Multiprocessing 12 workers 0 workers only Limited"},{"location":"benchmarks/scdataset_benchmarks/#possible-explanations","title":"Possible Explanations","text":"<ol> <li>Hardware differences: Our M1 Max may be faster than the paper's hardware</li> <li>Dataset differences: Different datasets may have different characteristics</li> <li>Implementation differences: Different scDataset versions or configurations</li> <li>Measurement methodology: Different benchmark setups</li> </ol> <p>Performance Validation</p> <p>Our results show that scDataset can achieve excellent performance with proper parameter tuning, even exceeding the paper's reported numbers on modern hardware.</p>"},{"location":"benchmarks/scdataset_benchmarks/#technical-challenges","title":"Technical Challenges","text":""},{"location":"benchmarks/scdataset_benchmarks/#1-pickling-issues","title":"1. Pickling Issues","text":"<ul> <li>Problem: h5py objects cannot be pickled for multiprocessing</li> <li>Impact: Prevents multiprocessing scaling</li> <li>Workaround: Use single worker with optimized parameters</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#2-parameter-sensitivity","title":"2. Parameter Sensitivity","text":"<ul> <li>Problem: Performance varies dramatically with parameters</li> <li>Impact: Requires careful tuning for optimal performance</li> <li>Solution: Systematic parameter sweeps</li> </ul>"},{"location":"benchmarks/scdataset_benchmarks/#recommendations","title":"Recommendations","text":""},{"location":"benchmarks/scdataset_benchmarks/#for-scdataset-users","title":"For scDataset Users","text":"<ol> <li>Parameter Tuning: Always test different <code>block_size</code> and <code>fetch_factor</code> combinations</li> <li>Single Worker: Use <code>num_workers=0</code> to avoid pickling issues</li> <li>Hardware Testing: Test on your specific hardware for optimal parameters</li> </ol>"},{"location":"benchmarks/scdataset_benchmarks/#for-developers","title":"For Developers","text":"<ol> <li>Pickling Fix: Address h5py pickling issues for multiprocessing support</li> <li>Parameter Documentation: Provide clearer guidance on parameter selection</li> <li>Benchmark Suite: Include comprehensive benchmark tools</li> </ol>"},{"location":"benchmarks/scdataset_benchmarks/#conclusion","title":"Conclusion","text":"<p>scDataset demonstrates excellent performance potential with proper parameter tuning, achieving 10,976 cells/sec in our benchmarks. However, multiprocessing limitations prevent the scaling benefits reported in their paper. The strong parameter scaling validates their design approach, but the pickling issues need to be addressed for broader adoption.</p> <p>The benchmark results show that scDataset can be a viable high-performance dataloader for single-cell data, but requires careful configuration and has limitations for multiprocessing scenarios.</p> <p>These benchmarks were conducted using scDataset with backed AnnData objects on the Tahoe-100M dataset. Results may vary with different datasets, hardware configurations, or scDataset versions.</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/","title":"SLAF vs h5ad Performance Benchmarks","text":"<p>This document provides a comprehensive performance comparison between SLAF and the traditional h5ad (AnnData) format across bioinformatics and machine learning workflows.</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#overview","title":"Overview","text":"<p>SLAF provides dramatic performance improvements over h5ad across all benchmark categories, demonstrating the advantages of modern columnar storage and optimized data access patterns.</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#key-performance-summary","title":"Key Performance Summary","text":"Category SLAF vs h5ad Speedup Memory Efficiency Dataset Cell Filtering 92.3x faster 115.7x less memory synthetic_50k_processed Gene Filtering 17.3x faster 2.2x less memory synthetic_50k_processed Expression Queries 9.5x faster 154.6x less memory synthetic_50k_processed ML Data Loading 55x faster 2.3x less memory Tahoe-100M <p>Performance Leadership</p> <p>SLAF consistently outperforms h5ad by 9.5x-92.3x across all operation types while using 2.2x-154.6x less memory.</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#bioinformatics-benchmarks","title":"Bioinformatics Benchmarks","text":"<p>Input Dataset: synthetic_50k_processed (49,955 cells \u00d7 25,000 genes, 722MB h5ad file)</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#cell-filtering-performance","title":"Cell Filtering Performance","text":"<p>Cell filtering operations are fundamental to single-cell analysis workflows, used for quality control, cell type selection, and data subsetting.</p> Scenario h5ad Total (ms) SLAF Total (ms) Speedup Description S1 530.0 2.9 183.6x Cells with &gt;=500 genes S2 169.7 2.0 83.3x High UMI count (total_counts &gt; 2000) S3 170.7 1.9 92.2x Mitochondrial fraction &lt; 0.1 S4 177.1 2.0 86.7x Complex multi-condition filter S5 186.6 2.8 67.2x Cell type annotation filter S6 171.4 2.0 86.3x Cells from batch_1 S7 207.0 2.3 89.4x Cells in clusters 0,1 from batch_1 S8 170.9 2.1 79.6x High-quality cells (&gt;=1000 genes, &lt;=10% mt) S9 172.1 2.5 70.2x Cells with 800-2000 total counts S10 173.5 2.1 84.6x Cells with 200-1500 genes <p>Average Performance:</p> <ul> <li>SLAF vs h5ad: 92.3x faster</li> <li>Memory Usage: SLAF uses 115.7x less memory than h5ad</li> </ul>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#gene-filtering-performance","title":"Gene Filtering Performance","text":"<p>Gene filtering operations are essential for feature selection, quality control, and dimensionality reduction.</p> Scenario h5ad Total (ms) SLAF Total (ms) Speedup Description S1 43.4 3.0 14.6x Genes expressed in &gt;=10 cells S2 32.3 1.7 19.4x Genes with &gt;=100 total counts S3 32.1 1.8 17.4x Genes with mean expression &gt;=0.1 S4 31.1 1.6 19.9x Exclude mitochondrial genes S5 32.7 1.7 19.7x Highly variable genes S6 31.7 2.1 15.4x Non-highly variable genes S7 31.5 2.0 15.8x Genes in &gt;=50 cells with &gt;=500 total counts S8 31.7 1.9 17.0x Genes with 100-10000 total counts S9 33.2 2.0 16.4x Genes in 5-1000 cells <p>Average Performance:</p> <ul> <li>SLAF vs h5ad: 17.3x faster</li> <li>Memory Usage: SLAF uses 2.2x less memory than h5ad</li> </ul>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#expression-queries-performance","title":"Expression Queries Performance","text":"<p>Expression queries retrieve specific expression data for cells or genes, supporting targeted analysis workflows.</p> Scenario h5ad Total (ms) SLAF Total (ms) Speedup Description S1 484.5 16.1 30.1x Single cell expression S2 251.3 13.9 18.1x Another single cell S3 328.3 14.2 23.1x Two cells S4 233.2 15.5 15.1x Three cells S5 232.7 523.7 0.4x Single gene across all cells S6 203.4 442.6 0.5x Another single gene S7 256.1 303.0 0.8x Two genes S8 212.0 655.9 0.3x Three genes S9 221.4 22.5 9.9x 100x50 submatrix S10 168.3 61.9 2.7x 500x100 submatrix S11 212.2 63.2 3.4x 500x500 submatrix <p>Average Performance:</p> <ul> <li>SLAF vs h5ad: 9.5x faster</li> <li>Memory Usage: SLAF uses 154.6x less memory than h5ad</li> </ul>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#machine-learning-benchmarks","title":"Machine Learning Benchmarks","text":"<p>Input Dataset: Tahoe-100M (5,481,420 cells \u00d7 62,710 genes, ~8B non-zero values)</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#raw-data-loading-performance","title":"Raw Data Loading Performance","text":"<p>Raw data loading measures the base throughput for machine learning workflows without tokenization overhead.</p> System Throughput (cells/sec) Memory Usage (GB) Notes SLAF 24,587 2.1 Optimized streaming h5ad (AnnDataLoader) 422 4.8 Traditional approach h5ad (AnnLoader) 239 5.2 Experimental loader <p>Performance Comparison:</p> <ul> <li>SLAF vs AnnDataLoader: 58.3x faster</li> <li>SLAF vs AnnLoader: 102.9x faster</li> <li>Memory Efficiency: SLAF uses 2.3x less memory</li> </ul>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#gpu-ready-output-performance","title":"GPU-Ready Output Performance","text":"<p>SLAF provides pre-tokenized sequences ready for GPU training, while h5ad-based loaders only provide raw data.</p> System Throughput (cells/sec) Throughput (tokens/sec) Output Type SLAF 7,487 15,332,896 Pre-tokenized sequences h5ad loaders N/A N/A Raw data only <p>GPU Training Advantage</p> <p>SLAF is the only system providing GPU-ready tokenized output, enabling efficient training of foundation models like Geneformer and scGPT.</p>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#technical-implementation-comparison","title":"Technical Implementation Comparison","text":"Aspect SLAF h5ad Storage Arrow-based columnar storage with Lance backend HDF5-based hierarchical storage with h5py backend Metadata Polars DataFrames for efficient filtering operations Pandas DataFrames with traditional filtering Expression Optimized sparse COO matrices with zero-copy access Sparse matrices with h5py backend Memory Minimal intermediate allocations, efficient memory management Full data loading with pandas overhead Access Asynchronous prefetching with background processing Synchronous loading with no streaming optimization"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#choose-slaf-for","title":"Choose SLAF for:","text":"<ul> <li>High-throughput bioinformatics workflows requiring fast filtering and querying</li> <li>Machine learning training on large single-cell datasets</li> <li>Cloud-based analysis requiring scalable, multi-user access</li> <li>Foundation model training requiring GPU-ready tokenized sequences</li> <li>Memory-constrained environments where efficiency is critical</li> </ul>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#consider-h5ad-for","title":"Consider h5ad for:","text":"<ul> <li>Legacy workflows that cannot be easily migrated</li> <li>Small-scale analysis where performance differences are negligible</li> <li>Educational purposes where traditional formats are more familiar</li> <li>Tool compatibility with systems that only support h5ad</li> </ul>"},{"location":"benchmarks/slaf_vs_h5ad_benchmarks/#conclusion","title":"Conclusion","text":"<p>The benchmarks demonstrate that SLAF's modern architecture, optimized data access patterns, and streaming capabilities provide massive advantages for both bioinformatics and machine learning workflows. For users looking to improve performance and scalability, migrating from h5ad to SLAF offers dramatic benefits with minimal workflow changes.</p> <p>For detailed migration guidance, see Migrating to SLAF. For comprehensive benchmark results, see Bioinformatics Benchmarks and ML Benchmarks.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/","title":"SLAF vs TileDB Performance Benchmarks","text":"<p>This document provides a comprehensive performance comparison between SLAF and TileDB SOMA across bioinformatics and machine learning workflows, focusing on modern columnar storage formats.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#overview","title":"Overview","text":"<p>Both SLAF and TileDB represent modern approaches to single-cell data storage, using Arrow-interoperable columnar formats. However, SLAF demonstrates consistent performance advantages across metadata filtering and dataloader throughput, highlighting the benefits of its optimized streaming architecture and efficient data access patterns.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#key-performance-summary","title":"Key Performance Summary","text":"Category SLAF vs TileDB Speedup Dataset Conversion Performance 9.9x faster synthetic_50k_processed Cell Filtering 9.4x faster synthetic_50k_processed Gene Filtering 8.6x faster synthetic_50k_processed Cell-centric Queries 1.1x-1.4x faster synthetic_50k_processed Gene-centric Queries 2.5x-3.3x slower synthetic_50k_processed Submatrix Queries 2.5x-5.0x slower synthetic_50k_processed ML Data Loading 45.7x faster Tahoe-100M <p>Modern Format Advantage</p> <p>Both SLAF and TileDB significantly outperform traditional h5ad-based approaches, demonstrating the benefits of modern cloud-native storage. SLAF provides additional performance optimizations for streaming and data access.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#conversion-performance","title":"Conversion Performance","text":"<p>Conversion from h5ad to modern formats demonstrates SLAF's efficiency in data ingestion workflows.</p> <p>Input Dataset: synthetic_50k_processed (49,955 cells \u00d7 25,000 genes, ~722MB h5ad file)</p> Metric SLAF TileDB SOMA SLAF vs TileDB Improvement Conversion Time 1.87s 18.44s 9.9x faster Output Size 349.2MB 561.1MB 38% smaller Peak Memory 826.6MB 4,399.6MB 5.3x more memory efficient <p>Key Advantages:</p> <ul> <li>9.9x faster conversion from h5ad to SLAF format</li> <li>38% smaller output size compared to TileDB SOMA</li> <li>5.3x more memory efficient - SLAF uses only 827MB vs TileDB's 4.4GB peak memory</li> <li>Optimized chunked processing with efficient memory management</li> </ul>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#tiledb-to-slaf-conversion","title":"TileDB to SLAF Conversion","text":"<p>Converting from TileDB SOMA to SLAF format is straightforward and efficient:</p> <p>TileDB to SLAF Performance: synthetic_50k_processed (49,955 cells \u00d7 25,000 genes)</p> <p>Migration Benefits:</p> <ul> <li>Fast conversion: 50k cell dataset converts in just 2 seconds</li> <li>Linear scaling: Performance scales linearly with the number of cells</li> <li>Simple command: <code>slaf convert data.tiledb output.slaf</code></li> </ul> <p>Easy Migration from TileDB</p> <p>Converting from TileDB to SLAF is simple and fast. A 50k cell dataset takes only 2 seconds to convert, with linear scaling performance. The conversion preserves all data types and metadata while providing significant performance improvements for downstream analysis.</p> <p>Fast Migration Path</p> <p>SLAF's superior conversion performance enables rapid migration of existing h5ad datasets, with conversion times under 2 seconds for 50k cell datasets and significantly smaller output files.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#bioinformatics-benchmarks","title":"Bioinformatics Benchmarks","text":""},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#cell-filtering-performance","title":"Cell Filtering Performance","text":"<p>Cell filtering operations demonstrate the efficiency of modern columnar storage for metadata operations.</p> Scenario TileDB Total (ms) SLAF Total (ms) Speedup Description S1 20.9 2.9 7.3x Cells with &gt;=500 genes S2 21.3 2.0 10.5x High UMI count (total_counts &gt; 2000) S3 21.3 1.9 11.5x Mitochondrial fraction &lt; 0.1 S4 18.6 2.0 9.1x Complex multi-condition filter S5 18.1 2.8 6.5x Cell type annotation filter S6 20.9 2.0 10.5x Cells from batch_1 S7 23.8 2.3 10.3x Cells in clusters 0,1 from batch_1 S8 23.0 2.1 10.7x High-quality cells (&gt;=1000 genes, &lt;=10% mt) S9 19.3 2.5 7.9x Cells with 800-2000 total counts S10 20.8 2.1 10.1x Cells with 200-1500 genes <p>Average Performance: 9.4x faster</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#gene-filtering-performance","title":"Gene Filtering Performance","text":"<p>Gene filtering operations show consistent performance advantages for SLAF's optimized Polars operations.</p> Scenario TileDB Total (ms) SLAF Total (ms) Speedup Description S1 22.3 3.0 7.5x Genes expressed in &gt;=10 cells S2 19.5 1.7 11.7x Genes with &gt;=100 total counts S3 9.3 1.8 5.0x Genes with mean expression &gt;=0.1 S4 15.8 1.6 10.1x Exclude mitochondrial genes S5 16.9 1.7 10.2x Highly variable genes S6 15.8 2.1 7.7x Non-highly variable genes S7 18.2 2.0 9.1x Genes in &gt;=50 cells with &gt;=500 total counts S8 19.8 1.9 10.6x Genes with 100-10000 total counts S9 11.3 2.0 5.6x Genes in 5-1000 cells <p>Average Performance: 8.6x faster</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#expression-queries-performance","title":"Expression Queries Performance","text":"<p>Expression queries demonstrate the efficiency of optimized sparse matrix operations, where TileDB often wins.</p> Scenario TileDB Total (ms) SLAF Total (ms) Speedup Description S1 63.8 16.1 4.0x Single cell expression S2 19.3 13.9 1.4x Another single cell S3 18.1 14.2 1.3x Two cells S4 19.0 15.5 1.2x Three cells S5 150.8 523.7 0.3x Single gene across all cells S6 84.2 442.6 0.2x Another single gene S7 97.7 303.0 0.3x Two genes S8 83.3 655.9 0.1x Three genes S9 9.9 22.5 0.4x 100x50 submatrix S10 12.1 61.9 0.2x 500x100 submatrix S11 10.8 63.2 0.2x 500x500 submatrix <p>Average Performance:</p> <ul> <li>SLAF wins cell-centric (1.2x-4.0x),</li> <li>TileDB wins gene-centric (3.3x-10x) and submatrix (2.5x-5.0x)</li> </ul>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#machine-learning-benchmarks","title":"Machine Learning Benchmarks","text":""},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#raw-data-loading-performance","title":"Raw Data Loading Performance","text":"<p>Raw data loading performance demonstrates the advantages of SLAF's optimized streaming architecture.</p> System Throughput (cells/sec) Notes SLAF 24,587 Optimized streaming TileDB DataLoader 518 Custom PyTorch loader <p>Performance Comparison: 47.5x faster</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#gpu-ready-output-performance","title":"GPU-Ready Output Performance","text":"<p>SLAF provides pre-tokenized sequences ready for GPU training, while TileDB DataLoader only provides raw data.</p> System Throughput (cells/sec) Throughput (tokens/sec) Output Type SLAF 7,487 15,332,896 Pre-tokenized sequences TileDB DataLoader N/A N/A Raw data only <p>GPU Training Advantage</p> <p>SLAF is the only system providing GPU-ready tokenized output, enabling efficient training of foundation models like Geneformer and scGPT.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#technical-implementation-comparison","title":"Technical Implementation Comparison","text":"Aspect SLAF TileDB Storage Arrow-interoperable columnar storage with Lance backend Arrow-interoperable array-native storage with TileDB backend Metadata Polars DataFrames for efficient filtering operations Arrow tables with Polars for filtering operations Expression Optimized sparse COO matrices with zero-copy access Smart indexing for both cell and gene based slicing ML Integration Native PyTorch DataLoader with tokenization support Needs third party custom PyTorch DataLoader for raw data"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#key-technical-advantages","title":"Key Technical Advantages","text":"<p>Optimized Streaming Architecture</p> <p>SLAF's asynchronous prefetching and background processing provide significant performance advantages over TileDB's synchronous loading approach.</p> <p>Enhanced ML Integration</p> <p>SLAF provides native PyTorch DataLoader integration with built-in tokenization support, while TileDB requires custom loader implementation.</p> <p>Memory Efficiency</p> <p>SLAF's optimized memory management and zero-copy operations result in 1.5-2.3x memory efficiency gains over TileDB.</p> <p>Simplified API</p> <p>SLAF provides a more streamlined API for common bioinformatics operations, while TileDB requires more manual configuration.</p>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#migration-benefits","title":"Migration Benefits","text":""},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#performance-improvements","title":"Performance Improvements","text":"<ul> <li>9.4x faster bioinformatics operations</li> <li>47.5x faster machine learning data loading</li> <li>Faster conversion from h5ad to SLAF format, enabling rapid migration of existing datasets</li> <li>Easy TileDB migration: 2-second conversion for 50k cell datasets with linear scaling</li> </ul>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#developer-experience","title":"Developer Experience","text":"<ul> <li>Simplified API with familiar Polars operations</li> <li>Scanpy-native lazy workflows with drop-in replacement of AnnData objects, enabling efficient lazy computation graphs for preprocessing and filtering pipelines</li> <li>Enhanced ML integration with native PyTorch support</li> <li>Built-in tokenization for foundation model training</li> </ul>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#choose-slaf-for","title":"Choose SLAF for:","text":"<ul> <li>High-throughput machine learning workflows requiring fast data loading</li> <li>Foundation model training requiring GPU-ready tokenized sequences</li> <li>Streaming applications where continuous data flow is critical</li> <li>Memory-constrained environments where efficiency is important</li> </ul>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#choose-tiledb-for","title":"Choose TileDB for:","text":"<ul> <li>Existing TileDB infrastructure where migration costs are high</li> <li>Multi-modal data where TileDB's broader ecosystem is beneficial</li> </ul>"},{"location":"benchmarks/slaf_vs_tiledb_benchmarks/#conclusion","title":"Conclusion","text":"<p>SLAF provides strong performance advantages over TileDB across most benchmark categories:</p> <ul> <li>Metadata filtering: 9.4x-8.6x faster (cell and gene filtering)</li> <li>Machine learning data loading: 47.5x faster</li> <li>Expression queries: Mixed performance (SLAF wins cell-centric, TileDB wins gene-centric)</li> </ul> <p>While both systems represent modern approaches to single-cell data storage, SLAF's optimized streaming architecture, enhanced ML integration, and simplified API provide significant advantages for most use cases. The benchmarks demonstrate that SLAF's performance optimizations and developer-friendly design make it the preferred choice for high-throughput bioinformatics and machine learning workflows.</p> <p>For detailed migration guidance including TileDB to SLAF conversion, see Migrating to SLAF. For comprehensive benchmark results, see Bioinformatics Benchmarks and ML Benchmarks.</p>"},{"location":"blog/","title":"SLAF Blog","text":"<p>Welcome to the SLAF blog, where we share insights, technical deep dives, and updates about the Sparse Lazy Array Format for single-cell genomics.</p>"},{"location":"blog/#latest-posts","title":"Latest Posts","text":""},{"location":"blog/#slaf-on-hugging-face-stream-single-cell-data-without-the-download","title":"SLAF on Hugging Face: Stream Single-Cell Data Without the Download","text":"<p>Last updated: January 30, 2026</p> <p>We're releasing three SLAF single-cell transcriptomics datasets (&gt; 100M cells) on the Hugging Face Hub (Tahoe-100M, Parse-10M, X-Atlas-Orion), and with Lance's new <code>hf://</code> support you can stream data directly from Hugging Face for exploratory data analysis, batch processing, and training without full downloads. This post covers what we're releasing, how to use them, and what Lance-on-HuggingFace means for AI/ML beyond single-cell genomics.</p>"},{"location":"blog/#blazing-fast-dataloaders-2-ignatius-takes-a-trip-to-the-library-of-congress","title":"Blazing Fast Dataloaders #2: Ignatius takes a trip to the Library of Congress","text":"<p>Last updated: September 8, 2025</p> <p>Remember Ignatius J Reilly from A Confederacy of Dunces? Voracious, impatient, impressionable, perambulatorily challenged? That's modern neural network pretraining on GPUs. In this post, we explore how SLAF's mixture of scanners approach achieves near-perfect randomization (88-90% of theoretical maximum) while maintaining 97% of sequential throughput performance. We dive deep into the \"Library of Congress\" metaphor to explain how our contraption delivers randomized books at high throughput without reorganizing the library.</p>"},{"location":"blog/#64x-faster-dataloaders-deconstructing-pytorch-for-single-cell-genomics","title":"6.4x Faster DataLoaders: Deconstructing PyTorch for Single-Cell Genomics","text":"<p>Last updated: August 22, 2025</p> <p>Single-cell transcriptomics datasets have reached escape velocity, with modern experiments yielding counts for upwards of 5M cells \u00d7 20k genes. This technical deep dive explores how we achieved 6.4x performance improvement over standard PyTorch DataLoaders, reaching 28,207 cells/second through five key innovations: contiguous reads, single-threaded prefetching, vectorized window functions, block shuffling, and vectorized tokenization.</p>"},{"location":"blog/#introducing-slaf-the-single-cell-data-format-for-the-virtual-cell-era","title":"Introducing SLAF: The Single-Cell Data Format for the Virtual Cell Era","text":"<p>Last updated: August 14, 2025</p> <p>Single-cell datasets have grown from 50k to 100M cells in less than a decade, creating a fundamental mismatch between our tools and our needs. This introduction to SLAF (Sparse Lazy Array Format) explores how we're solving 2025 problems with modern technology, combining the best ideas from Zarr, Dask, Lance, and Polars into a cloud-native, SQL-powered format designed for the modern single-cell era.</p>"},{"location":"blog/#about-slaf","title":"About SLAF","text":"<p>SLAF (Sparse Lazy Array Format) is a cloud-native single-cell storage format for the virtual cell era:</p> <ul> <li>Zarr-inspired: Zero-copy, query-in-place access to cloud storage</li> <li>Dask-inspired: Lazy computation graphs that optimize before execution</li> <li>Lance + Polars-inspired: OLAP-powered SQL with pushdown optimization</li> <li>Scanpy-compatible: Drop-in replacement for existing workflows</li> </ul>"},{"location":"blog/#get-started","title":"Get Started","text":"<p>Ready to try SLAF? Check out our quickstart guide or explore the API documentation. Find it on Github. Deep dive into benchmarks.</p> <p>Have questions or want to contribute? We'd love to hear from you!</p>"},{"location":"blog/benchmark-overhaul-plan/","title":"SLAF Benchmark Overhaul Plan","text":""},{"location":"blog/benchmark-overhaul-plan/#overview","title":"Overview","text":"<p>This document outlines our plan to completely overhaul the SLAF benchmarks to demonstrate our competitive advantages against state-of-the-art dataloaders like scDataset, BioNeMo SCDL, and AnnDataLoader.</p>"},{"location":"blog/benchmark-overhaul-plan/#key-competitive-advantages","title":"Key Competitive Advantages","text":"<p>SLAF demonstrates significant advantages over state-of-the-art dataloaders:</p> <ol> <li>4.2x Higher Throughput: 16,789 vs 4,000 cells/sec</li> <li>100x Lower Memory: 2.5 GB vs 256 GB RAM</li> <li>Single Process: No multiprocessing overhead</li> <li>GPU-Ready Output: Pre-tokenized sequences vs raw data</li> <li>Inferior Hardware: M1 Max laptop vs enterprise server</li> </ol>"},{"location":"blog/benchmark-overhaul-plan/#benchmark-structure","title":"Benchmark Structure","text":""},{"location":"blog/benchmark-overhaul-plan/#class-1-internal-slaf-performance-analysis","title":"Class 1: Internal SLAF Performance Analysis","text":"<p>Goal: Show SLAF's performance across different tokenization strategies using Tahoe100M dataset.</p> <p>Scope:</p> <ul> <li>Single batch size (32 cells) - most common in literature</li> <li>4 tokenization strategies:</li> <li>scGPT with binning</li> <li>scGPT without binning</li> <li>Geneformer with percentile filtering</li> <li>Geneformer without percentile filtering</li> <li>No multi-process scaling or batch size scaling</li> <li>Focus on throughput, memory, and tokenization quality</li> </ul> <p>Key Metrics:</p> <ul> <li>Throughput (cells/sec, tokens/sec)</li> <li>Memory efficiency (RAM usage)</li> <li>Tokenization strategy comparison</li> <li>Batch processing efficiency</li> </ul> <p>Expected Results:</p> <ul> <li>Peak throughput: ~16,789 cells/sec (Geneformer xlarge batch)</li> <li>Token generation: Up to 34M tokens/sec for large batches</li> <li>Memory efficiency: Linear scaling with batch size (~1.2MB per 512 cells)</li> <li>Strategy advantage: Geneformer shows ~10% higher throughput than scGPT</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#class-2-external-dataloader-comparisons","title":"Class 2: External Dataloader Comparisons","text":"<p>Goal: Compare SLAF against state-of-the-art dataloaders using standardized benchmark setup.</p> <p>Two-Tier Comparison:</p>"},{"location":"blog/benchmark-overhaul-plan/#tier-1-raw-data-loading-cellssec","title":"Tier 1: Raw Data Loading (cells/sec)","text":"<p>Compare raw data loading performance across systems:</p> <ul> <li>SLAF: Raw cell \u00d7 gene data (new raw mode)</li> <li>scDataset: Raw cell \u00d7 gene data</li> <li>AnnDataLoader: Raw cell \u00d7 gene data</li> <li>BioNeMo SCDL: Raw cell \u00d7 gene data</li> </ul> <p>Metrics:</p> <ul> <li>Throughput (cells/sec)</li> <li>Memory usage (GB)</li> <li>Output format consistency</li> <li>Resource requirements</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#tier-2-gpu-ready-output-comparison","title":"Tier 2: GPU-Ready Output Comparison","text":"<p>Compare end-to-end pipeline performance:</p> <ul> <li>SLAF: Raw data \u2192 GPU-ready tensors (tokenized sequences)</li> <li>Others: Raw data only (requires manual tokenization)</li> </ul> <p>Metrics:</p> <ul> <li>End-to-end throughput</li> <li>Computational output quality</li> <li>Training integration readiness</li> <li>Manual processing requirements</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#implementation-plan","title":"Implementation Plan","text":""},{"location":"blog/benchmark-overhaul-plan/#phase-1-add-raw-mode-to-slafdataloader","title":"Phase 1: Add Raw Mode to SLAFDataLoader","text":"<p>Goal: Implement raw data mode for fair comparisons.</p> <p>Changes Required:</p> <ol> <li>Add <code>raw_mode: bool = False</code> parameter to <code>SLAFDataLoader.__init__()</code></li> <li>Implement <code>_raw_iterator()</code> method that bypasses tokenization</li> <li>Return raw cell \u00d7 gene data in consistent format</li> <li>Maintain existing API compatibility</li> </ol> <p>Raw Mode Output Format:</p> <p>We will match AnnDataLoader's sparse tensor format for consistency:</p> <pre><code>{\n    \"X\": torch.sparse_csr_tensor(\n        crow_indices=tensor([0, 4717, 5733, ...]),  # Row pointers\n        col_indices=tensor([12, 16, 19, ...]),      # Column indices\n        values=tensor([2., 2., 3., ...]),           # Expression values\n        size=(batch_size, n_genes),                  # Matrix dimensions\n        nnz=46260,                                   # Non-zero elements\n        layout=torch.sparse_csr\n    )\n}\n</code></pre> <p>Reference Output Formats:</p> <p>AnnDataLoader Output:</p> <pre><code>&gt;&gt;&gt; batches[0]\n{'x': tensor(crow_indices=tensor([    0,  4717,  5733,  6680,  9053, 11346, 12990,\n                            14045, 14753, 16498, 17114, 18070, 18930, 19753,\n                            20704, 22037, 23718, 25057, 27270, 28387, 30116,\n                            33086, 34236, 35661, 36678, 37567, 38388, 40268,\n                            41664, 43124, 43781, 44738, 46260]),\n       col_indices=tensor([   12,    16,    19,  ..., 62502, 62530, 62611]),\n       values=tensor([2., 2., 3.,  ..., 1., 1., 1.]), size=(32, 62710),\n       nnz=46260, layout=torch.sparse_csr)}\n</code></pre> <p>scDataset Output:</p> <pre><code>&gt;&gt;&gt; sc_batches[0]\nAnnCollectionView object with n_obs \u00d7 n_vars = 32 \u00d7 62710\n    obs: 'sample', 'gene_count', 'tscp_count', 'mread_count', 'drugname_drugconc', 'drug', 'cell_line', 'sublibrary', 'BARCODE', 'pcnt_mito', 'S_score', 'G2M_score', 'phase', 'pass_filter', 'cell_name', 'plate', '_scvi_cell_line'\n&gt;&gt;&gt; sc_batches[0].X\n&lt;Compressed Sparse Row sparse matrix of dtype 'float32'\n    with 43852 stored elements and shape (32, 62710)&gt;\n</code></pre>"},{"location":"blog/benchmark-overhaul-plan/#phase-2-create-benchmark-scripts","title":"Phase 2: Create Benchmark Scripts","text":""},{"location":"blog/benchmark-overhaul-plan/#benchmark_dataloaders_internalpy","title":"benchmark_dataloaders_internal.py","text":"<ul> <li>Focus on 4 tokenization strategies</li> <li>Single batch size (32 cells)</li> <li>30-second measurement windows</li> <li>1-second warm-up</li> <li>Memory tracking</li> <li>Rich output formatting</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#benchmark_dataloaders_externalpy","title":"benchmark_dataloaders_external.py","text":"<ul> <li>Tier 1: Raw data loading comparisons</li> <li>Tier 2: GPU-ready output comparisons</li> <li>Integration with scDataset and AnnDataLoader</li> <li>Standardized metrics and reporting</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#phase-3-update-documentation","title":"Phase 3: Update Documentation","text":""},{"location":"blog/benchmark-overhaul-plan/#docsbenchmarksml_benchmarksmd","title":"docs/benchmarks/ml_benchmarks.md","text":"<ul> <li>Complete overhaul with new results</li> <li>Clear competitive positioning</li> <li>Technical architecture advantages</li> <li>Real-world training impact analysis</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#blog-post-content","title":"Blog Post Content","text":"<ul> <li>Competitive analysis against scDataset paper</li> <li>Technical deep dive into SLAF advantages</li> <li>Performance benchmarks and insights</li> <li>Future roadmap and implications</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#technical-implementation-details","title":"Technical Implementation Details","text":""},{"location":"blog/benchmark-overhaul-plan/#raw-mode-implementation","title":"Raw Mode Implementation","text":"<p>We'll add a <code>raw_mode</code> parameter to <code>SLAFDataLoader</code> that bypasses tokenization and returns raw cell \u00d7 gene data in AnnDataLoader-compatible format:</p> <pre><code>class SLAFDataLoader:\n    def __init__(self, ..., raw_mode: bool = False):\n        self.raw_mode = raw_mode\n        # ... rest of init\n\n    def __iter__(self):\n        if self.raw_mode:\n            yield from self._raw_iterator()\n        else:\n            yield from self._tokenized_iterator()\n\n    def _raw_iterator(self):\n        \"\"\"Yield raw cell \u00d7 gene data without tokenization\"\"\"\n        # Reuse existing Lance loading infrastructure\n        # Bypass window functions and tokenization\n        # Return raw expression data in AnnDataLoader format\n</code></pre> <p>Raw Mode Implementation Details:</p> <ol> <li> <p>Reuse Existing Infrastructure:</p> </li> <li> <p>Use existing <code>SLAFIterableDataset</code> and <code>PrefetchBatchProcessor</code></p> </li> <li>Bypass tokenization step in <code>PrefetchBatchProcessor.load_prefetch_batch()</code></li> <li> <p>Keep cell shuffling and batching logic</p> </li> <li> <p>Output Format (AnnDataLoader Compatible):</p> </li> </ol> <pre><code>{\n    \"X\": torch.sparse_csr_tensor(\n        crow_indices=tensor([0, 4717, 5733, ...]),  # Row pointers\n        col_indices=tensor([12, 16, 19, ...]),      # Column indices\n        values=tensor([2., 2., 3., ...]),           # Expression values\n        size=(batch_size, n_genes),                  # Matrix dimensions\n        nnz=46260,                                   # Non-zero elements\n        layout=torch.sparse_csr\n    )\n}\n</code></pre> <ol> <li> <p>Implementation Steps:</p> </li> <li> <p>Add <code>raw_mode</code> parameter to <code>SLAFDataLoader.__init__()</code></p> </li> <li>Modify <code>PrefetchBatchProcessor.load_prefetch_batch()</code> to skip tokenization when <code>raw_mode=True</code></li> <li>Convert Lance expression data directly to sparse CSR tensor</li> <li>Handle partial cells and cell boundaries (already done in existing code)</li> <li> <p>Return batch in AnnDataLoader format</p> </li> <li> <p>Key Changes: <pre><code># In PrefetchBatchProcessor.load_prefetch_batch()\nif self.raw_mode:\n    # Skip window functions and tokenization\n    # Convert expression data directly to sparse CSR\n    return self._create_raw_batch(complete_df)\nelse:\n    # Existing tokenization logic\n    return self._create_tokenized_batch(grouped)\n</code></pre></p> </li> </ol>"},{"location":"blog/benchmark-overhaul-plan/#benchmark-infrastructure","title":"Benchmark Infrastructure","text":"<p>Common Benchmark Class:</p> <pre><code>class BenchmarkResult:\n    scenario_name: str\n    cells_per_sec: float\n    memory_mb: float\n    measurement_time: float\n    total_cells: int\n    total_batches: int\n    output_type: str  # \"raw\" or \"tokenized\"\n</code></pre> <p>Standardized Measurement:</p> <ul> <li>30-second measurement windows</li> <li>1-second warm-up period</li> <li>Memory tracking with psutil</li> <li>Rich console output</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#external-dataloader-integration","title":"External Dataloader Integration","text":"<p>scDataset Integration:</p> <pre><code>from scdataset import scDataset\nfrom anndata.experimental import AnnCollection\n\ncollection = AnnCollection([adata])\nsc_dataset = scDataset(data_collection=collection, batch_size=32)\ndataloader = DataLoader(sc_dataset, batch_size=None)\n</code></pre> <p>AnnDataLoader Integration:</p> <pre><code>from scvi.dataloaders import AnnDataLoader\nfrom scvi.data import AnnDataManager\nfrom scvi.data.fields import LayerField\n\nanndata_fields = [LayerField(registry_key=\"X\", layer=None, is_count_data=True)]\nadata_manager = AnnDataManager(fields=anndata_fields)\nadata_manager.register_fields(adata)\n\ndataloader = AnnDataLoader(\n    adata_manager,\n    batch_size=32,\n    shuffle=True,\n    drop_last=False,\n    load_sparse_tensor=True\n)\n</code></pre>"},{"location":"blog/benchmark-overhaul-plan/#expected-outcomes","title":"Expected Outcomes","text":""},{"location":"blog/benchmark-overhaul-plan/#competitive-positioning","title":"Competitive Positioning","text":"<p>Raw Data Loading (Tier 1):</p> <ul> <li>SLAF: ~8,000 cells/sec (estimated)</li> <li>scDataset: 4,000 cells/sec</li> <li>AnnDataLoader: ~500 cells/sec</li> <li>Result: 2x faster than scDataset, 16x faster than AnnDataLoader</li> </ul> <p>GPU-Ready Output (Tier 2):</p> <ul> <li>SLAF: 16,789 cells/sec (tokenized sequences)</li> <li>Others: Raw data only (requires manual processing)</li> <li>Result: 4.2x faster + GPU-ready output</li> </ul>"},{"location":"blog/benchmark-overhaul-plan/#key-messages","title":"Key Messages","text":"<ol> <li>Performance: SLAF achieves higher throughput with fewer resources</li> <li>Efficiency: Single process vs multiprocessing, lower memory usage</li> <li>Functionality: GPU-ready output vs raw data requiring manual processing</li> <li>Hardware: Inferior hardware achieving superior results</li> <li>Architecture: Streaming design enables massive dataset handling</li> </ol>"},{"location":"blog/benchmark-overhaul-plan/#blog-post-structure","title":"Blog Post Structure","text":"<ol> <li>Introduction: Data loading bottleneck in single-cell foundation models</li> <li>Competitive Analysis: scDataset paper review and positioning</li> <li>SLAF Architecture: Technical deep dive into advantages</li> <li>Benchmark Results: Comprehensive performance analysis</li> <li>Real-World Impact: Training implications and future directions</li> <li>Conclusion: SLAF as the leading solution for high-performance single-cell data loading</li> </ol>"},{"location":"blog/benchmark-overhaul-plan/#timeline","title":"Timeline","text":"<ol> <li>Week 1: Implement raw mode in SLAFDataLoader</li> <li>Week 2: Create benchmark scripts and run internal benchmarks</li> <li>Week 3: Integrate external dataloaders and run external comparisons</li> <li>Week 4: Update documentation and prepare blog post content</li> </ol>"},{"location":"blog/benchmark-overhaul-plan/#success-criteria","title":"Success Criteria","text":"<ol> <li>Technical: Raw mode implementation working correctly</li> <li>Performance: SLAF shows clear advantages in both tiers</li> <li>Documentation: Clear, compelling benchmark results</li> <li>Blog Post: Engaging narrative that positions SLAF competitively</li> <li>Reproducibility: All benchmarks can be run independently</li> </ol>"},{"location":"blog/benchmark-overhaul-plan/#risk-mitigation","title":"Risk Mitigation","text":"<ol> <li>External Dependencies: Test scDataset and AnnDataLoader integration early</li> <li>Performance Variability: Use consistent measurement methodology</li> <li>Hardware Differences: Document hardware specifications clearly</li> <li>API Changes: Ensure backward compatibility in SLAFDataLoader changes</li> </ol> <p>This plan provides a comprehensive roadmap for overhauling the SLAF benchmarks to clearly demonstrate our competitive advantages and position SLAF as the leading solution for high-performance single-cell data loading.</p>"},{"location":"blog/blazing-fast-dataloaders-2/","title":"Blazing Fast Dataloaders #2: Ignatius takes a trip to the Library of Congress","text":"<p>Remember Ignatius J Reilly from A Confederacy of Dunces? Voracious, impatient, impressionable, perambulatorily challenged? That's modern neural network pretraining on GPUs.</p> <p>Ignatius's traits map perfectly to GPU training challenges:</p> <ul> <li>Voracious: Insatiable appetite for data at massive scale</li> <li>Impatient: Can't tolerate delays\u2014any idle time is expensive compute waste</li> <li>Impressionable: Easily biased by whatever patterns it encounters first</li> <li>Perambulatorily challenged: Can't move to the data; everything must be streamed directly to it</li> </ul> <p>What if, instead of wandering the streets of New Orleans, Ignatius spent his days in the Library of Congress? And what if you were the only librarian who could tend to his endless demands? That's the challenge of dataloader design.</p> <p>In our previous post on SLAF dataloaders, we tackled voraciousness (scale), impatience (throughput), and perambulation (streaming). Today, let's examine impressionability\u2014and why it's the trickiest challenge of all.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-library-and-its-readers","title":"The Library and Its Readers","text":"<p>Picture the Library of Congress: a vast, modern institution housing millions of books (your data) meticulously organized across multiple floors (partitions). Every book is catalogued alphabetically by topic, author, and title (your indexing schema), creating a systematic way to locate any piece of information quickly.</p> <p>You are the sole librarian (query engine) responsible for serving three distinctly different types of readers (query patterns), each with their own peculiar demands:</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-three-types-of-readers","title":"The Three Types of Readers","text":"<p>1. The Impatient Wanderer This reader bursts through the doors with a specific agenda: they need exactly 5 books on diverse topics\u2014medieval poetry, quantum mechanics, cookbook recipes, legal precedents, and art history\u2014and they need them now. They want to read just a few pages from each before moving on to their next random assortment.</p> <p>In Technical Terms</p> <p>Interactive applications, real-time dashboards, chat interfaces. These require filtering tiny subsets of data with random access patterns, demanding ultra-low latency responses.</p> <p>2. The Diligent Specialist This reader has a laser focus: they want to know the publication year of every single book in the library\u2014nothing more, nothing less. They're methodical, patient, and willing to wait for comprehensive results. They don't mind if it takes hours, as long as they get complete coverage.</p> <p>In Technical Terms</p> <p>Batch processing pipelines, analytics workloads, aggregation queries. These scan entire datasets with column-specific operations, optimizing for throughput over latency.</p> <p>3. The Impressionable Voracious Reader (Ignatius) This is our GPU: they want to read every book in the library, cover to cover, as fast as possible. But here's the critical constraint: they're highly impressionable. If they encounter all the poetry books first, they might develop an unconscious bias toward flowery language. If they read all the legal texts together, they might start thinking everything is a contract dispute. They need books delivered in a carefully randomized order to avoid these biases, but they also can't wait around: they need a steady stream of books arriving at their reading desk.</p> <p>In Technical Terms</p> <p>Neural network training workloads that require high-throughput streaming of randomized data to prevent model bias while maintaining GPU utilization.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#whats-within-the-librarians-control","title":"What's Within the Librarian's Control","text":"<p>As the resourceful librarian, you have several tools at your disposal:</p> <ul> <li>Query optimization: You can choose the most efficient path through the library</li> <li>Caching strategies: You can anticipate requests and pre-position books</li> <li>Resource allocation: You can decide how to distribute your time and energy</li> <li>Delivery methods: You can design clever systems to transport books to readers</li> </ul>"},{"location":"blog/blazing-fast-dataloaders-2/#whats-beyond-the-librarians-control","title":"What's Beyond the Librarian's Control","text":"<p>However, you face significant constraints that you cannot change:</p> <ul> <li>The library architecture: Floors, rooms, and basic layout are fixed</li> <li>The cataloging system: Books are organized alphabetically by topic/author/title\u2014you can't reorganize the entire collection</li> <li>Storage format: Books are stored as books, not reorganized into reader-specific formats</li> <li>Physical infrastructure: You can't rebuild the building or add new wings</li> </ul> <p>Technical Constraints</p> <p>You can't change the underlying data schema, create additional indexes (too expensive), pre-shuffle the entire dataset (violates \"store once, query many\" principle), or reorganize partitions for specific workloads.</p> <p>The challenge becomes clear: how do you serve Ignatius's need for randomized, high-throughput data delivery while working within these constraints? You need to be resourceful, not revolutionary.</p> <p>This is where the librarian's ingenuity shines: building a clever contraption within the existing constraints to solve the impossible problem.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-librarians-contraption-assistant-librarians-chutes-and-tumble-drums","title":"The Librarian's Contraption: Assistant Librarians, Chutes, and Tumble Drums","text":"<p>Faced with Ignatius's impossible demands, our resourceful librarian devises an ingenious solution: a mechanical contraption that delivers randomized books at high throughput without reorganizing the library.</p> <p>The system works through three coordinated components that operate continuously in the background:</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-assistant-librarians-scanners","title":"The Assistant Librarians (Scanners)","text":"<p>Rather than running around the library herself, our librarian employs assistant librarians, one stationed on each floor. But here's the clever part: at each moment in time, only a randomly selected subset of assistants are active.</p> <p>Imagine 5 floors in our library, each with an assistant. At each iteration, the librarian randomly selects 2 assistants to collect books. This random selection is the \"mixture\" in \"mixture of scanners\": different assistants contribute to the book stream at different times, ensuring no systematic bias toward any particular floor's content.</p> <pre><code>graph TD\n    subgraph \"Library Floors (Partitions)\"\n        F1[Floor 1: Arts &amp; Literature]\n        F2[Floor 2: Sciences]\n        F3[Floor 3: History]\n        F4[Floor 4: Law &amp; Politics]\n        F5[Floor 5: Philosophy]\n    end\n\n    subgraph \"Assistant Librarians\"\n        A1[Assistant 1]\n        A2[Assistant 2]\n        A3[Assistant 3]\n        A4[Assistant 4]\n        A5[Assistant 5]\n    end\n\n    F1 --- A1\n    F2 --- A2\n    F3 --- A3\n    F4 --- A4\n    F5 --- A5\n\n    style A2 fill:#ff6b6b\n    style A4 fill:#ff6b6b\n\n    note1[Iteration 1: Assistants 2 &amp; 4 selected]</code></pre> <p>Each active assistant collects books in the most efficient way possible: contiguous sequences from their floor. Rather than randomly jumping around their floor (which would be slow), they grab books from consecutive shelves. This is crucial for performance.</p> <p>The Lance Advantage</p> <p>Here's where our choice of storage format matters enormously. Unlike traditional formats (like Parquet) that organize data into fixed \"row groups,\" Lance has no such constraints. This means our assistants can read truly contiguous sequences of any size from their partition, maximizing I/O efficiency. With Parquet, assistants would be forced to read in predetermined chunk sizes, often fetching unwanted data or making multiple I/O calls.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-chute-system-queue","title":"The Chute System (Queue)","text":"<p>As assistants collect their books, they don't deliver them directly to Ignatius. Instead, they drop books down a pneumatic chute system that connects all floors to a central processing area. This chute (the async queue) serves two critical functions:</p> <ol> <li>Buffering: Books from different floors arrive at different times, but the chute stores them temporarily</li> <li>Mixing: Books from Floor 2 (Sciences) and Floor 4 (Law) arrive in the same chute, beginning the randomization process</li> </ol> <pre><code>flowchart TD\n    subgraph \"Iteration Timeline\"\n        direction LR\n        I1[Iteration 1&lt;br/&gt;Select 2&amp;4] --&gt; I2[Iteration 2&lt;br/&gt;Select 1&amp;5] --&gt; I3[Iteration 3&lt;br/&gt;Select 2&amp;3] --&gt; I4[Iteration 4&lt;br/&gt;Select 1&amp;4]\n    end\n\n    subgraph \"Floor to Chute Flow\"\n        F1[Floor 1] -.-&gt; C[Central Chute]\n        F2[Floor 2] --&gt; C\n        F3[Floor 3] -.-&gt; C\n        F4[Floor 4] --&gt; C\n        F5[Floor 5] -.-&gt; C\n    end\n\n    style F2 fill:#ff6b6b\n    style F4 fill:#ff6b6b\n    style C fill:#4ecdc4</code></pre>"},{"location":"blog/blazing-fast-dataloaders-2/#the-tumble-drum-shuffling-mechanism","title":"The Tumble Drum (Shuffling Mechanism)","text":"<p>Here's where the magic happens. Before books reach Ignatius, they pass through a tumble drum, like a clothes dryer that thoroughly mixes the books from different floors and different time periods. This mechanical shuffling ensures that:</p> <ul> <li>A science book from Iteration 1 might be delivered alongside a literature book from Iteration 3</li> <li>Books that were stored next to each other on the same floor are now separated</li> <li>Ignatius receives a truly randomized stream despite the systematic collection process</li> </ul> <pre><code>flowchart LR\n    A1[Assistant 1] -.-&gt; Q[\"Queue&lt;br/&gt;(Chute)\"]\n    A2[Assistant 2] --&gt; Q\n    A3[Assistant 3] -.-&gt; Q\n    A4[Assistant 4] --&gt; Q\n    A5[Assistant 5] -.-&gt; Q\n\n    Q --&gt; T[\"Tumble Drum&lt;br/&gt;(Shuffler)\"]\n    T --&gt; CB[Conveyor Belt]\n    CB --&gt; I[\"Ignatius&lt;br/&gt;(GPU)\"]\n\n    style A2 fill:#ff6b6b\n    style A4 fill:#ff6b6b\n    style T fill:#f39c12\n    style I fill:#27ae60</code></pre> <p>The librarian's contraption runs continuously:</p> <ol> <li>Every few seconds: Randomly select which assistants should collect books</li> <li>Assistants work in parallel: Each reads a contiguous sequence from their floor</li> <li>Books flow into the chute: Mixed timing creates natural diversity</li> <li>Tumble drum activates: Mechanical shuffling adds final randomization</li> <li>Steady delivery: Ignatius receives randomized books at a consistent, high rate</li> </ol> <p>The key insight is that randomization happens in the delivery mechanism, not in the storage organization. The library maintains its efficient alphabetical organization, but the contraption ensures Ignatius gets the randomized experience he needs to avoid bias.</p> <p>This system is configurable: in practice, you might have 50 floors (partitions) with 5\u201310 assistants active per iteration, but the principle remains the on-the-fly randomness through query-engine design rather than storage reorganization.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#ignatius-gets-his-books-real-world-results","title":"Ignatius Gets His Books: Real-World Results","text":"<p>Now comes the crucial question: does this elaborate contraption actually deliver on its promise? Does Ignatius get the randomized reading experience he needs while maintaining the throughput he demands?</p> <p>To answer this, we need to measure two things: randomness quality (how unbiased is the book delivery?) and delivery speed (does the contraption keep up with Ignatius's voracious appetite?).</p>"},{"location":"blog/blazing-fast-dataloaders-2/#measuring-randomness-the-library-distance-test","title":"Measuring Randomness: The Library Distance Test","text":"<p>To measure how random our book delivery truly is, we developed a clever test that doesn't require knowing anything about the books' contents. Instead, we look at the \"distance\" between book locations in the library.</p> <p>Think of it this way: if our contraption is working properly, books that end up in the same delivery batch should come from vastly different parts of the library. A batch containing books from shelf 1, shelf 50,000, shelf 120,000, and shelf 300,000 indicates good randomization. A batch with books from shelves 1, 2, 3, and 4 suggests our system isn't mixing well enough.</p> <p>We measured this using L1 distance: essentially, how far apart the book locations are numerically. We tested this across 10,000 delivery batches (32 books each) from our 5.4 million book library.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-three-strategies-lazy-fragmented-and-resourceful","title":"The Three Strategies: Lazy, Fragmented, and Resourceful","text":"<p>We compared our mixture of scanners contraption against two simpler approaches:</p> <p>The Lazy Sequential Librarian</p> <p>Our baseline: a librarian who simply walks through the library in order, floor by floor, shelf by shelf. This represents the most efficient but least random approach\u2014books in each batch come from consecutive shelves.</p> <p>The Floor Stepper</p> <p>A slightly cleverer approach that reads larger contiguous chunks\u2014entire floors at a time rather than just a few shelves. The tumble drum can then achieve better block-wise randomization (good mixing within each floor's books, but floors are still processed sequentially).</p> <p>The Resourceful Contraption (Mixture of Scanners)</p> <p>Our full system with assistant librarians, chutes, and tumble drums working together to create maximum randomization while maintaining efficiency.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#the-results-near-perfect-randomization","title":"The Results: Near-Perfect Randomization","text":"<p>Here's how each approach performed in our library distance test:</p> <p>Randomness Measurement (L1 Distance Between Book Locations)</p> Strategy Within-Batch Distance Across-Batch Distance Sequential (Lazy) 94.1 104.5 Floor Stepping 1,643.5 1,672.6 Mixture of Scanners (MoS) 1,608,648.2 1,642,829.9 Perfect Random (Theoretical) 1,828,595.2 1,824,468.9 <p>Normalized Entropy Scores (0 = Sequential, 1 = Perfect Random)</p> Strategy Within-Batch Score Across-Batch Score Sequential (Lazy) 0.000 0.000 Floor Stepping 0.001 0.001 Mixture of Scanners (MoS) 0.880 0.900 <p>Near-Perfect Randomization</p> <p>Our contraption achieves 88-90% of theoretical maximum randomness\u2014nearly indistinguishable from perfect random selection! This means books in Ignatius's batches come from wildly different parts of the library, preventing any systematic bias.</p> <p>The beautiful part is that this randomization comes with minimal throughput cost:</p> <p>Delivery Throughput Results</p> Strategy Throughput (books/sec) Performance vs Sequential Sequential (Lazy) 23,728 100% (baseline) Floor Stepping 26,769 113% (slightly faster) Mixture of Scanners (MoS) 22,972 97% (nearly identical) <p>Speed Without Sacrificing Throughput</p> <p>Our contraption delivers books at 97% the speed of the simple sequential approach while achieving 88-90% of perfect randomization.</p>"},{"location":"blog/blazing-fast-dataloaders-2/#why-the-contraption-works-so-well","title":"Why the Contraption Works So Well","text":"<p>Why the Contraption Works So Well</p> <p>The magic lies in the multi-layered randomization working together:</p> <ol> <li>Scanner Selection Randomness: Randomly choosing which assistant librarians are active ensures books come from different floors</li> <li>Starting Point Variety: Each active assistant begins from a different point on their floor</li> <li>Temporal Mixing: The chute system means books collected in different rounds get mixed together</li> <li>Tumble Drum Shuffling: Final mechanical mixing ensures even books collected consecutively are delivered in random order</li> </ol> <p>The result: Ignatius gets books that are scattered across the entire library\u2014a medieval poetry book might be followed by quantum mechanics, then a cookbook, then legal precedents\u2014exactly the unpredictable sequence he needs to avoid developing reading biases.</p> <p>Ready to Use</p> <p>The mixture of scanners approach is available to use and is the default <code>SLAFDataLoader</code> option as of <code>v0.3.0</code>.</p> <p>First, install SLAF with ML support:</p> <pre><code>pip install \"slafdb[ml]&gt;=0.3.0\"\n</code></pre> <p>Then use the mixture of scanners approach:</p> <pre><code>from slaf.core.slaf import SLAFArray\nfrom slaf.ml.dataloaders import SLAFDataLoader\n\n# Load your single-cell dataset\nslaf_array = SLAFArray(\"path/to/dataset.slaf\")\n\n# Create dataloader with mixture of scanners (maximum entropy)\ndataloader = SLAFDataLoader(\n    slaf_array=slaf_array,\n    batch_size=32,\n    n_epochs=1000,\n    raw_mode=True,\n    n_scanners=16,                 # Number of assistant librarians\n    prefetch_batch_size=4194304,   # How many book pages they fetch at once: 4M rows for MoS\n)\n\n# Ignatius gets his randomized books\nfor batch in dataloader:\n    train_step(model, batch)\n</code></pre> <p>For a more comprehensive performance comparison against other state-of-the-art dataloaders, benchmarks are available at: https://slaf-project.github.io/slaf/benchmarks/ml_benchmarks/</p>"},{"location":"blog/blazing-fast-dataloaders/","title":"6.4x Faster DataLoaders: Deconstructing PyTorch for Single-Cell Genomics","text":"<p>Single-cell transcriptomics datasets have reached escape velocity. With falling assay and sequencing costs, a modern experiment can yield counts for upwards of 5M cells \u00d7 20k genes, representing a 100-fold increase in scale from just 5 years ago.</p> <p>As we've seen in other domains, this is also the precise moment where Jevons paradox (the principle that demand will rise to meet surplus in supply) and Sutton's bitter lesson (the principle that large enough data and large enough models can outperform alternatives derived from decades of domain-specific creativity) kick in.</p> <p>Cue: Atlas-scale datasets (Tahoe-100M, Human Cell Atlas, scBaseCamp), transformer-based foundation models (scGPT, Transcriptformer, STATE), and the virtual cell era.</p> <p>To keep pace with this deluge, I developed Sparse Lazy Array Format (SLAF). SLAF is a ground-up rethink of storage and compute for single cell data.</p> <ul> <li>SLAF is a columnar, cloud-native, vector-first storage container (on top of an open standard like Lance).</li> <li>SLAF provides lazy compute interfaces that resemble familiar scanpy idioms, and state-of-the art data loaders for training transformer-based foundation models.</li> </ul> <p>Learn about SLAF here and start using it for your single-cell workflows today.</p> <p>In this blog post, we'll go behind the scenes of SLAF's data loader, and the various optimizations that improve throughput ~6.4x with respect to a naive PyTorch implementation on SLAF, and ~100x with respect to status quo approaches for single-cell data loading. Look for comprehensive dataloader benchmarks here.</p>"},{"location":"blog/blazing-fast-dataloaders/#streaming-transcriptomics-data-to-modern-gpus","title":"Streaming Transcriptomics Data to Modern GPUs","text":"<p>Training foundation models on single-cell RNA-seq data requires streaming massive datasets to GPU clusters. Let's do some napkin math:</p> <p>Performance Requirements</p> <ul> <li>Dataset: 100M cells, ~60B non-zero expression values</li> <li>Model: 1.2B parameter transformer (scGPT)</li> <li>Batch size: 32 cells</li> <li>Training loop throughput: 20 batches/second on H100 GPU</li> <li>Per-GPU requirement: 640 cells/second</li> <li>8 \u00d7 H100 node requirement: ~5-6K cells/second</li> </ul> <p>SLAF Constraints</p> <p>SLAF serves three user personas:</p> <p>(1) dashboard builders for no-code users</p> <p>(2) computational biologists building scanpy pipelines</p> <p>(3) ML engineers training foundation models</p> <p>To support the first two personas, SLAF relies on performance-optimized dataframes (polars) and OLAP query engines (duckdb), both of which benefit from pre-sorted cell and gene IDs in Lance tables.</p> <p>Given the mission of \"store once, query-in-place\", pre-randomization and pre-tokenization weren't realistic options.</p> <p>The challenge: Stream 5-6K cells/second from cloud-native Lance storage, doing on-the-fly randomization and tokenization, providing GPU-ready cell sentences.</p>"},{"location":"blog/blazing-fast-dataloaders/#the-standard-approach-and-its-limitations","title":"The Standard Approach and Its Limitations","text":"<p>Let's start with a traditional PyTorch approach:</p> <pre><code>class SingleCellDataset(IterableDataset):\n    def __iter__(self):\n        # Load one cell at a time\n        for cell_id in range(self.num_cells):\n            # Load cell data from storage\n            cell_data = self.load_cell(cell_integer_id)\n            # Apply window functions for gene ranking\n            ranked_genes = self.rank_genes(cell_data)\n            # Tokenize the sequence\n            tokens = self.tokenizer.tokenize(ranked_genes)\n            yield tokens\n\n# Use standard DataLoader with multiprocessing\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=8,      # Multiprocessing for parallelism\n    prefetch_factor=4   # Asynchronously prefetch subsequent batches\n)\n</code></pre> <pre><code>def load_cell(self, cell_id):\n    \"\"\"Load cell data using SQL query to expression lance table\"\"\"\n    sql_query = f\"\"\"\n    SELECT cell_integer_id, gene_integer_id, value\n    FROM expression\n    WHERE cell_integer_id = {cell_id}\n    \"\"\"\n    return self.expression_dataset.sql(sql_query).to_pandas()\n\ndef rank_genes(self, cell_data):\n    \"\"\"Rank genes using pandas-based windowing\"\"\"\n    cell_data['gene_rank'] = cell_data.groupby('cell_integer_id')['value'].rank(\n        method='dense', ascending=False\n    )\n    return cell_data[cell_data['gene_rank'] &lt;= self.max_genes]\n</code></pre> <p>Result: 4,408 cells/second</p> <p>This naive approach doesn't scale because it ignores several critical aspects unique to our data format:</p> <p>Key Limitations</p> <ol> <li>Storage format mismatch: Lance tables store expression counts as <code>(cell_integer_id, gene_integer_id, value)</code> per record, but models need one cell per data point</li> <li>Inefficient I/O: Sample-by-sample loading ignores Lance's optimized <code>to_batches()</code> generator for contiguous reads</li> <li>Wrong shuffling granularity: Standard shuffling breaks cell data integrity</li> <li>Missing vectorization: No batch processing for ranking and tokenization</li> <li>Multiprocessing overhead: PyTorch's approach conflicts with Lance's multi-threaded I/O</li> </ol> <p>As we'll see, fully embracing these deviations meant writing our own async prefetcher and vectorized transforms\u2014which is why state-of-the-art solutions like Mosaic ML's mosaicml-streaming and Ray Data didn't work out of the box for SLAF.</p>"},{"location":"blog/blazing-fast-dataloaders/#the-journey-from-naive-to-optimized","title":"The Journey: From Naive to Optimized","text":""},{"location":"blog/blazing-fast-dataloaders/#phase-1-sql-window-functions","title":"Phase 1: SQL Window Functions","text":"<p>We started with the pushdown promise of OLAP databases, leveraging SQL window functions to combine cell loading and gene ranking by pushing compute down to Lance tables.</p> <pre><code>WITH ranked_genes AS (\n    SELECT\n        cell_integer_id,\n        gene_integer_id,\n        value,\n        ROW_NUMBER() OVER (\n            PARTITION BY cell_integer_id\n            ORDER BY value DESC, gene_integer_id ASC\n            LIMIT {max_genes}\n        ) as gene_rank\n    FROM expression\n    WHERE cell_integer_id BETWEEN {cell_start} AND {cell_start + batch_size}\n)\n</code></pre> <p>Result: 4,582 cells/second</p> <p>Problem: Range scans are expensive for massive tables without indexes. For datasets of this scale (~60B rows), indexes add 200% storage overhead, so we want to do as much as possible without indexes.</p>"},{"location":"blog/blazing-fast-dataloaders/#phase-2-switch-from-range-scans-to-random-reads","title":"Phase 2: Switch from Range Scans to Random Reads","text":"<p>After recognizing the range scan bottleneck, I realized we could exploit lancedb's 100x faster-than-parquet random access capability. I switched to PyArrow's <code>take()</code> method using an inverted index for cell start and end positions.</p> <pre><code>def load_cells_efficient(self, cell_ids):\n    \"\"\"Load cells using Lance take() for efficient row access\"\"\"\n    # Convert cell IDs to integer IDs\n    integer_ids = self._normalize_entity_ids(cell_ids, \"cell\")\n\n    # Get row indices using RowIndexMapper\n    row_indices = self.row_mapper.get_cell_row_ranges(integer_ids)\n\n    # Load data with Lance take() - much faster than SQL queries\n    expression_data = self.expression.take(row_indices)\n\n    # Convert to Polars DataFrame\n    return pl.from_arrow(expression_data)\n</code></pre> <p>Result: 7,086 cells/second</p> <p>Problem: Pandas-based windowing operations become the bottleneck.</p> <p>We could stop here because we've reached our original criteria of streaming 5-6k cells / second, but let's keep going!</p>"},{"location":"blog/blazing-fast-dataloaders/#phase-3-switching-window-functions-from-pandas-to-polars","title":"Phase 3: Switching Window Functions from Pandas to Polars","text":"<p>With Rust-based acceleration of data frame operations, and drop-in replacement to Pandas, Polars was the natural choice for window functions.</p> <pre><code>result = filtered_df.with_columns([\n    pl.col(\"value\").rank(method=\"dense\", descending=True)\n    .over(\"cell_integer_id\")\n    .alias(\"gene_rank\")\n])\n</code></pre> <p>Result: 16,620 cells/second (much better!)</p>"},{"location":"blog/blazing-fast-dataloaders/#understanding-the-scaling-laws","title":"Understanding the Scaling Laws","text":"<p>Before going further, I needed to understand why larger batches were performing better. I began measuring how different operations scaled with batch size, and discovered a series of scaling laws that would guide the final architecture. These laws describe how the per-cell cost of operations changes as batch size increases: crucial information for designing an optimal prefetching strategy.</p> <p>The Scaling Law Discovery</p> <p>As I optimized the pipeline, I began to notice patterns in how different operations scaled with batch size. This led me to measure the actual scaling characteristics of our key operations. What I found was a series of sub-linear scaling laws that would guide the final architecture:</p> <p>Window Function Scaling (Gene Ranking)</p> Batch Size (cells) Runtime (\u03bcs) Per-cell Cost (\u03bcs) 32 1,983 62.0 64 2,237 35.0 128 3,888 30.4 256 6,173 24.1 512 10,533 20.6 1024 19,523 19.1 <pre><code>xychart\n    title \"Window Function Scaling: Per-cell Cost vs Batch Size\"\n    x-axis \"Batch Size (cells)\" [32, 64, 128, 256, 512, 1024]\n    y-axis \"Per-cell Cost (\u03bcs)\" 0 --&gt; 70\n    line [62.0, 35.0, 30.4, 24.1, 20.6, 19.1]</code></pre> <p>Lance Data Loading Scaling</p> Batches per Chunk Runtime (\u03bcs) Per-row Cost (\u03bcs) Rows Processed 10 58,587 715.2 81,920 25 97,178 474.5 204,800 50 50,104 122.3 409,600 100 43,335 52.9 819,200 200 51,451 31.4 1,638,400 <pre><code>xychart\n    title \"Lance Data Loading Scaling: Per-row Cost vs Chunk Size\"\n    x-axis \"Batches per Chunk\" [10, 25, 50, 100, 200]\n    y-axis \"Per-row Cost (\u03bcs)\" 0 --&gt; 800\n    line [715.2, 474.5, 122.3, 52.9, 31.4]</code></pre> <p>Block Shuffling Scaling</p> Unique Cells Runtime (\u03bcs) Per-cell Cost (\u03bcs) 100 950 9.5 500 3,087 6.2 1000 11,573 11.6 2000 13,044 6.5 5000 29,567 5.9 <pre><code>xychart\n    title \"Block Shuffling Scaling: Per-cell Cost vs Number of Cells\"\n    x-axis \"Unique Cells\" [100, 500, 1000, 2000, 5000]\n    y-axis \"Per-cell Cost (\u03bcs)\" 0 --&gt; 15\n    line [9.5, 6.2, 11.6, 6.5, 5.9]</code></pre> <p>These scaling laws revealed critical insights:</p> <ul> <li>Window functions: 0.31x scaling factor (larger batches are 3x more efficient)</li> <li>Lance loading: 0.04x scaling factor (larger chunks are 25x more efficient)</li> <li>Block shuffling: 0.62x scaling factor (more cells are 1.6x more efficient)</li> </ul> <p>The solution became clear: async prefetching and preprocessing at a different batch size than the training loop, leveraging these sub-linear scaling characteristics.</p>"},{"location":"blog/blazing-fast-dataloaders/#phase-4-the-deconstructed-dataloader","title":"Phase 4: The deconstructed dataloader","text":"<p>Armed with this insight, I looked around for libraries that provide the flexibility to decouple async prefetching batch size from training batch size in the dataloader ecosystem without having the write my own async prefetcher and queue management systems. I looked into Mosaic ML's streaming library and Ray Data.</p> <ul> <li> <p><code>mosaic-ml-streaming</code> was dead on arrival because it expects users to convert to Mosaic's custom format or other standards like jsonl. This violates our self-imposed constraint of \"store once, query in place\" and wouldn't be a drop-in replacement for PyTorch over SLAF.</p> </li> <li> <p>Ray Data actually supports most of our requirements - it has direct Lance integration with <code>read_lance()</code>, supports Arrow tables and custom transformations, and can handle different batch sizes for prefetching vs training. However, Ray's <code>transform_data</code> method is opinionated about input types, accepting pandas dataframes and Arrow Tables but not polars dataframes. Ultimately, we chose to build a custom solution to avoid the conversion overhead between <code>pyarrow.RecordBatch</code> \u2192 <code>pyarrow.Table</code> \u2192 Polars \u2192 <code>ray.data.Dataset</code>, and to have tighter control over the specific optimizations like block shuffling and window functions in polars. Once Ray's <code>transform_data</code> supported polars dataframes directly, we could switch.</p> </li> </ul> <p>Given this state, I decided to bite the bullet and write my own thread-based async prefetcher. The hardest part was unlearning PyTorch's received wisdom about the roles of the Dataset and DataLoader classes.</p> Component Traditional PyTorch Our Deconstructed Approach Dataset \"I just return one sample at a time\" \"I'm a streaming interface that pre-fetches and pre-processes many training batches at once\" DataLoader \"I handle batching, shuffling, and parallelization\" \"I'm a simple iterator with async prefetching\" Tokenizer \"I process sequences on-demand in the training loop\" \"I batch-process sequences in background threads outside the training loop\" <p>Once I reframed it this way, the rest was straightforward.</p> <p>1. Contiguous Reads with Two-Level Hierarchy</p> <p>Pre-fetching large batches enables switching from random reads (<code>take()</code>) to contiguous reads (<code>to_batches()</code>), then block-shuffling cells within the prefetch batch. Random reads are always slower than contiguous reads. The upward pressure on prefetch batch size comes from minimizing disk reads, while downward pressure comes from I/O scaling laws and vectorized operation scaling laws.</p> <pre><code>class PrefetchBatchProcessor:\n    def __init__(self, slaf_array, window, shuffle, tokenizer):\n        # Create Lance dataset and batch generator for contiguous reads\n        self.expression_dataset = lance.dataset(f\"{slaf_array.slaf_path}/expression.lance\")\n        self.batch_generator = self.expression_dataset.to_batches()  # Contiguous reads\n\n    def load_prefetch_batch(self):\n        # Load ~1.5M records (multiple Lance batches)\n        batch_dfs = []\n        for _ in range(self.batches_per_chunk):  # 50 batches\n            batch = next(self.batch_generator)  # Contiguous read from Lance\n            batch_dfs.append(pl.from_arrow(batch))\n\n        # Apply window functions and shuffling at prefetch level\n        combined_df = pl.concat(batch_dfs)\n        ranked_df = self.apply_window_functions(combined_df)\n        shuffled_chunks = self.apply_block_shuffling(ranked_df)\n\n        return shuffled_chunks  # Multiple training batches\n</code></pre> <p>Key benefits: I/O efficiency through contiguous reads, vectorized processing with sub-linear scaling, and memory efficiency through chunked processing.</p> <p>2. Single-Threaded Async Prefetching</p> <p>Our choice of single-threaded prefetching aligns with a broader trend in the PyTorch community away from multiprocessing toward threading for I/O-bound workloads. Recent work by NVIDIA has shown that threading can significantly outperform multiprocessing for GPU-accelerated workloads. Modern frameworks like Ray Train have embraced threading-based prefetching.</p> <pre><code>class AsyncPrefetcher:\n    def __init__(self, batch_processor, max_queue_size=500):\n        self.queue = Queue(maxsize=max_queue_size)\n        self.worker_thread = threading.Thread(target=self._prefetch_worker)\n\n    def _prefetch_worker(self):\n        while not self.should_stop:\n            # Load prefetch batch (CPU-bound operations)\n            batch = self.batch_processor.load_prefetch_batch()\n            # Put in queue for training loop\n            self.queue.put(batch)\n</code></pre> <p>Key benefits: No serialization overhead, better cache locality, simpler coordination, and optimal Lance I/O utilization.</p> <p>3. Vectorized Window Functions in Polars</p> <p>The breakthrough came from Polars' vectorized window functions. As batch size increases, the per-cell cost of window functions decreases, making larger prefetch batches more efficient.</p> <pre><code># Vectorized gene ranking across all cells in a fragment\ngrouped = (\n    fragment_df.with_columns([\n        pl.col(\"value\")\n        .rank(method=\"dense\", descending=True)\n        .over(\"cell_integer_id\")\n        .alias(\"gene_rank\")\n    ])\n    .filter(pl.col(\"gene_rank\") &lt;= max_genes)\n    .group_by(\"cell_integer_id\")\n    .agg([\n        pl.col(\"gene_integer_id\").alias(\"gene_sequence\"),\n    ])\n)\n</code></pre> <p>Key benefits: Columnar processing on memory-contiguous chunks, Rust acceleration for near-native performance, sub-linear scaling with batch size, and memory efficiency without intermediate copies.</p> <p>4. Block Shuffling for Structured Data</p> <p>Traditional shuffling operates at the sample level, but our data format requires block shuffling to keep all records of a cell together:</p> <pre><code>def efficient_block_shuffle(df, batch_size, seed):\n    \"\"\"Shuffle at the cell level, then create batches\"\"\"\n    # Partition by cell_integer_id (fast since data is pre-sorted)\n    chunks = df.partition_by(\"cell_integer_id\", as_dict=False)\n\n    # Shuffle the list of chunks\n    random.seed(seed)\n    random.shuffle(chunks)\n\n    # Create batches of shuffled cells\n    return [\n        pl.concat(chunks[i : i + batch_size])\n        for i in range(0, len(chunks), batch_size)\n    ]\n</code></pre> <p>Key benefits: Memory efficiency by shuffling cell IDs instead of entire DataFrames, cache-friendly data locality preservation, and vectorized operations leveraging Polars' optimized filter operations.</p> <p>5. Vectorized Tokenization</p> <p>Batch tokenization provides significant speedups through vectorized operations and efficient memory management:</p> <pre><code>def batch_tokenize(self, gene_sequences, expr_sequences=None, max_genes=2048):\n    \"\"\"Vectorized tokenization across entire prefetch batch\"\"\"\n    batch_size = len(gene_sequences)\n\n    # Pre-allocate numpy array for speed\n    token_array = np.full((batch_size, max_genes), self.special_tokens[\"PAD\"], dtype=np.int64)\n\n    # Vectorized processing for each sequence\n    for i, gene_sequence in enumerate(gene_sequences):\n        # Convert gene IDs to tokens (fast mapping)\n        gene_tokens = np.array(gene_sequence, dtype=np.int64) + 4\n\n        # Build sequence: [CLS] genes [SEP]\n        if len(gene_tokens) &gt; 0:\n            tokens = np.concatenate([\n                [self.special_tokens[\"CLS\"]],\n                gene_tokens,\n                [self.special_tokens[\"SEP\"]],\n            ])\n        else:\n            tokens = np.array([self.special_tokens[\"CLS\"], self.special_tokens[\"SEP\"]], dtype=np.int64)\n\n        # Pad/truncate to max_genes\n        tokens = tokens[:max_genes]\n        if len(tokens) &lt; max_genes:\n            padding = np.full(max_genes - len(tokens), self.special_tokens[\"PAD\"], dtype=np.int64)\n            tokens = np.concatenate([tokens, padding])\n\n        # Fill array\n        token_array[i, :] = tokens\n\n    # Convert to tensors in one operation\n    input_ids = torch.from_numpy(token_array)\n    attention_mask = input_ids != self.special_tokens[\"PAD\"]\n\n    return input_ids, attention_mask\n</code></pre> <p>Key benefits: Pre-allocation to avoid repeated tensor creation, vectorized vocabulary lookup for entire batches, and memory layout optimization with contiguous tensor storage.</p> <p>The Complete Solution</p> <p>By combining these five key innovations, we achieved 6.4x performance improvement over the standard PyTorch approach:</p> <ol> <li>Contiguous Reads: Switch from random to contiguous I/O with two-level hierarchy</li> <li>Single-Threaded Prefetching: Eliminate multiprocessing overhead with async threading</li> <li>Vectorized Window Functions: Leverage Polars' Rust-accelerated columnar processing</li> <li>Block Shuffling: Preserve data locality while enabling randomization</li> <li>Vectorized Tokenization: Batch process sequences with numpy optimization</li> </ol> <p>The result: 28,207 cells/second, enabling GPU-saturating throughput for single-cell foundation model training.</p>"},{"location":"blog/blazing-fast-dataloaders/#the-optimized-pipeline","title":"The Optimized Pipeline","text":"<pre><code>%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'Comic Sans MS, cursive', 'fontSize': '14px', 'primaryColor': '#4ecdc4', 'primaryTextColor': '#2c3e50', 'primaryBorderColor': '#27ae60', 'lineColor': '#3498db', 'secondaryColor': '#f39c12', 'tertiaryColor': '#e74c3c', 'noteBkgColor': '#f8f9fa', 'noteBorderColor': '#dee2e6', 'messageFontStyle': 'italic', 'messageFontSize': '12px'}}}%%\nsequenceDiagram\n    participant S as \"\ud83c\udfd7\ufe0f Lance Storage\"\n    participant P as \"\u26a1 Prefetcher\"\n    participant T as \"\ud83d\udd04 Transformer\"\n    participant Q as \"\ud83d\udce6 Queue\"\n    participant D as \"\ud83c\udfaf DataLoader\"\n    participant G as \"\ud83d\ude80 GPU\"\n\n    Note over S,G: Optimized Data Pipeline Flow\n\n    S-&gt;&gt;P: \ud83d\udcca Arrow Batches&lt;br/&gt;(8192 records)\n    P-&gt;&gt;P: \ud83d\udd04 Aggregate 100 batches \u2192 1 chunk\n    P-&gt;&gt;T: \ud83d\udce6 Chunk (~819k records)\n    T-&gt;&gt;T: \ud83c\udfb2 Shuffle cells\n    T-&gt;&gt;T: \ud83d\udcc8 Window functions (gene ranking)\n    T-&gt;&gt;T: \ud83d\udd24 Tokenize sequences\n    T-&gt;&gt;Q: \ud83c\udfaf Training batches (32 cells)\n    Q-&gt;&gt;D: \u26a1 Pre-tokenized tensors\n    D-&gt;&gt;G: \ud83d\ude80 GPU-ready data\n\n    Note over G: Ready for training! \ud83c\udf89</code></pre>"},{"location":"blog/blazing-fast-dataloaders/#performance-comparison","title":"Performance Comparison","text":"Approach Performance Bottleneck Standard PyTorch 4,408.4 cells/sec Loading and Preprocessing SQL Window Functions 4,582.4 cells/sec Expensive range scans Direct Random Access 7,086.3 cells/sec Pandas window functions Polars Window Functions 16,619.7 cells/sec Small batch size Deconstructed DataLoader 28,207.1 cells/sec None"},{"location":"blog/blazing-fast-dataloaders/#results-and-future-directions","title":"Results and Future Directions","text":"<p>Our deconstructed approach achieved 6.4x performance improvement over standard PyTorch DataLoaders, reaching 28,207 cells/second. This enables GPU-saturating throughput for single-cell foundation model training.</p> <p>The key insight is that the biggest performance gains come from rethinking what each component should be responsible for rather than optimizing within existing boundaries. By challenging PyTorch's traditional role assumptions and embracing domain-specific optimizations for genomics data, we achieved order-of-magnitude improvements.</p> <p>As Python moves toward removing the GIL and hardware continues to evolve, thread-based approaches will become even more attractive. The future of high-performance data loading lies not in adding more parallelism, but in smarter orchestration of the entire data pipeline.</p> <p>What's next?</p> <ul> <li> <p>We're actively collaborating with teams developing dataloaders for this corner of biology: the scanpy team on AnnLoader, the SCVI team on AnnDataLoader, the Lamin team on arrayloaders, and the scDataset team, to expand the scope of fair dataloader benchmarks. Please reach out if you're developing one and would like to participate!</p> </li> <li> <p>We're working on integration with Ray Train in order to make multi-node, multi-GPU training seamless and performant. If you're a developer or experienced user of Ray, or if you're a GPU cloud provider that wants to sponsor foundation model training run benchmarks, we want to hear from you!</p> </li> </ul> <p>The complete implementation of <code>SLAFDataLoader</code> is available on Github, along with example notebooks and comprehensive API documentation. Go forth and train!</p>"},{"location":"blog/huggingface/","title":"SLAF on Hugging Face","text":"<p>We're excited to share that SLAF datasets are now freely available and streamable on Hugging Face\u00a0\ud83c\udf89</p> <ol> <li> <p>Three open single-cell transcriptomics datasets in SLAF format are live at huggingface.co/slaf-project.</p> </li> <li> <p>These datasets are streamable directly from the Huggingface Hub to your compute for exploratory analysis, batch processing, or training without bulk downloads.</p> </li> </ol> <p>If you work with 10M+ cell datasets, you've hit the ceiling of \"download the h5ad and load it in memory.\" SLAF on Hugging Face is the alternative: same workflows, streamed from the Hub. Beyond single-cell, the lance-format org is already hosting Lance across text, image, video, and robotics today.</p>"},{"location":"blog/huggingface/#first-the-datasets","title":"First, the datasets","text":"<p>We're releasing three popular datasets spanning different biology and scale in SLAF format:</p> Source Scale Repo Description Tahoe Therapeutics 100M cells Tahoe-100M Chemical perturbation screen on 50 cancer cell lines Parse Biosciences 10M cells Parse-10M Cytokine-stimulated PBMCs across multiple donors Xaira Therapeutics 6M cells X-Atlas-Orion Genome-wide deeply sequenced CRISPR screen on two cell lines <p>Point your code at a <code>hf://datasets/slaf-project/&lt;repo&gt;</code> URI and stream.</p> <p>Licenses</p> <p>Note that Tahoe is the most permissive license, whereas Parse and Xaira are free to use or reshare with attribution for non-profit use. Specific licenses and links to original versions are available in the repos.</p> <p>Heed the rate limits</p> <p>Lance can read directly from Hugging Face's backend for free, subject to rate limits. We've currently hosted these datasets on a Pro Plan but can upgrade to better rate limits or help you host your favorite datasets on your private repo depending on usage.</p>"},{"location":"blog/huggingface/#the-status-quo-and-who-were-building-for","title":"The Status Quo and Who We're Building For","text":"<p>Before we get into how streaming works, it helps to see what we're moving away from and who we're building for.</p> <p>The problem. The status quo for large single-cell (and many other) datasets is: copy from a public bucket to your own, then pull into local storage, load into memory with format-specific readers, and hit the usual limits: egress cost, network throughput, I/O concurrency underutilization due to the Python GIL, RAM ceilings, and libraries that perform single-threaded ops. Every researcher, app, and workload often ends up with their own copy and pipeline.</p> <p></p> <p>Figure 1: Status quo workload in depth.</p> <p>Who we're building for. SLAF is designed for multiple personas: computational biologists who want to explore data without bulk downloads, data engineers who need to distribute at scale without egress blow-up, AI/ML researchers training large models efficiently, product engineers building interactive visualization on big data without self-hosting, and agent engineers who need subagents to read from a commodity sandbox. Those are exactly the personas that benefit from \"point at <code>hf://</code>, stream, don't download.\"</p> <p></p> <p>Figure 2: Personas SLAF needed to support.</p>"},{"location":"blog/huggingface/#lance-hugging-face-stream-to-your-compute","title":"Lance + Hugging Face: Stream to Your Compute","text":"<p>Hugging Face has invested heavily in streaming at scale: fewer startup requests, faster resolution, and an fsspec-compatible <code>hf://</code> filesystem so that any fsspec-capable library can read from the Hub without a full download. Lance's <code>hf://</code> support plugs into that: a Lance dataset can live on the Hugging Face Hub and be opened like this:</p> <pre><code>import lance\nds = lance.dataset(\"hf://datasets/slaf-project/Parse-10M/data/train/cells.lance\")\nfor batch in ds.to_batches(batch_size=256):\n    # process batch (PyArrow RecordBatch)\n    ...\n</code></pre> <p>A URI and an iterator without <code>huggingface-cli download</code> makes it possible to run exploratory analysis, batch jobs, and training jobs directly from Hugging Face, subject to plan-specific rate limits.</p>"},{"location":"blog/huggingface/#three-ways-to-use-the-data","title":"Three Ways to Use the Data","text":""},{"location":"blog/huggingface/#1-exploratory-analysis-with-sql","title":"1. Exploratory analysis with SQL","text":"<p>Use SLAF's SQL layer to filter and aggregate over the dataset without loading it into memory. Example (from the Hugging Face README):</p> <pre><code>from slaf import SLAFArray\n\nslaf_array = SLAFArray(\"hf://datasets/slaf-project/Parse-10M\")\nresults = slaf_array.query(\"\"\"\n    SELECT\n        cytokine,\n        cell_type,\n        AVG(gene_count) as avg_gene_count\n    FROM cells\n    WHERE donor = 'Donor10'\n      AND cytokine IN ('C5a', 'CD40L')\n    GROUP BY cytokine, cell_type\n    ORDER BY cytokine, avg_gene_count DESC\n\"\"\")\n</code></pre> <p>Queries are executed with predicate and projection pushdown against the Lance tables on the Hub; only the columns and rows needed for the result are streamed.</p>"},{"location":"blog/huggingface/#2-batch-processing-streaming-scans","title":"2. Batch processing (streaming scans)","text":"<p>For full-table or large scans (e.g. normalization, aggregation, export), you can stream batches from Hugging Face and process them in a pipeline. We benchmarked streaming from three sources:</p> <ul> <li>the Hugging Face Hub (dataset repo),</li> <li>an S3 bucket in the same availability zone as the worker,</li> <li>and a colocated Modal volume (data local to the worker).</li> </ul> <p>For the Hugging Face source we compared the Datasets library's native streaming (<code>load_dataset(..., streaming=True)</code>) with Lance over <code>hf://</code> (same repo, same table). The S3 and Modal volume runs use Lance only, as reference points for same-region object storage and colocated NFS-like store.</p> <p>We streamed ~5.12M records from the Tahoe-100M cells table (batch size 256 \u00d7 20,000 batches) to a Modal worker:</p> <pre><code>Throughput (MiB/s)\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nModal volume (lance)   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 189.72\nTigris S3 (lance)      \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 62.06\nHF repo (lance)        \u2588\u2588\u2588\u2588\u2588\u2588\u2588 44.84\nHF repo (datasets)     \u2588\u2588\u2588\u2588\u2588 29.53\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                       0    50   100   150   200\n</code></pre> <p>Takeaways</p> <p>Lance over <code>hf://</code> is ~1.5\u00d7 faster than the Datasets library's native streaming on the same HF repo (44.84 vs 29.53 MiB/s).</p> <p>Note that the Hugging Face team has invested in several optimizations for Parquet streaming (e.g. persistent file-list cache, prefetching, configurable buffering), which have made <code>load_dataset(..., streaming=True)</code> much more efficient for Parquet-backed datasets. The same level of tuning is not yet in place for Lance in the Datasets library. Small improvements there could likely bring Datasets streaming to parity with Lance over <code>hf://</code> for Lance-backed repos. Meanwhile, for batch processing or training jobs that stream from the Hub, Lance's <code>hf://</code> support gives you a meaningful throughput gain over Datasets streaming without leaving the Hugging Face ecosystem.</p> <p>Same-region S3 (Tigris) with Lance is faster still (62.06 MiB/s), and a colocated Modal volume gives the highest throughput (189.72 MiB/s), as expected when data is local to the worker.</p>"},{"location":"blog/huggingface/#3-dataloading-for-training-slafdataloader","title":"3. Dataloading for training (SLAFDataLoader)","text":"<p>For foundation-model-style training on tokenized cell sentences, use <code>SLAFDataLoader</code> with an <code>hf://</code> SLAF dataset so batches are streamed from Hugging Face, randomized and tokenized on the fly:</p> <pre><code>from slaf import SLAFArray\nfrom slaf.ml.dataloaders import SLAFDataLoader\n\nslaf_array = SLAFArray(\"hf://datasets/slaf-project/Parse-10M\")\ndataloader = SLAFDataLoader(\n    slaf_array=slaf_array,\n    tokenizer_type=\"geneformer\",\n    batch_size=32,\n    max_genes=2048,\n    vocab_size=50000,\n    prefetch_batch_size=1_000_000\n)\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    # Your training code here\n</code></pre> <p>Same API as with a local or S3 path; only the URI changes. The dataloader's prefetcher and tokenization run on the stream coming from the Hub.</p>"},{"location":"blog/huggingface/#one-store-many-consumers-the-slaf-vision","title":"One Store, Many Consumers: The SLAF Vision","text":"<p>The three use cases above: exploratory data analysis, batch processing, and training, all point at the same <code>hf://</code> datasets and stream only what they need. That's the SLAF vision operationlized: one public store (Lance in a bucket or on the Hub) feeding many consumers: researcher notebooks (Polars, LazyAnnData), training pipelines (Polars, SLAFDataLoader, PyTorch), visualization apps (Polars, LanceDB), batch pipelines, and agent sandboxes. That architecture eliminates repeated egress and duplicate copies, and enables true concurrency, multithreading, commodity RAM/CPU, and independent scaling of compute. Hugging Face is one of the ways we get that \"public bucket\" in front of everyone: same datasets, same <code>hf://</code> path, streamed on demand.</p> <p></p> <p>Figure 3: The SLAF vision: one store, many consumers.</p> <p>Putting SLAF on the Hub brings that vision closer: streaming from a central, well-known store without bulk downloads, so more of these workloads can run against the same data the same way.</p>"},{"location":"blog/huggingface/#how-this-happened-the-backstory","title":"How This Happened: The Backstory","text":"<p>Lance-on-HuggingFace didn't happen in a vacuum. It sits on top of Hugging Face's investment in streaming at scale and their fsspec-compatible <code>hf://</code> filesystem, starting with a concrete use case: making Lance (and SLAF) readable from the Hub without downloading hundreds of GB.</p>"},{"location":"blog/huggingface/#the-hugging-face-side-streaming-and-the-hf-filesystem","title":"The Hugging Face side: streaming and the <code>hf://</code> filesystem","text":"<p>Hugging Face has been on a tear with streaming datasets. In their October 2025 post Streaming datasets: 100\u00d7 More Efficient, they detailed the problem: at scale, every DataLoader worker used to initialize the dataset on its own, hammering the Hub with 100,000+ requests per minute and getting IPs blocked. Their solution: a persistent file-list cache (so only the first worker hits the Hub, the rest share) plus smarter resolution logic, cutting startup requests by as much as 100\u00d7, making resolution 10\u00d7 faster, and doubling streaming throughput. Now Hugging Face trains nanoVLM by streaming from the Hub, getting performance that rivals reading from local SSDs \ud83e\udd2f</p> <p>For custom formats (like Lance), they didn't leave you with only <code>load_dataset(..., streaming=True)</code>. They improved the HfFileSystem in <code>huggingface_hub</code>: an fsspec-compatible interface so that any library that speaks fsspec can read from the Hub using the <code>hf://</code> URI scheme. From the docs:</p> <pre><code>from huggingface_hub import HfFileSystem\n\npath = f\"hf://datasets/{dataset_id}/{path_in_repo}\"\nwith HfFileSystem().open(path) as f:\n    # stream with .read() / .readline(), or random access with .seek()\n</code></pre> <p>The documentation spells it out: \"The HfFileSystem can be used with any library that integrates fsspec, provided the URL follows the scheme.\" That's how Pandas, Dask, Polars, DuckDB, and Zarr already work with the Hub, and how Lance now plugs in.</p>"},{"location":"blog/huggingface/#how-lance-and-opendal-fit-in","title":"How Lance and OpenDAL fit in","text":"<p>Our side started with a GitHub issue asking whether the <code>datasets</code> library could stream Lance and other modern table formats stored on the Hugging Face Hub so that SLAF (and others) could serve large tabular/scientific datasets in Lance format without full-dataset downloads.</p> <p>That led to collaboration across several projects:</p> <ul> <li>Hugging Face's work on streaming and the fsspec-compatible filesystem meant that Lance's access patterns (range requests, listing, etc.) already had a solid, documented foundation to build on.</li> <li>OpenDAL (the unified data access layer Lance uses for cloud I/O) added a Hugging Face backend (#6801 feat(hugginface): allow specifying huggingface endpoint), so <code>hf://</code> URIs resolve to the Hub and Lance can read from it.</li> <li>Lance added support for the <code>hf://</code> URI scheme (#5353 feat: add huggingface native support), so <code>lance.dataset(\"hf://datasets/org/repo/path/to/table.lance\")</code> works against the same Hub storage and CDN that HfFileSystem uses.</li> <li>Lance and Hugging Face then worked together to integrate Lance into the datasets repo for automated discovery, display, and native streaming (#7913 Add lance format support, #7950 Add examples for Lance datasets, #7964 handle blob lance, #7966 Infer types from lance blobs, #7969 Count examples in lance).</li> </ul> <p>Community-driven integration</p> <p>The integration was driven by a concrete use case (SLAF on the Hub), implemented in Lance and OpenDAL on top of Hugging Face's streaming and HfFileSystem work, and validated with Hugging Face's team. We're super grateful to everyone who helped make <code>hf://</code> a first-class path for Lance.</p> <p>For more context, see Hugging Face's Streaming datasets: 100\u00d7 More Efficient and the Interact with the Hub through the Filesystem API guide.</p>"},{"location":"blog/huggingface/#beyond-single-cell-genomics-what-lance-on-hugging-face-means-for-aiml","title":"Beyond Single-Cell Genomics: What Lance on Hugging Face Means for AI/ML","text":"<p>Lance-on-HuggingFace isn't just for single-cell genomics or SLAF. The lance-format org on Hugging Face already hosts Lance datasets across modalities, all streamable via <code>hf://</code>:</p> <ul> <li>Long-form text: fineweb-edu (1.5B+ rows) and other tabular text datasets for pretraining and fine-tuning.</li> <li>Images: laion-1m and similar image (and image\u2013text) collections with optional embeddings and vector search.</li> <li>Video: openvid-lance and related video datasets for multimodal training.</li> <li>Robotics: lerobot_xvla-soft-fold and other robotics traces in Lance format for imitation and foundation models.</li> </ul> <p>The pattern is the same everywhere: publish once on the Hub, point Lance (or SLAF) at <code>hf://</code>, stream only what you need. That reduces duplication, speeds up iteration, and makes it easier to run large-scale training and batch jobs from a central, well-known platform.</p>"},{"location":"blog/huggingface/#future-workloads-embeddings-vector-search-and-update-rarely","title":"Future workloads: embeddings, vector search, and \"update rarely\"","text":"<p>The integration also opens the door to emerging AI/ML workloads on SLAF's roadmap: embedding data management, multimodal data apps, and low-latency vector search; all with the same \"store on the Hub, stream to your compute\" idea.</p> <p>Embedding data management. Suppose you want scGPT or Transcriptformer embeddings on your private dataset. You can run batch inference pipelines directly off Lance on Hugging Face: stream cells from <code>hf://</code>, compute embeddings via serverless Inference Endpoints or Hugging Face Jobs on Hugging Face compute, and write results back into Lance tables on the Hub. You can mix and match inference, batch compute, and storage to your preferred alternatives to Hugging Face; the common thread is Lance and streaming, with Hugging Face making the plumbing convenient.</p> <p>Apps that need vector search. Suppose you want a data app that uses nearest-neighbor search on embeddings to interactively apply cell typing to a new private dataset. Store embeddings in Lance tables on the Hub, attach a vector index (e.g. via LanceDB), and stream search results to your app. No need to download the full table or run your own vector store.</p> <p>Upgrading embeddings without 100\u00d7 copies. When a new version of a third-party embedding model ships, you don't have to clone the dataset and re-embed everything. In Lance, adding a new embedding version is just a new column in your table on Hugging Face; you backfill only that column and leave the rest of the data untouched. That shifts the vision from \"store once, stream everywhere\" to \"store once, update rarely, stream everywhere\", where \"update rarely\" is exactly this kind of inference-endpoint\u2013driven embedding lifecycle.</p>"},{"location":"blog/huggingface/#what-this-unlocks-for-lance-and-hugging-face","title":"What this unlocks for Lance and Hugging Face","text":"<p>For Lance, the integration opens a new distribution channel: teams that already distribute multimodal datasets or embeddings on the Hub represent a beachhead, and Lance can expand into managed offerings and richer tooling for such teams. For Hugging Face, it strengthens a path to stickiness around end-to-end AI/ML workloads: not just data distribution, but streaming, distributed training on HF compute, batch inference, open vector and multimodal data lake use cases, and hybrid search applications via LanceDB and Lance. We're super excited to see how both ecosystems evolve.</p> <p>For the broader Lance \u00d7 Hugging Face story including multimodal blobs, vector search, and training-friendly storage, see LanceDB's post Lance \u00d7 Hugging Face: A New Era of Sharing Multimodal Data on the Hub.</p> <p>We're excited to see how the community uses Lance and SLAF on Hugging Face and eager to expand support for more datasets.</p> <p>Single-cell folks: Try Parse-10M or Tahoe-100M from the Hub this week: point <code>SLAFArray</code> or <code>read_slaf</code> at <code>hf://datasets/slaf-project/&lt;repo&gt;</code> and run a query or a training loop without downloading. Install: <code>pip install slafdb[ml]</code>. For more, see the SLAF documentation and the slaf-project org on Hugging Face.</p>"},{"location":"blog/introducing-slaf/","title":"Introducing SLAF: The Single-Cell Data Format for the Virtual Cell Era","text":"<p>Single-cell datasets have grown from 50k to 100M cells in less than a decade. What used to fit comfortably in memory now requires out-of-core, distributed computing. What used to transfer in minutes now takes days. What used to be a simple analysis pipeline for a bioinformatician now requires a team of infrastructure engineers.</p> <p>The single-cell data explosion has created a fundamental mismatch between our tools and our needs. We're trying to solve 2025 problems with 2015 technology. But it doesn't have to be that way.</p> <p>This is the story of SLAF (Sparse Lazy Array Format) --- a cloud-native, SQL-powered format designed for the modern single-cell era.</p>"},{"location":"blog/introducing-slaf/#a-10-year-lookback-at-storage-and-compute-revolutions","title":"A 10-year lookback at storage and compute revolutions","text":"<p>Before diving into single-cell data, I thought it would be worth a historical detour through some key innovations over the past decade that directly inspired SLAF's architecture.</p> <p>The Big Picture</p> <p>We'll explore six key innovations that revolutionized data storage and compute:</p> <ol> <li>Compressed, chunked storage with optimized I/O</li> <li>Cloud-native chunked storage enabling concurrent I/O and computation</li> <li>Lazy computation for complex workflows (Dask, Polars)</li> <li>Skip what you don't need (Parquet's predicate/projection pushdown)</li> <li>Zero-infrastructure embedded databases on disk (Lance, DuckDB)</li> <li>High-performance query engines (Polars, DuckDB, Data Fusion)</li> </ol>"},{"location":"blog/introducing-slaf/#the-zarr-revolution-compressed-chunked-storage","title":"The Zarr Revolution: Compressed, Chunked Storage","text":"<p>The Problem</p> <p>In 2016, Alistair Miles, a genomics scientist at the Wellcome Trust Sanger Institute, was struggling with genetic variation data containing around 20 billion elements - too large for memory, too small for distributed computing. Traditional compute would OOM-kill his research before it began.</p> <p>Miles discovered HDF5 with the h5py Python library, which provided a great solution for storing multi-dimensional arrays. The arrays were divided into chunks, each compressed, enabling efficient storage and fast access. But there was a problem: HDF5 was still too slow for interactive work.</p> <p>He then discovered Bcolz, which used the Blosc compression library. Blosc could use multiple threads internally, worked well with CPU cache architecture, and was much faster than HDF5. In his benchmarks, Bcolz was more than 10 times faster at storing data than HDF5.</p> <p>But Bcolz had a limitation: it could only be chunked along the first dimension. Taking slices of other dimensions required reading and decompressing the entire array.</p> <p>So Miles created Zarr, which like Bcolz used Blosc internally but supported chunking along multiple dimensions. This enabled better performance for multiple data access patterns.</p> <p>Key Innovation #1</p> <p>Compressed, chunked storage with optimized encoding/decoding for speed of I/O.</p> <p>From these early beginnings, Zarr has grown far beyond genomics. Today it powers massive array formats across climate science, geospatial data, medical imaging, and microscopy --- any domain dealing with large, multi-dimensional arrays that need efficient cloud-native access.</p>"},{"location":"blog/introducing-slaf/#the-zarrdask-collaboration-remote-storage-concurrency","title":"The Zarr/Dask Collaboration: Remote Storage &amp; Concurrency","text":"<p>The Problem</p> <p>Matt Rocklin (then, at the Anaconda foundation) was tackling a similar challenge: how to work with arrays too big for memory in numpy for embarassingly parallel computations.</p> <p>Miles created Zarr for compressed, chunked storage. Rocklin built Dask for lazy, out-of-core computation (early ideas, consolidation).</p> <p>But there were two separate technical challenges to solve for true cloud-native array access:</p> <p>Challenge 1: Remote Storage Access Zarr could read chunks efficiently from local storage, but accessing remote storage (like S3) required a different approach. Traditional file I/O was designed for local filesystems, not HTTP-based object storage with different latency characteristics. Zarr solved this by implementing HTTP-based chunk access that could work directly with cloud object storage.</p> <p>Challenge 2: The Python GIL Bottleneck The fundamental problem was the Python Global Interpreter Lock (GIL). As Alistair Miles documented, h5py doesn't release the GIL during I/O operations, which means other threads cannot run while h5py is doing anything - even if those threads want to do something unrelated to HDF5 I/O. This serialized all parallel operations, limiting CPU utilization to just over 1 core (~130% CPU).</p> <p>The breakthrough came when Zarr was designed to release the GIL during compression and decompression. This simple but crucial design decision meant other threads could carry on working in parallel. This GIL-aware design enabled true concurrent I/O and computation --- a fundamental requirement for cloud-native array access.</p> <p>When they collaborated, combining Zarr's remote chunked storage with Dask's process-based distributed computation, they finally achieved distributed computation on remote arrays for truly concurrent cloud-native array access.</p> <p>Key Innovation #2</p> <p>Cloud-native chunked storage enabling concurrent I/O and computation.</p>"},{"location":"blog/introducing-slaf/#the-lazy-computation-revolution","title":"The Lazy Computation Revolution","text":"<p>The Problem</p> <p>Traditional eager computation loads data into memory immediately and processes it step by step. This works fine for small datasets, but becomes impossible when your data is larger than RAM. You can't even start the computation because the first step fails with \"Out of Memory\".</p> <p>As Matt Rocklin described, Dask started as \"a parallel on-disk array\" that solved a fundamental problem: how do you compute on data that's too big for memory?</p> <p>The key insight was lazy computation graphs. Instead of immediately loading and processing data, Dask builds a recipe (a directed acyclic graph) of all the operations you want to perform. This graph is just functions, dicts, and tuples - lightweight and easy to understand. Only when you call <code>.compute()</code> does Dask actually execute the operations, and it can optimize the entire graph before execution.</p> <p>For example, if you want to sum a trillion numbers, Dask breaks this into a million smaller sums that each fit in memory, then sums the sums. A previously impossible task becomes a million and one easy ones.</p> <p>Polars took this to the next level for dataframes, enabling complex query chains that only touch the data they need. The entire query is optimized before any data is loaded.</p> <p>Key Innovation #3</p> <p>Lazy computation graphs that optimize before execution.</p>"},{"location":"blog/introducing-slaf/#the-parquet-revolution-skip-what-you-dont-need","title":"The Parquet Revolution: Skip What You Don't Need","text":"<p>The Problem</p> <p>The Zarr/Dask combination was fantastic for embarrassingly parallel work across entire datasets. But what if you only wanted to analyze a subset of data that met certain criteria?</p> <p>Meanwhile, Parquet didn't solve the partial compute problem but its design taught us how we might. Parquet was born at Twitter and Cloudera during the early days of the transition from OLTP to OLAP databases, and from row-based to columnar storage. The insight was simple: analytical queries often access only a few columns, so storing data column-by-column enables much better compression and faster queries.</p> <p>Parquet stores data in compressed columnar chunks called \"row groups\" and stores metadata about these chunks. At query time, you only materialize the metadata in memory, then use it to decide:</p> <ul> <li>Which row groups to skip reading (predicate pushdown)</li> <li>Which columns to skip reading (projection pushdown)</li> </ul> <p>Key Innovation #4</p> <p>Don't just make I/O faster, skip most I/O you don't need to do.</p>"},{"location":"blog/introducing-slaf/#the-lance-revolution-zero-infrastructure-embedded-databases","title":"The Lance Revolution: Zero-Infrastructure Embedded Databases","text":"<p>The Problem</p> <p>Parquet was revolutionary for analytical queries, but it had fundamental limitations that became apparent as data workloads evolved:</p> <ul> <li> <p>Database features missing: Schema evolution, partitioning, time travel, and ACID transactions weren't first-class citizens. Apache Iceberg emerged to address these limitations, but locked into Parquet as a format.</p> </li> <li> <p>AI-era data types: Parquet preceded the AI era, so vector embeddings, multimodal data, and vector search weren't native. Modern AI workloads need to store and query embeddings alongside structured data.</p> </li> <li> <p>Metadata overhead: Parquet's metadata can become expensive and suboptimal for random access patterns, especially as datasets grow.</p> </li> <li> <p>Infrastructure complexity: Traditional databases require servers, configuration, and ongoing maintenance --- a barrier for many use cases.</p> </li> </ul> <p>Lance directly addressed these limitations by creating a table format that provides:</p> <ul> <li>Database features: Schema evolution, time travel, partitioning, and ACID transactions as first-class citizens (parity with Iceberg)</li> <li>AI-native design: Native support for vector embeddings, multimodal data, and vector search</li> <li>Optimized metadata: Efficient random access patterns with minimal metadata overhead</li> <li>Zero infrastructure: A file on disk is a full database, like the next generation SQLite</li> </ul> <p>Polars takes a different but complementary approach: a high-performance DataFrame library written in Rust that provides blazing-fast SQL query execution with zero infrastructure. While Lance focuses on the table format, Polars focuses on both its own DataFrame operations and a highly extensible query engine. Both eliminate the need for servers and configuration.</p> <p>Key Innovation #5</p> <p>Zero-infrastructure embedded databases on disk.</p>"},{"location":"blog/introducing-slaf/#the-olap-query-engine-revolution-fast-languages-lazy-execution","title":"The OLAP Query Engine Revolution: Fast Languages + Lazy Execution","text":"<p>The final piece was query engines written in faster languages than Python. Polars is written in Rust and executes chained query workloads lazily with blazing-fast SQL execution that can push down complex queries to the storage layer.</p> <p>By combining high-performance query engines with metadata-rich table formats like Parquet and Lance, you get superpowers: complex queries that only touch the data they need.</p> <p>Key Innovation #6</p> <p>High-performance query engines that can skip unnecessary computation.</p>"},{"location":"blog/introducing-slaf/#the-single-cell-gap","title":"The Single-Cell Gap","text":"<p>The single-cell community evolved from simple MTX format (essentially TSV files) to H5AD (HDF5-based AnnData). Innovation happened on the metadata storage side with AnnData, and computational workflows with Scanpy leveraging scipy sparse matrices and numpy. But storage hasn't leveraged the amazing innovations in cloud-native, out-of-core, query-optimized, lazy, metadata-rich workflows that revolutionized other domains.</p> <p>A format like SLAF can bring these proven innovations to single-cell data.</p>"},{"location":"blog/introducing-slaf/#the-evolution-of-single-cell-data-storage","title":"The Evolution of Single-Cell Data Storage","text":"<p>Single-cell data storage has evolved from simple MTX format (TSV files) to H5AD (HDF5-based AnnData) as the de facto standard. While H5AD brought compression and Scanpy integration, it has fundamental limitations: single-file format (no concurrent access), in-memory operations (everything must fit in RAM), and no cloud-native access.</p> <p>Recent attempts like SOMA (TileDB/CZI collaboration) and cellxgene-census recognize these limitations but have different focuses: SOMA is broad-scope sparse arrays, while cellxgene-census is tied to specific datasets. The gap remains: we need cloud-native storage that works seamlessly with existing bioinformatics tools while expanding to AI use cases.</p>"},{"location":"blog/introducing-slaf/#the-scale-problem-why-current-tools-are-breaking","title":"The Scale Problem: Why Current Tools Are Breaking","text":"<p>Let's paint a picture of what happens when you try to analyze a 100M-cell dataset with current tools.</p> <p> Infrastructure Bottlenecks</p> <p>A typical 100M-cell dataset might be 500GB-1TB. With H5AD: download takes hours to days, must fit entire dataset in memory (impossible on most machines), all operations happen in memory.</p> <p>You can't analyze data larger than your RAM.</p> <p> Human Bottlenecks</p> <p>Bioinformatician wants to analyze large dataset \u2192 requests infrastructure changes \u2192 waits weeks \u2192 downloads massive dataset \u2192 runs into memory issues \u2192 requests more RAM \u2192 waits again.</p> <p>Every scale-up requires infrastructure engineers.</p> <p> Data Duplication</p> <p>Team of 5 bioinformaticians \u00d7 500GB dataset \u00d7 5 experiments = 12.5TB for what should be 500GB. Each researcher needs their own copy, each experiment needs a copy.</p> <p>This data multiplication problem is unsustainable at scale.</p> <p> AI Workload Mismatch</p> <p>Traditional workflows: filtering, normalization, PCA, clustering. New AI-native workflows: nearest neighbor search on embeddings, gene-gene relationship ranking, transformer training, distributed training.</p> <p>Current tools weren't designed for these workloads.</p>"},{"location":"blog/introducing-slaf/#who-is-slaf-for","title":"Who is SLAF For?","text":"<p>SLAF is designed for the modern single-cell ecosystem facing scale challenges:</p> Persona Problem Current Reality How SLAF Helps Bioinformaticians OOM errors on 10M+ cell datasets, can't do self-service analysis Need infrastructure engineers, wait weeks for provisioning Lazy evaluation eliminates OOM errors, enables 100M-cell analysis on laptop Foundation Model Builders Spending more time on data engineering than model training, data transfer bottlenecks Copy 500GB datasets to each node, experiments bottlenecked by throughput Cloud-native streaming eliminates duplication, SQL queries enable efficient tokenization Tech Leaders &amp; Architects Storage costs exploding, data multiplication problem 5 bioinformaticians \u00d7 500GB \u00d7 5 experiments = 12.5TB for 500GB data Zero-copy, query-in-place storage means one copy serves everyone Tool Builders (cellxgene, LatchBio etc.) Can't provide interactive experiences on massive datasets Users wait minutes, need expensive infrastructure Concurrent, cloud-scale access with high QPS, commodity web services Atlas Builders (CZI, etc.) Can't serve massive datasets to research community Datasets too large to download, expensive serving infrastructure Cloud-native, zero-copy storage enables global access without downloads Data Integrators (Elucidata, etc.) Complex data integration workflows don't scale Multiple data transfers, expensive compute for joins/aggregations SQL-native design enables complex integration with pushdown optimization"},{"location":"blog/introducing-slaf/#how-slaf-works","title":"How SLAF Works","text":"<p>SLAF combines the best ideas from multiple domains into a unified solution:</p> <ul> <li>Zarr-inspired: Zero-copy, query-in-place access to cloud storage</li> <li>Dask-inspired: Lazy computation graphs that optimize before execution</li> <li>Lance + Polars-inspired: OLAP-powered SQL with pushdown optimization</li> <li>Scanpy-compatible: Drop-in replacement for existing workflows</li> </ul> <p>For a detailed technical deep dive into SLAF's architecture, including SQL-native relational schema, lazy evaluation, cloud-native concurrent access, and foundation model training support, see How SLAF Works.</p>"},{"location":"blog/introducing-slaf/#getting-started","title":"Getting Started","text":"<p>SLAF is designed to be easy to adopt. Here's how to get started:</p>"},{"location":"blog/introducing-slaf/#installation","title":"Installation","text":"<pre><code># Using uv (recommended)\nuv add slafdb\n\n# Or pip\npip install slafdb\n</code></pre>"},{"location":"blog/introducing-slaf/#basic-usage","title":"Basic Usage","text":"<pre><code>from slaf import SLAFArray\n\n# Load a SLAF dataset\nslaf_array = SLAFArray(\"path/to/dataset.slaf\")\n\n# Query with SQL\nresults = slaf_array.query(\"\"\"\n    SELECT cell_type, COUNT(*) as count\n    FROM cells\n    GROUP BY cell_type\n\"\"\")\n\n# Use with Scanpy\nfrom slaf.integrations import read_slaf\nadata = read_slaf(\"path/to/dataset.slaf\")\n</code></pre>"},{"location":"blog/introducing-slaf/#migration-from-h5ad","title":"Migration from H5AD","text":"<pre><code># Convert existing H5AD to SLAF\nfrom slaf.data import SLAFConverter\n\n# Convert h5ad file to SLAF format\nconverter = SLAFConverter()\nconverter.convert(\"input.h5ad\", \"output.slaf\")\n\n# Or convert from AnnData object\nimport scanpy as sc\nadata = sc.read_h5ad(\"input.h5ad\")\nconverter.convert_anndata(adata, \"output.slaf\")\n</code></pre>"},{"location":"blog/introducing-slaf/#future-directions-and-community-feedback","title":"Future Directions and Community Feedback","text":"<p>SLAF could go in several directions and I'd like to get feedback on which of them would benefit the community the most.</p> <p>Visualization Integration: We lack direct visualization support. A sketch of how SLAF could be more directly compatible with cellxgene might be super useful for Atlas builders to transition to SaaS-based analytics. This could enable interactive exploration of 100M-cell datasets without the infrastructure headaches that currently plague large-scale visualization. What visualization workflows would you prioritize?</p> <p>Embeddings Support: Native storage and querying of cell/gene embeddings could unlock entirely new AI-native workflows. Instead of computing embeddings separately and storing them in different systems, SLAF could store embeddings alongside expression data and enable queries like \"find cells similar to this embedding\" or \"rank genes by embedding similarity.\" This would be particularly valuable for foundation model training and retrieval-augmented generation. What embedding use cases are most critical for your work?</p> <p>Hosted Solution: A cloud-hosted database version of SLAF could push query performance even further by eliminating the need for local storage entirely. This would enable true zero-infrastructure analysis where researchers query massive datasets directly from the cloud without any local setup. The challenge is balancing performance with cost. What query patterns and performance requirements would make this viable for your workflows?</p> <p>Migration Tools: Better tools to migrate from mtx/h5ad to SLAF could accelerate adoption significantly. This includes not just format conversion but intelligent optimization of hydrating the SLAF format for different dataset characteristics. What migration pain points are most blocking your adoption of new formats?</p> <p>Distributed Computing: Better support for distributed analysis workflows could enable truly massive-scale analysis beyond what any single machine can handle. This includes both distributed query execution and distributed training support for foundation models. What distributed computing patterns are most important for your large-scale analysis needs?</p> <p>Scanpy Feature Parity: We currently only support a couple of limited use cases from scanpy's preprocessing module. Building lazy operations for broader needs like PCA, UMAP, or differential expression computations at scale could make SLAF a true drop-in replacement for existing scanpy workflows. This would enable researchers to run familiar analysis pipelines on datasets that would otherwise cause memory explosions. Which scanpy operations are most critical for your large-scale analysis workflows?</p> <p>Foundation Model Training Lifecycles: Our current dataloader is basic - no smart shuffling, shard-aware streaming, async pre-fetching, or real-world benchmarks on multi-node training yet. SLAF could help frame and drive moonshots like \"the $100 overnight scGPT training experiment\" by driving up the efficiencies of multi-node, multi-GPU training and fine-tuning workloads. What training bottlenecks are most limiting your foundation model experiments?</p> <p>Cloud-Native Benchmarks: Limited real-world benchmarks for cloud-native datasets. Today's benchmarks are all on local storage for small datasets. Would this be a blocker to adoption? What performance characteristics and benchmarks would give you confidence to adopt SLAF for production workloads?</p> <p>If you've read this far, thank you! You're definitely invested in the space.</p> <p>The single-cell data explosion isn't going to slow down. If anything, it's accelerating. We need tools that can keep up. SLAF is an early attempt to build the modern companion for bioinformaticians turned AI engineers.</p> <p>I'm excited to see how you adopt and extend SLAF. Together, we can build the infrastructure needed for the next decade of single-cell research.</p> <p>Want to learn more? Check out the SLAF documentation or join the conversation on GitHub.</p>"},{"location":"blog/refactor-dataloaders/","title":"Refactoring SLAF DataLoaders: From SQL to Fragment-Based Processing","text":"<p>Implementation plan for achieving GPU-saturating throughput in single-cell data loading</p>"},{"location":"blog/refactor-dataloaders/#motivation-opportunity","title":"Motivation &amp; Opportunity","text":"<p>Our recent experiments revealed a massive performance opportunity in single-cell data loading. Starting with SQL-based window functions that achieved only 256 cells/minute, we systematically optimized through multiple approaches:</p> <ul> <li>SQL Window Functions: 32 cells/minute</li> <li>Prefilter + Pandas: 35K cells/minute</li> <li>Prefilter + Polars: 60K cells/minute</li> <li>Fragment + Polars: 38K cells/second</li> </ul> <p>This represents a 70,000x performance improvement, transforming single-cell data loading from a bottleneck into a GPU-saturating pipeline. The key insight: Lance fragments are already optimally sized (~380MB each) and load in &lt;0.5 seconds, making direct fragment access far more efficient than query engine-based approaches.</p> <p>With modern AI training requiring 5-6K cells/second for 8\u00d7H100 nodes, our fragment-based approach can support ~50 GPUs directly from SSD storage, enabling the paradigm of \"$100 scGPT in an hour.\"</p>"},{"location":"blog/refactor-dataloaders/#implementation-status","title":"Implementation Status","text":""},{"location":"blog/refactor-dataloaders/#phase-0-risk-mitigation-completed","title":"\u2705 Phase 0: Risk Mitigation (COMPLETED)","text":"<p>Goal: Validate core assumptions before major refactoring</p> <p>Completed Validations:</p> <ol> <li>\u2705 Fragment Metadata Access Performance - Validated as extremely fast (~0.22s for 193 fragments)</li> <li>\u274c MosaicML StreamingDataset Integration - ABANDONED (incompatible with direct Lance fragment processing)</li> <li>\u2705 Fragment-to-Cell-Range Mapping - Validated and working efficiently</li> <li>\u2705 Polars Window Function Performance - Validated at 38K cells/second</li> </ol>"},{"location":"blog/refactor-dataloaders/#phase-1-fragment-based-tokenizer-completed","title":"\u2705 Phase 1: Fragment-Based Tokenizer (COMPLETED)","text":"<p>Goal: Replace SQL with Polars fragment processing</p> <p>Completed Tasks:</p> <ol> <li>\u2705 Fragment utilities - <code>FragmentLoader</code> class with optimized fragment loading</li> <li>\u2705 Polars window functions - Working scGPT tokenization with Polars</li> <li>\u2705 Async prefetching - <code>AsyncFragmentPrefetcher</code> with threading</li> <li>\u2705 Streaming dataloader - <code>StreamingDataLoader</code> for batch processing</li> </ol>"},{"location":"blog/refactor-dataloaders/#phase-15-update-slaftokenizer-critical-gap","title":"\ud83d\udd04 Phase 1.5: Update SLAFTokenizer (CRITICAL GAP)","text":"<p>Goal: Update <code>slaf/ml/tokenizers.py</code> to use fragment-based processing instead of SQL queries</p> <p>Current Problem:</p> <ul> <li><code>tokenizers.py</code> now uses SQL queries via Polars (fast: 38K cells/second)</li> <li>Our new implementation uses fragment-based Polars processing (fast: 38K cells/second)</li> <li>This creates a performance mismatch and architectural inconsistency</li> </ul> <p>Better Architectural Approach:</p> <p>Analysis of Current Working Architecture:</p> <ol> <li>FragmentLoader: Tokenizer-agnostic, does Polars window functions \u2192 returns <code>RawFragment</code> with gene sequences</li> <li>StreamingDataLoader: Takes <code>RawFragment</code> \u2192 converts gene sequences to tokens \u2192 yields batches</li> <li>Separation: Fragment processing (Polars) vs Tokenization (Python)</li> </ol> <p>Key Discovery: Window Functions Are Different!</p> <ul> <li>scGPT: Uses <code>ROW_NUMBER()</code> and returns both <code>gene_sequence</code> AND <code>expr_sequence</code></li> <li>Geneformer: Uses <code>RANK()</code> and returns only <code>ranked_genes</code></li> <li>scGPT: Needs expression values for binning \u2192 <code>array_agg(value ORDER BY gene_rank) as expr_sequence</code></li> <li>Geneformer: Only needs gene IDs \u2192 <code>array_agg(gene_id ORDER BY expression_rank) as ranked_genes</code></li> </ul> <p>Corrected Design:</p> <ol> <li>\ud83d\udd04 Create Tokenizer-Specific Fragment Processors</li> </ol> <pre><code>class ScGPTFragmentProcessor:\n    \"\"\"Fragment processor specialized for scGPT tokenization\"\"\"\n    def process_fragment(self, fragment_id: int) -&gt; RawFragment:\n        # Returns gene_sequences AND expr_sequences\n\nclass GeneformerFragmentProcessor:\n    \"\"\"Fragment processor specialized for Geneformer tokenization\"\"\"\n    def process_fragment(self, fragment_id: int) -&gt; RawFragment:\n        # Returns gene_sequences only\n</code></pre> <ol> <li>\ud83d\udd04 Update SLAFTokenizer to use specific processors</li> </ol> <pre><code>class SLAFTokenizer:\n    def __init__(self, slaf_array: SLAFArray, use_fragment_processing: bool = True):\n        if use_fragment_processing:\n            self.scgpt_processor = ScGPTFragmentProcessor(slaf_array)\n            self.geneformer_processor = GeneformerFragmentProcessor(slaf_array)\n\n    def tokenize_scgpt_fragment_based(self, cell_range: tuple[int, int], max_genes: int):\n        # Use ScGPTFragmentProcessor for fragment processing\n        # Apply scGPT-specific tokenization with expression values\n\n    def tokenize_geneformer_fragment_based(self, cell_range: tuple[int, int], max_genes: int):\n        # Use GeneformerFragmentProcessor for fragment processing\n        # Apply Geneformer-specific tokenization (genes only)\n</code></pre> <ol> <li>\ud83d\udd04 Update RawFragment to support optional expression sequences <pre><code>@dataclass\nclass RawFragment:\n    fragment_id: int\n    gene_sequences: list[list[int]]\n    cell_integer_ids: list[int]\n    expr_sequences: list[list[float]] | None = None  # For scGPT only\n</code></pre></li> </ol> <p>Benefits of This Corrected Approach:</p> <ul> <li>Correct Window Functions: Each tokenizer gets the right data structure</li> <li>Performance: 70,000x improvement (32 cells/min \u2192 38K cells/sec)</li> <li>Accuracy: scGPT gets expression values, Geneformer gets genes only</li> <li>Maintainability: Clear separation of concerns with tokenizer-specific logic</li> </ul> <p>Implementation Steps:</p> <ol> <li> <p>\ud83d\udd04 Extract FragmentProcessor from FragmentLoader</p> </li> <li> <p>Move Polars window functions to reusable class</p> </li> <li> <p>Make it tokenizer-agnostic</p> </li> <li> <p>\ud83d\udd04 Update SLAFTokenizer methods</p> </li> <li> <p><code>tokenize_scgpt_fragment()</code> - Use FragmentProcessor + scGPT logic</p> </li> <li> <p><code>tokenize_geneformer_fragment()</code> - Use FragmentProcessor + Geneformer logic</p> </li> <li> <p>\ud83d\udd04 Create comprehensive tests</p> </li> <li> <p>Test FragmentProcessor independently</p> </li> <li>Test each tokenizer with FragmentProcessor</li> <li> <p>Ensure backward compatibility</p> </li> <li> <p>\ud83d\udd04 Update DataLoader integration</p> </li> <li>Make DataLoader use new fragment-based tokenizers</li> <li>Maintain existing API for backward compatibility</li> </ol>"},{"location":"blog/refactor-dataloaders/#phase-2-threading-based-prefetching-completed","title":"\u2705 Phase 2: Threading-Based Prefetching (COMPLETED)","text":"<p>Goal: Background fragment loading</p> <p>Completed Tasks:</p> <ol> <li>\u2705 Fragment prefetch queue - <code>AsyncFragmentPrefetcher</code> with <code>Queue</code></li> <li>\u2705 Async tokenization - Background fragment loading and processing</li> <li>\u2705 Performance optimization - Achieved ~150 batches/sec with GPU capacity for ~300</li> </ol>"},{"location":"blog/refactor-dataloaders/#phase-3-pytorch-harmonization-in-progress","title":"\ud83d\udd04 Phase 3: PyTorch Harmonization (IN PROGRESS)","text":"<p>Goal: Replace custom DataLoader with standard PyTorch interfaces</p> <p>Current Progress:</p> <ol> <li>\u2705 <code>SLAFIterableDataset</code> - Proper PyTorch <code>IterableDataset</code> subclass</li> <li>\u2705 Updated <code>SLAFDataLoader</code> - Optional use of new dataset</li> <li>\u2705 Standard PyTorch interfaces - Compatible with <code>torch.utils.data.DataLoader</code></li> </ol> <p>Remaining Tasks:</p> <ol> <li>\ud83d\udd04 Complete PyTorch integration - Finalize dataset and dataloader classes</li> <li>\ud83d\udd04 Add PyTorch features - Multiprocessing, pin_memory, custom samplers</li> <li>\ud83d\udd04 Performance optimization - Address bottleneck in <code>pl.from_arrow(fragment.to_table())</code></li> <li>\ud83d\udd04 Comprehensive testing - Ensure correctness and performance</li> </ol>"},{"location":"blog/refactor-dataloaders/#current-performance-status","title":"Current Performance Status","text":"<ul> <li>Throughput: ~150 batches/sec (target: 300+ batches/sec for GPU saturation)</li> <li>Bottleneck: <code>pl.from_arrow(fragment.to_table())</code> operation (~3-4s per fragment)</li> <li>GPU Capacity: Can handle ~300 batches/sec</li> <li>Optimization Opportunity: Minimize GPU idle time by improving data pipeline throughput</li> </ul>"},{"location":"blog/refactor-dataloaders/#next-steps","title":"Next Steps","text":""},{"location":"blog/refactor-dataloaders/#immediate-phase-3-completion","title":"Immediate (Phase 3 Completion)","text":"<ol> <li> <p>Complete PyTorch harmonization</p> </li> <li> <p>Finalize <code>SLAFIterableDataset</code> implementation</p> </li> <li>Add comprehensive PyTorch features (multiprocessing, pin_memory)</li> <li> <p>Create custom samplers for advanced use cases</p> </li> <li> <p>Performance optimization</p> </li> <li> <p>Investigate alternatives to <code>pl.from_arrow(fragment.to_table())</code></p> </li> <li>Consider fragment size tuning for different storage types</li> <li> <p>Explore parallel fragment loading strategies</p> </li> <li> <p>Comprehensive testing</p> </li> <li>Add unit tests for all PyTorch components</li> <li>Create performance benchmarks</li> <li>Validate correctness across different datasets</li> </ol>"},{"location":"blog/refactor-dataloaders/#future-considerations","title":"Future Considerations","text":"<ol> <li> <p>Fragment loading optimization</p> </li> <li> <p>Investigate direct Lance-to-Polars conversion</p> </li> <li>Consider memory-mapped fragment access</li> <li> <p>Explore GPU-accelerated fragment loading</p> </li> <li> <p>Advanced PyTorch features</p> </li> <li> <p>Custom <code>torch.utils.data.Sampler</code> implementations</p> </li> <li><code>torch.utils.data.Dataset</code> for non-streaming use cases</li> <li> <p>Integration with PyTorch Lightning and other frameworks</p> </li> <li> <p>Cloud deployment optimization</p> </li> <li>Async I/O for object storage</li> <li>Fragment caching strategies</li> <li>Network-optimized fragment loading</li> </ol>"},{"location":"blog/refactor-dataloaders/#design-decisions","title":"Design Decisions","text":""},{"location":"blog/refactor-dataloaders/#1-api-design-fragment-agnostic","title":"1. API Design: Fragment-Agnostic","text":"<pre><code># User specifies cell ranges, we handle fragment mapping internally\ntokens = tokenizer.tokenize_scgpt(cell_integer_id_range=(0, 1024))\n</code></pre> <p>Benefits:</p> <ul> <li>Storage-agnostic API</li> <li>Easier migration for existing users</li> <li>Can optimize fragment loading internally</li> <li>Better for future storage backends</li> </ul>"},{"location":"blog/refactor-dataloaders/#2-fragment-mapping-strategy","title":"2. Fragment Mapping Strategy","text":"<pre><code>def _map_cell_range_to_fragments(self, cell_range: tuple[int, int]) -&gt; list[int]:\n    \"\"\"Map cell range to required fragment IDs\"\"\"\n    start, end = cell_range\n\n    # Query fragment metadata to find which fragments contain our cells\n    fragment_ids = []\n    for fragment_id in range(self.num_fragments):\n        fragment_cells = self.get_fragment_cell_range(fragment_id)\n        if self._ranges_overlap((start, end), fragment_cells):\n            fragment_ids.append(fragment_id)\n\n    return fragment_ids\n</code></pre>"},{"location":"blog/refactor-dataloaders/#3-polars-based-window-functions","title":"3. Polars-Based Window Functions","text":"<p>Replace all SQL with Polars operations:</p> <pre><code>def _tokenize_scgpt_polars(self, fragment_df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Apply scGPT tokenization using Polars window functions\"\"\"\n\n    # Rank genes by expression within each cell\n    result = fragment_df.with_columns([\n        pl.col(\"value\").rank(method=\"dense\", descending=True)\n        .over(\"cell_integer_id\")\n        .alias(\"gene_rank\")\n    ]).filter(pl.col(\"gene_rank\") &lt;= self.max_genes)\n\n    # Group by cell and create arrays\n    final_result = result.group_by(\"cell_integer_id\").agg([\n        pl.col(\"gene_integer_id\").alias(\"gene_sequence\"),\n        pl.col(\"value\").alias(\"expr_sequence\")\n    ])\n\n    return final_result\n</code></pre>"},{"location":"blog/refactor-dataloaders/#4-pytorch-integration-strategy","title":"4. PyTorch Integration Strategy","text":"<p>Instead of MosaicML (abandoned due to incompatibility), we're using standard PyTorch interfaces:</p> <pre><code>class SLAFIterableDataset(torch.utils.data.IterableDataset):\n    def __init__(self, slaf_array, tokenizer, **kwargs):\n        super().__init__(**kwargs)\n        self.slaf_array = slaf_array\n        self.tokenizer = tokenizer\n        self.prefetcher = AsyncFragmentPrefetcher(...)\n\n    def __iter__(self):\n        # Stream batches with async prefetching\n        return self._stream_batches()\n</code></pre>"},{"location":"blog/refactor-dataloaders/#5-async-prefetching-strategy","title":"5. Async Prefetching Strategy","text":"<p>For 20 fragments/GPU with &lt;1 second load times, we use threading-based prefetch queue:</p> <pre><code>class AsyncFragmentPrefetcher:\n    def __init__(self, fragment_loader: FragmentLoader, max_queue_size: int = 10):\n        self.queue = Queue(maxsize=max_queue_size)\n        self.worker_thread = Thread(target=self._prefetch_worker)\n\n    def _prefetch_worker(self):\n        while True:\n            fragment_id = self.get_next_fragment_id()\n            fragment = self.fragment_loader.load_fragment(fragment_id)\n            self.queue.put(fragment)\n</code></pre> <p>Why threading over asyncio:</p> <ul> <li>Threading: Better for I/O-bound operations (disk reads)</li> <li>Asyncio: Better for network I/O and CPU-bound tasks</li> <li>Our case: Fragment loading is I/O-bound, so threading wins</li> <li>Cloud note: For object store/EFS deployments, asyncio may be preferred</li> </ul>"},{"location":"blog/refactor-dataloaders/#risk-assessment-mitigation","title":"Risk Assessment &amp; Mitigation","text":""},{"location":"blog/refactor-dataloaders/#high-risk-items","title":"High Risk Items:","text":"<ol> <li>MosaicML Integration Complexity - MITIGATED: Abandoned in favor of PyTorch harmonization</li> <li>Polars Performance Regression - MITIGATED: Benchmark against current SQL</li> <li>Fragment Mapping Accuracy - MITIGATED: Comprehensive testing</li> </ol>"},{"location":"blog/refactor-dataloaders/#medium-risk-items","title":"Medium Risk Items:","text":"<ol> <li>Threading Race Conditions - MITIGATED: Proper queue management</li> <li>Memory Usage Spikes - MITIGATED: Fragment size limits</li> <li>PyTorch Integration Complexity - MITIGATION: Gradual migration with backward compatibility</li> </ol>"},{"location":"blog/refactor-dataloaders/#low-risk-items","title":"Low Risk Items:","text":"<ol> <li>API Changes - MITIGATED: Backward compatibility maintained</li> <li>Fragment Loading - MITIGATED: Proven fast in benchmarks</li> </ol>"},{"location":"blog/refactor-dataloaders/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"blog/refactor-dataloaders/#1-backward-compatibility","title":"1. Backward Compatibility","text":"<ul> <li>Keep existing API methods</li> <li>Add new fragment-optimized methods with <code>_fragment</code> suffix</li> <li>Gradual migration path with <code>use_new_dataset</code> parameter</li> </ul>"},{"location":"blog/refactor-dataloaders/#2-configuration-options","title":"2. Configuration Options","text":"<pre><code>class SLAFDataLoader:\n    def __init__(self,\n                 slaf_array: SLAFArray,\n                 use_new_dataset: bool = False,  # Use PyTorch-compatible dataset\n                 **kwargs):\n</code></pre>"},{"location":"blog/refactor-dataloaders/#3-performance-monitoring","title":"3. Performance Monitoring","text":"<pre><code>class TokenizationMetrics:\n    def __init__(self):\n        self.fragment_load_times = []\n        self.window_function_times = []\n        self.total_times = []\n</code></pre>"},{"location":"blog/refactor-dataloaders/#questions-for-discussion","title":"Questions for Discussion","text":"<ol> <li>Fragment Loading: Should we investigate alternatives to <code>pl.from_arrow(fragment.to_table())</code>?</li> <li>PyTorch Features: Which PyTorch features should we prioritize (multiprocessing, pin_memory, custom samplers)?</li> <li>Performance Target: Should we accept 150 batches/sec or continue optimizing for 300+?</li> <li>Fragment Size: Should we tune fragment sizes for different storage types?</li> <li>Error Handling: How should we handle fragment loading failures in PyTorch context?</li> </ol>"},{"location":"blog/refactor-dataloaders/#next-steps_1","title":"Next Steps","text":"<ol> <li>Complete Phase 3: Finish PyTorch harmonization</li> <li>Performance optimization: Address the fragment loading bottleneck</li> <li>Comprehensive testing: Ensure correctness and performance</li> <li>Documentation: Create migration guide for users</li> </ol> <p>This plan gives us a clear path from our current SQL-based approach to a high-performance, fragment-optimized, PyTorch-compatible system that can handle the scale of modern single-cell AI training!</p>"},{"location":"blog/refactor-tokenizers/","title":"Refactoring SLAF Tokenizers: From SQL to Vectorized Processing","text":"<p>Implementation plan for achieving optimal performance and clean architecture in single-cell data tokenization</p>"},{"location":"blog/refactor-tokenizers/#motivation-current-state","title":"Motivation &amp; Current State","text":"<p>Our recent performance optimizations revealed a fundamental architectural challenge: traditional tokenizer boundaries don't align with performance-optimized data loading.</p>"},{"location":"blog/refactor-tokenizers/#current-performance-achievements","title":"Current Performance Achievements","text":"<ul> <li>Throughput: ~260 batches/sec (8,200+ cells/sec) for both Geneformer and scGPT</li> <li>Architecture: Lance batch-based processing with vectorized tokenization</li> <li>Bottleneck: Device transfer overhead (resolved by device-agnostic design)</li> </ul>"},{"location":"blog/refactor-tokenizers/#current-architecture-problems","title":"Current Architecture Problems","text":"<ol> <li>SLAFTokenizer Existential Crisis: Traditional tokenizer is bypassed in the fast path</li> <li>Responsibility Split: Tokenization logic split between BatchLoader (upstream) and DataLoader (downstream)</li> <li>Confusing Naming: \"Batch\" means different things in different contexts</li> <li>Limited Extensibility: Adding new tokenization strategies requires changes in multiple places</li> </ol>"},{"location":"blog/refactor-tokenizers/#performance-vs-architecture-trade-off","title":"Performance vs Architecture Trade-off","text":"<pre><code>Current Fast Path:\nBatchLoader.load_batch_chunk()\n  \u2192 RawBatch(gene_sequences, cell_integer_ids)\n    \u2192 DataLoader._tokenize_batch_vectorized()\n      \u2192 torch.Tensor\n\nCurrent Slow Path:\nSLAFTokenizer.tokenize_scgpt_sql()\n  \u2192 SQL queries\n    \u2192 list[list[int]]\n</code></pre> <p>The Challenge: How do we maintain performance while providing a clean, extensible interface?</p>"},{"location":"blog/refactor-tokenizers/#proposed-solution-strategy-based-architecture","title":"Proposed Solution: Strategy-Based Architecture","text":""},{"location":"blog/refactor-tokenizers/#core-philosophy","title":"Core Philosophy","text":"<ol> <li>Performance First: Vectorized tokenization stays in DataLoader</li> <li>Clean Separation: Each component has a single, well-defined responsibility</li> <li>Extensible Design: Easy to add new tokenization strategies</li> <li>Backward Compatibility: SQL methods remain available for legacy use</li> </ol>"},{"location":"blog/refactor-tokenizers/#new-architecture","title":"New Architecture","text":"<pre><code>PrefetchBatchProcessor.load_prefetch_batch()\n  \u2192 Window.apply() (scGPT/Geneformer specific)\n    \u2192 Shuffle.apply() (sequential/random/stratified)\n      \u2192 PrefetchBatch(gene_sequences, cell_integer_ids)\n        \u2192 DataLoader._tokenize_batch_vectorized()\n          \u2192 torch.Tensor\n</code></pre>"},{"location":"blog/refactor-tokenizers/#component-responsibilities","title":"Component Responsibilities","text":""},{"location":"blog/refactor-tokenizers/#1-window-functions-slafmlaggregatorspy","title":"1. Window Functions (<code>slaf/ml/aggregators.py</code>)","text":"<pre><code>class Window:\n    \"\"\"Defines how to apply window functions to fragment data\"\"\"\n    def apply(self, fragment_df: pl.DataFrame, max_genes: int) -&gt; pl.DataFrame:\n        raise NotImplementedError\n\nclass ScGPTWindow(Window):\n    \"\"\"scGPT: ROW_NUMBER() with expression values for binning\"\"\"\n\nclass GeneformerWindow(Window):\n    \"\"\"Geneformer: RANK() with optional percentile filtering\"\"\"\n</code></pre>"},{"location":"blog/refactor-tokenizers/#2-shuffling-strategies-slafmlsamplerspy","title":"2. Shuffling Strategies (<code>slaf/ml/samplers.py</code>)","text":"<pre><code>class Shuffle:\n    \"\"\"Defines how to shuffle cells within fragments\"\"\"\n    def apply(self, cell_ids: list[int], seed: int) -&gt; list[int]:\n        raise NotImplementedError\n\nclass SequentialShuffle(Shuffle):\n    \"\"\"Sequential loading (current implementation)\"\"\"\n\nclass RandomShuffle(Shuffle):\n    \"\"\"Random shuffling across fragments\"\"\"\n</code></pre>"},{"location":"blog/refactor-tokenizers/#3-slaftokenizer-refined-role","title":"3. SLAFTokenizer (Refined Role)","text":"<pre><code>class SLAFTokenizer:\n    \"\"\"Tokenization strategy interface - defines HOW to tokenize, not WHEN\"\"\"\n\n    def tokenize_gene_sequences_vectorized(self, gene_sequences: list[list[int]], max_genes: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Vectorized tokenization - used by DataLoader\"\"\"\n\n    def tokenize_fragment_sql(self, cell_range: tuple[int, int], max_genes: int) -&gt; list[list[int]]:\n        \"\"\"SQL-based tokenization - for legacy compatibility\"\"\"\n\n    def get_vocab_info(self) -&gt; dict:\n        \"\"\"Vocabulary management\"\"\"\n</code></pre>"},{"location":"blog/refactor-tokenizers/#4-prefetchbatchprocessor","title":"4. PrefetchBatchProcessor","text":"<pre><code>class PrefetchBatchProcessor:\n    \"\"\"Processes Lance fragments into prefetch batches\"\"\"\n\n    def __init__(self, window: Window, shuffle: Shuffle):\n        self.window = window\n        self.shuffle = shuffle\n\n    def load_prefetch_batch(self) -&gt; PrefetchBatch:\n        # Load Lance fragments\n        # Apply window.apply()\n        # Apply shuffle.apply()\n        # Return PrefetchBatch\n</code></pre>"},{"location":"blog/refactor-tokenizers/#naming-convention","title":"Naming Convention","text":"Current Proposed Rationale <code>BatchLoader</code> <code>PrefetchBatchProcessor</code> Clear about processing prefetch batches <code>RawBatch</code> <code>PrefetchBatch</code> Distinguishes from training batches <code>AsyncBatchPrefetcher</code> <code>AsyncPrefetcher</code> Generic, agnostic about content <code>WindowFunctionStrategy</code> <code>Window</code> Simple, clear interface <code>ShufflingStrategy</code> <code>Shuffle</code> Simple, clear interface"},{"location":"blog/refactor-tokenizers/#terminology-clarification","title":"Terminology Clarification","text":"<ul> <li>Lance Fragment: Physical storage unit (~380MB each)</li> <li>Prefetch Batch: Chunk of Arrow batches loaded by Lance's to_batches() generator</li> <li>Training Batch: PyTorch batch for training (32 cells, padded tensors)</li> </ul>"},{"location":"blog/refactor-tokenizers/#implementation-plan-phased-approach","title":"Implementation Plan: Phased Approach","text":""},{"location":"blog/refactor-tokenizers/#phase-1-foundation-20-minutes","title":"Phase 1: Foundation (20 minutes)","text":"<p>Goal: Create new modules and interfaces without breaking existing functionality</p>"},{"location":"blog/refactor-tokenizers/#tasks","title":"Tasks:","text":"<ol> <li> <p>Create <code>slaf/ml/aggregators.py</code></p> </li> <li> <p><code>Window</code> base class</p> </li> <li><code>ScGPTWindow</code> implementation</li> <li> <p><code>GeneformerWindow</code> implementation</p> </li> <li> <p>Create <code>slaf/ml/samplers.py</code></p> </li> <li> <p><code>Shuffle</code> base class</p> </li> <li><code>SequentialShuffle</code> implementation</li> <li> <p><code>RandomShuffle</code> implementation (future)</p> </li> <li> <p>Update <code>slaf/ml/datasets.py</code></p> </li> <li> <p>Rename <code>BatchLoader</code> \u2192 <code>PrefetchBatchProcessor</code></p> </li> <li>Rename <code>RawBatch</code> \u2192 <code>PrefetchBatch</code></li> <li>Rename <code>AsyncBatchPrefetcher</code> \u2192 <code>AsyncPrefetcher</code></li> <li> <p>Integrate with new <code>Window</code> and <code>Shuffle</code> interfaces</p> </li> <li> <p>Add comprehensive tests</p> </li> <li>Test each window function independently</li> <li>Test each shuffle strategy independently</li> <li>Test integration with existing DataLoader</li> </ol>"},{"location":"blog/refactor-tokenizers/#success-criteria","title":"Success Criteria:","text":"<ul> <li>All existing functionality works unchanged</li> <li>New interfaces are well-tested</li> <li>Performance is maintained or improved</li> </ul>"},{"location":"blog/refactor-tokenizers/#phase-2-integrate-new-tokenizer-interface-completed","title":"Phase 2: Integrate New Tokenizer Interface \u2705 COMPLETED","text":"<p>Duration: ~20 minutes</p> <p>Tasks:</p> <ul> <li> Add <code>tokenize()</code> method to <code>SLAFTokenizer</code> for vectorized tokenization</li> <li> Update <code>SLAFDataLoader</code> to use new architecture by default with <code>sql_fallback=False</code></li> <li> Update <code>datasets.py</code> to use new <code>tokenizer.tokenize()</code> method</li> <li> Add tokenizer type selection by updating factory functions</li> <li> Update tests to use new interface</li> </ul> <p>Key Changes:</p> <ul> <li>\u2705 Added <code>TokenizerType</code> enum for clear tokenizer type specification</li> <li>\u2705 Created clean <code>SLAFTokenizer.tokenize()</code> method that handles both scGPT and Geneformer formats</li> <li>\u2705 Updated <code>SLAFDataLoader</code> to use new architecture by default with SQL fallback flag</li> <li>\u2705 Fixed Polars Series handling in tokenizer methods</li> <li>\u2705 Updated all tests to use new interface</li> <li>\u2705 All 23 PyTorch dataset tests passing</li> <li>\u2705 All 40 aggregator/sampler tests passing</li> </ul> <p>Benefits Achieved:</p> <ul> <li>Clean separation of concerns between tokenization and data loading</li> <li>Direct tokenizer type specification instead of inference</li> <li>Simplified interface with single <code>tokenize()</code> method</li> <li>Backward compatibility maintained with SQL fallback</li> <li>Comprehensive test coverage for new architecture</li> </ul>"},{"location":"blog/refactor-tokenizers/#phase-3-test-cleanup-and-organization-completed","title":"Phase 3: Test Cleanup and Organization \u2705 COMPLETED","text":"<p>Duration: ~30 minutes</p> <p>Tasks:</p> <ul> <li> Clean up <code>test_tokenizers.py</code> to only test the new tokenizer interface</li> <li> Remove old SQL-based tests from <code>test_tokenizers.py</code></li> <li> Migrate tokenizer-specific tests from <code>test_pytorch_datasets.py</code> to <code>test_tokenizers.py</code></li> <li> Update <code>test_dataloaders.py</code> to focus on new architecture</li> <li> Remove redundant tests and ensure clean separation of concerns</li> <li> Fix all test failures and ensure comprehensive coverage</li> </ul> <p>Key Changes:</p> <ul> <li>\u2705 Created clean <code>test_tokenizers.py</code> with 14 comprehensive tests for new tokenizer interface</li> <li>\u2705 Removed all old SQL-based tests and deprecated functionality tests</li> <li>\u2705 Migrated vectorized tokenization tests from <code>test_pytorch_datasets.py</code> to <code>test_tokenizers.py</code></li> <li>\u2705 Updated <code>test_dataloaders.py</code> to focus on new architecture with 15 tests</li> <li>\u2705 Fixed DataLoader to use new tokenizer interface in SQL fallback mode</li> <li>\u2705 All 48 tests passing across tokenizers, dataloaders, and pytorch datasets</li> </ul> <p>Benefits Achieved:</p> <ul> <li>Clean test organization with clear separation of concerns</li> <li>Comprehensive coverage of new tokenizer interface</li> <li>Focused dataloader tests on new architecture</li> <li>Removed redundant and deprecated test code</li> <li>All tests passing with minimal warnings</li> </ul> <p>Test Coverage Summary:</p> <ul> <li> <p><code>test_tokenizers.py</code>: 14 tests covering new tokenizer interface</p> </li> <li> <p>Tokenizer initialization and configuration</p> </li> <li>Geneformer and scGPT tokenization</li> <li>Expression binning and gene ID mapping</li> <li>Vocabulary management and token decoding</li> <li> <p>Real data integration tests</p> </li> <li> <p><code>test_dataloaders.py</code>: 15 tests covering new architecture</p> </li> <li> <p>DataLoader initialization and configuration</p> </li> <li>Geneformer and scGPT iteration</li> <li>SQL fallback mode testing</li> <li>Device detection and memory efficiency</li> <li> <p>Integration with new tokenizer interface</p> </li> <li> <p><code>test_pytorch_datasets.py</code>: 19 tests covering dataset functionality</p> </li> <li>IterableDataset initialization and iteration</li> <li>Prefetch batch processing</li> <li>Async prefetcher functionality</li> <li>Window and shuffle strategy integration</li> <li>PyTorch DataLoader integration</li> </ul> <p>Architecture Benefits:</p> <ol> <li>Clean Separation: Tokenizer tests focus only on tokenization, dataloader tests focus on data loading</li> <li>Comprehensive Coverage: All major functionality is tested with real and mock data</li> <li>Maintainable Tests: Clear test organization makes it easy to add new features</li> <li>Performance Preserved: All tests pass while maintaining the performance optimizations</li> <li>Backward Compatibility: SQL fallback mode is tested and functional</li> </ol>"},{"location":"blog/refactor-tokenizers/#phase-4-sql-fallback-removal-and-cleanup-completed","title":"Phase 4: SQL Fallback Removal and Cleanup \u2705 COMPLETED","text":"<p>Duration: ~15 minutes</p> <p>Tasks:</p> <ul> <li> Remove <code>sql_fallback</code> parameter from <code>SLAFDataLoader</code></li> <li> Remove all SQL fallback logic and conditional code paths</li> <li> Remove <code>_use_sql_fallback</code>, <code>_use_iterable_dataset</code>, and <code>cell_integer_ranges</code> attributes</li> <li> Remove <code>_get_cell_integer_ranges()</code> method</li> <li> Simplify availability checks and error handling</li> <li> Update tests to remove SQL fallback references</li> <li> Ensure all tests pass with new simplified architecture</li> </ul> <p>Key Changes:</p> <ul> <li>\u2705 Removed <code>sql_fallback</code> parameter from <code>SLAFDataLoader.__init__()</code></li> <li>\u2705 Removed all SQL fallback conditional logic and code paths</li> <li>\u2705 Removed legacy attributes: <code>_use_sql_fallback</code>, <code>_use_iterable_dataset</code>, <code>cell_integer_ranges</code></li> <li>\u2705 Removed <code>_get_cell_integer_ranges()</code> method</li> <li>\u2705 Simplified availability checks with clear error messages</li> <li>\u2705 Updated all tests to remove SQL fallback references</li> <li>\u2705 All 47 tests passing across tokenizers, dataloaders, and pytorch datasets</li> </ul> <p>Benefits Achieved:</p> <ul> <li>Simplified Architecture: Single code path using only the new optimized architecture</li> <li>Reduced Complexity: Removed ~50 lines of conditional logic and fallback code</li> <li>Cleaner Interface: Removed confusing <code>sql_fallback</code> parameter</li> <li>Better Error Handling: Clear error messages when dependencies are missing</li> <li>Maintained Performance: All performance optimizations preserved</li> <li>Comprehensive Testing: All tests updated and passing</li> </ul> <p>Architecture Summary:</p> <p>The final architecture is now clean and focused:</p> <pre><code>SLAFDataLoader\n  \u2192 SLAFIterableDataset (always)\n    \u2192 PrefetchBatchProcessor\n      \u2192 Window.apply() (scGPT/Geneformer specific)\n        \u2192 Shuffle.apply() (sequential/random)\n          \u2192 PrefetchBatch(gene_sequences, cell_integer_ids)\n            \u2192 SLAFTokenizer.tokenize()\n              \u2192 torch.Tensor\n</code></pre> <p>Legacy Support:</p> <ul> <li>All SQL-based methods are preserved in <code>tokenizers_sql.py</code> for backward compatibility</li> <li>Users can still import and use SQL-based tokenization if needed</li> <li>No breaking changes for existing code that uses SQL methods directly</li> </ul>"},{"location":"blog/refactor-tokenizers/#phase-5-performance-optimization-and-final-validation-completed","title":"Phase 5: Performance Optimization and Final Validation \u2705 COMPLETED","text":"<p>Duration: ~30 minutes</p> <p>Tasks:</p> <ul> <li> Identify and fix performance regression in new architecture</li> <li> Implement simple tokenization method matching original test performance</li> <li> Optimize loading and windowing bottlenecks</li> <li> Validate performance parity with original test</li> <li> Ensure device-agnostic design with CPU tensors</li> <li> Final performance validation and documentation</li> </ul> <p>Key Achievements:</p> <ul> <li>\u2705 Performance Restored: Achieved 268.7 batches/sec (8,363.5 cells/sec) for scGPT</li> <li>\u2705 Performance Parity: Matched original test performance (~260 batches/sec)</li> <li>\u2705 Simple Tokenization: Implemented <code>_tokenize_batch_simple()</code> matching original test approach</li> <li>\u2705 Device-Agnostic Design: Confirmed CPU tensor output for training flexibility</li> <li>\u2705 Clean Architecture: Maintained all architectural improvements while preserving performance</li> <li>\u2705 Comprehensive Testing: All tests passing with performance validation</li> </ul> <p>Performance Results:</p> <pre><code>scGPT Performance (New Architecture):\n- Throughput: 268.7 batches/sec\n- Throughput: 8,363.5 cells/sec\n- Device: CPU (device-agnostic design)\n- Shape: torch.Size([32, 1024])\n- \u2713 CPU tensors confirmed\n</code></pre> <p>Architecture Benefits Achieved:</p> <ol> <li>Performance Preserved: Maintained original test performance levels</li> <li>Clean Separation: Tokenizer, Window, and Shuffle components have clear responsibilities</li> <li>Extensible Design: Easy to add new tokenization strategies and window functions</li> <li>Device Flexibility: CPU tensors allow training loop to handle device transfer</li> <li>Backward Compatibility: SQL methods preserved in <code>tokenizers_sql.py</code></li> <li>Comprehensive Testing: 47 tests covering all functionality</li> </ol> <p>Final Architecture:</p> <pre><code>SLAFDataLoader (268.7 batches/sec)\n  \u2192 SLAFIterableDataset\n    \u2192 PrefetchBatchProcessor\n      \u2192 Window.apply() (scGPT/Geneformer specific)\n        \u2192 Shuffle.apply() (sequential/random)\n          \u2192 PrefetchBatch(gene_sequences, cell_integer_ids)\n            \u2192 _tokenize_batch_simple() (optimized)\n              \u2192 torch.Tensor (CPU, device-agnostic)\n</code></pre> <p>Success Metrics Achieved:</p> <ul> <li>\u2705 Performance: \u2265250 batches/sec (achieved 268.7)</li> <li>\u2705 Architecture: Clean separation of concerns</li> <li>\u2705 Extensibility: Easy to add new tokenization strategies</li> <li>\u2705 Testing: 100% coverage of new interfaces</li> <li>\u2705 Compatibility: No breaking changes for existing users</li> </ul>"},{"location":"blog/refactor-tokenizers/#phase-5-advanced-features-20-minutes","title":"Phase 5: Advanced Features (20 minutes)","text":"<p>Goal: Add advanced features and optimizations</p>"},{"location":"blog/refactor-tokenizers/#tasks_1","title":"Tasks:","text":"<ol> <li> <p>Add advanced shuffling strategies (nice to have)</p> </li> <li> <p><code>StratifiedShuffle</code> for cell type balance</p> </li> <li><code>WeightedShuffle</code> for expression-based sampling</li> <li> <p><code>CrossValidationShuffle</code> for CV folds</p> </li> <li> <p>Add advanced window functions (nice to have)</p> </li> <li> <p><code>PercentileWindow</code> for expression percentile filtering</p> </li> <li><code>BinnedWindow</code> for expression binning</li> <li> <p><code>CustomWindow</code> for user-defined functions</p> </li> <li> <p>Performance optimizations (nice to have)</p> </li> <li> <p>Parallel window function processing</p> </li> <li>Memory-efficient shuffling</li> <li> <p>GPU-accelerated tokenization (future)</p> </li> <li> <p>Monitoring and debugging (nice to have)</p> </li> <li>Add performance metrics</li> <li>Add debugging utilities</li> <li>Add visualization tools</li> </ol>"},{"location":"blog/refactor-tokenizers/#success-criteria_1","title":"Success Criteria:","text":"<ul> <li>Advanced features work seamlessly</li> <li>Performance is optimal</li> <li>Monitoring provides insights</li> <li>Easy to debug and optimize</li> </ul>"},{"location":"blog/refactor-tokenizers/#phase-4-cleanup-and-documentation-20-minutes","title":"Phase 4: Cleanup and Documentation (20 minutes)","text":"<p>Goal: Finalize architecture and comprehensive documentation</p>"},{"location":"blog/refactor-tokenizers/#tasks_2","title":"Tasks:","text":"<ol> <li> <p>Remove deprecated code (must have)</p> </li> <li> <p>Remove old fragment-based methods</p> </li> <li>Remove unused SQL methods (if no longer needed)</li> <li> <p>Clean up imports and dependencies</p> </li> <li> <p>Comprehensive documentation (must have)</p> </li> <li> <p>API documentation for all new interfaces</p> </li> <li>Architecture diagrams</li> <li>Performance benchmarks</li> <li> <p>Migration guides</p> </li> <li> <p>Example implementations (must have)</p> </li> <li> <p>Custom window function examples</p> </li> <li>Custom shuffle strategy examples</li> <li> <p>Integration examples with other frameworks</p> </li> <li> <p>Final testing and validation (must have)</p> </li> <li>End-to-end testing</li> <li>Performance validation</li> <li>Backward compatibility verification</li> </ol>"},{"location":"blog/refactor-tokenizers/#success-criteria_2","title":"Success Criteria:","text":"<ul> <li>Clean, maintainable codebase</li> <li>Comprehensive documentation</li> <li>All tests pass</li> <li>Performance targets met</li> </ul>"},{"location":"blog/refactor-tokenizers/#benefits-of-this-approach","title":"Benefits of This Approach","text":""},{"location":"blog/refactor-tokenizers/#1-performance-preserved","title":"1. Performance Preserved","text":"<ul> <li>Vectorized tokenization stays in DataLoader</li> <li>No device transfer overhead</li> <li>Optimized fragment processing</li> </ul>"},{"location":"blog/refactor-tokenizers/#2-clean-architecture","title":"2. Clean Architecture","text":"<ul> <li>Clear separation of concerns</li> <li>Single responsibility principle</li> <li>Extensible design patterns</li> </ul>"},{"location":"blog/refactor-tokenizers/#3-extensibility","title":"3. Extensibility","text":"<ul> <li>Easy to add new window functions</li> <li>Easy to add new shuffling strategies</li> <li>Easy to add new tokenization methods</li> </ul>"},{"location":"blog/refactor-tokenizers/#4-backward-compatibility","title":"4. Backward Compatibility","text":"<ul> <li>SQL methods remain available</li> <li>Existing code continues to work</li> <li>Gradual migration path</li> </ul>"},{"location":"blog/refactor-tokenizers/#5-developer-experience","title":"5. Developer Experience","text":"<ul> <li>Clear interfaces and naming</li> <li>Comprehensive documentation</li> <li>Easy to understand and extend</li> </ul>"},{"location":"blog/refactor-tokenizers/#risk-assessment-mitigation","title":"Risk Assessment &amp; Mitigation","text":""},{"location":"blog/refactor-tokenizers/#high-risk-items","title":"High Risk Items:","text":"<ol> <li>Performance Regression: Mitigated by maintaining vectorized tokenization</li> <li>Breaking Changes: Mitigated by backward compatibility and gradual migration</li> <li>Interface Complexity: Mitigated by clear documentation and examples</li> </ol>"},{"location":"blog/refactor-tokenizers/#medium-risk-items","title":"Medium Risk Items:","text":"<ol> <li>Integration Complexity: Mitigated by phased approach</li> <li>Testing Coverage: Mitigated by comprehensive test suite</li> <li>Documentation Gap: Mitigated by dedicated documentation phase</li> </ol>"},{"location":"blog/refactor-tokenizers/#low-risk-items","title":"Low Risk Items:","text":"<ol> <li>Naming Confusion: Mitigated by clear terminology</li> <li>Code Duplication: Mitigated by interface-based design</li> </ol>"},{"location":"blog/refactor-tokenizers/#success-metrics","title":"Success Metrics","text":""},{"location":"blog/refactor-tokenizers/#performance-metrics","title":"Performance Metrics:","text":"<ul> <li>Maintain \u2265250 batches/sec throughput</li> <li>Maintain \u22658,000 cells/sec throughput</li> <li>Zero performance regression</li> </ul>"},{"location":"blog/refactor-tokenizers/#architecture-metrics","title":"Architecture Metrics:","text":"<ul> <li>100% test coverage for new interfaces</li> <li>Zero breaking changes for existing users</li> <li>Clean separation of concerns</li> </ul>"},{"location":"blog/refactor-tokenizers/#developer-experience-metrics","title":"Developer Experience Metrics:","text":"<ul> <li>Clear documentation for all interfaces</li> <li>Easy-to-follow migration guide</li> <li>Comprehensive examples</li> </ul>"},{"location":"blog/refactor-tokenizers/#next-steps","title":"Next Steps","text":"<ol> <li>Review and approve this plan</li> <li>Begin Phase 1 implementation</li> <li>Set up monitoring and testing infrastructure</li> <li>Establish performance baselines</li> </ol> <p>This refactoring will transform SLAF from a performance-optimized but architecturally complex system into a clean, extensible, and maintainable framework while preserving all performance gains.</p>"},{"location":"development/benchmarks/","title":"SLAF Benchmark System","text":"<p>This document describes the SLAF benchmark suite for performance testing and documentation generation. The benchmark system has been refactored to separate bioinformatics and ML benchmarks.</p>"},{"location":"development/benchmarks/#quick-start-recommended","title":"\ud83d\ude80 Quick Start (Recommended)","text":""},{"location":"development/benchmarks/#bioinformatics-benchmarks-cli-integration","title":"Bioinformatics Benchmarks (CLI Integration)","text":"<p>Use the unified CLI interface for bioinformatics benchmark operations:</p> <pre><code># Run bioinformatics benchmarks\nslaf benchmark run --datasets pbmc3k_processed --types cell_filtering,expression_queries --verbose\n\n# Generate summary from results\nslaf benchmark summary --results comprehensive_benchmark_results.json\n\n# Update documentation\nslaf benchmark docs --summary benchmark_summary.json\n\n# Run complete workflow\nslaf benchmark all --datasets pbmc3k_processed --auto-convert\n</code></pre>"},{"location":"development/benchmarks/#ml-benchmarks-standalone-scripts","title":"ML Benchmarks (Standalone Scripts)","text":"<p>ML benchmarks are run as standalone scripts:</p> <pre><code># External dataloader comparisons\npython benchmarks/benchmark_dataloaders_external.py\n\n# Internal tokenization strategies\npython benchmarks/benchmark_dataloaders_internal.py\n\n# Prefetcher performance analysis\npython benchmarks/benchmark_prefetcher.py\n</code></pre>"},{"location":"development/benchmarks/#file-structure","title":"\ud83d\udcc1 File Structure","text":""},{"location":"development/benchmarks/#core-files","title":"Core Files","text":"<ul> <li><code>benchmarks/benchmark.py</code> - Main bioinformatics benchmark runner with CLI integration</li> <li><code>benchmarks/benchmark_utils.py</code> - Shared utilities for bioinformatics benchmarks</li> </ul>"},{"location":"development/benchmarks/#bioinformatics-benchmark-modules-cli-integrated","title":"Bioinformatics Benchmark Modules (CLI Integrated)","text":"<ul> <li><code>benchmarks/benchmark_cell_filtering.py</code> - Cell filtering performance tests</li> <li><code>benchmarks/benchmark_gene_filtering.py</code> - Gene filtering performance tests</li> <li><code>benchmarks/benchmark_expression_queries.py</code> - Expression query performance tests</li> <li><code>benchmarks/benchmark_anndata_ops.py</code> - AnnData operation performance tests</li> <li><code>benchmarks/benchmark_scanpy_preprocessing.py</code> - Scanpy preprocessing performance tests</li> </ul>"},{"location":"development/benchmarks/#ml-benchmark-modules-standalone","title":"ML Benchmark Modules (Standalone)","text":"<ul> <li><code>benchmarks/benchmark_dataloaders_external.py</code> - External dataloader comparisons (SLAF vs scDataset, BioNeMo, etc.)</li> <li><code>benchmarks/benchmark_dataloaders_internal.py</code> - Internal tokenization strategy comparisons (scGPT, Geneformer, etc.)</li> <li><code>benchmarks/benchmark_prefetcher.py</code> - Prefetcher pipeline performance analysis</li> </ul>"},{"location":"development/benchmarks/#output-files","title":"Output Files","text":"<ul> <li><code>benchmarks/comprehensive_benchmark_results.json</code> - Complete bioinformatics benchmark results</li> <li><code>benchmarks/benchmark_summary.json</code> - Documentation-ready summary</li> <li><code>benchmarks/benchmark_output.txt</code> - Detailed benchmark output</li> <li><code>benchmarks/benchmark_results.json</code> - Legacy results file</li> </ul>"},{"location":"development/benchmarks/#cli-commands-bioinformatics-only","title":"\ud83d\udd27 CLI Commands (Bioinformatics Only)","text":""},{"location":"development/benchmarks/#run-benchmarks","title":"Run Benchmarks","text":"<pre><code># Run all bioinformatics benchmark types\nslaf benchmark run --datasets pbmc3k_processed --auto-convert\n\n# Run specific benchmark types\nslaf benchmark run --datasets pbmc3k_processed --types cell_filtering,expression_queries\n\n# Run with verbose output\nslaf benchmark run --datasets pbmc3k_processed --verbose --auto-convert\n\n# Run on multiple datasets\nslaf benchmark run --datasets pbmc3k_processed pbmc_68k --auto-convert\n</code></pre>"},{"location":"development/benchmarks/#generate-summary","title":"Generate Summary","text":"<pre><code># Generate summary from existing results\nslaf benchmark summary --results comprehensive_benchmark_results.json\n\n# Generate summary with custom output\nslaf benchmark summary --results comprehensive_benchmark_results.json --output custom_summary.json\n</code></pre>"},{"location":"development/benchmarks/#update-documentation","title":"Update Documentation","text":"<pre><code># Update bioinformatics_benchmarks.md with summary data\nslaf benchmark docs --summary benchmark_summary.json\n\n# Update with custom summary file\nslaf benchmark docs --summary custom_summary.json\n</code></pre>"},{"location":"development/benchmarks/#complete-workflow","title":"Complete Workflow","text":"<pre><code># Run benchmarks, generate summary, and update docs\nslaf benchmark all --datasets pbmc3k_processed --auto-convert --verbose\n</code></pre>"},{"location":"development/benchmarks/#available-benchmark-types","title":"\ud83d\udcca Available Benchmark Types","text":""},{"location":"development/benchmarks/#bioinformatics-benchmarks-cli-integrated","title":"Bioinformatics Benchmarks (CLI Integrated)","text":"<ul> <li>cell_filtering - Metadata-based cell filtering performance</li> <li>gene_filtering - Metadata-based gene filtering performance</li> <li>expression_queries - Expression matrix slicing performance</li> <li>anndata_ops - AnnData operation performance</li> <li>scanpy_preprocessing - Scanpy preprocessing pipeline performance</li> </ul>"},{"location":"development/benchmarks/#ml-benchmarks-standalone-scripts_1","title":"ML Benchmarks (Standalone Scripts)","text":"<ul> <li>External Dataloader Comparisons - SLAF vs scDataset, BioNeMo SCDL, AnnDataLoader</li> <li>Internal Tokenization Strategies - scGPT, Geneformer, raw data loading</li> <li>Prefetcher Performance - Pipeline timing analysis across configurations</li> </ul>"},{"location":"development/benchmarks/#usage-examples","title":"\ud83c\udfaf Usage Examples","text":""},{"location":"development/benchmarks/#bioinformatics-development-workflow","title":"Bioinformatics Development Workflow","text":"<pre><code># Quick test of cell filtering\nslaf benchmark run --datasets pbmc3k_processed --types cell_filtering --verbose\n\n# Comprehensive testing\nslaf benchmark all --datasets pbmc3k_processed --auto-convert --verbose\n</code></pre>"},{"location":"development/benchmarks/#ml-development-workflow","title":"ML Development Workflow","text":"<pre><code># Compare against external dataloaders\npython benchmarks/benchmark_dataloaders_external.py\n\n# Test different tokenization strategies\npython benchmarks/benchmark_dataloaders_internal.py\n\n# Analyze prefetcher performance\npython benchmarks/benchmark_prefetcher.py\n</code></pre>"},{"location":"development/benchmarks/#performance-analysis","title":"Performance Analysis","text":"<pre><code># Generate bioinformatics performance summary\nslaf benchmark summary --results comprehensive_benchmark_results.json\n\n# Update bioinformatics documentation with latest results\nslaf benchmark docs --summary benchmark_summary.json\n</code></pre>"},{"location":"development/benchmarks/#multi-dataset-testing","title":"Multi-Dataset Testing","text":"<pre><code># Test bioinformatics benchmarks on multiple datasets\nslaf benchmark run --datasets pbmc3k_processed pbmc_68k --types cell_filtering,expression_queries --auto-convert\n</code></pre>"},{"location":"development/benchmarks/#output-files_1","title":"\ud83d\udcc8 Output Files","text":""},{"location":"development/benchmarks/#bioinformatics-results-files","title":"Bioinformatics Results Files","text":"<ul> <li><code>comprehensive_benchmark_results.json</code> - Complete benchmark results with detailed timing and memory data</li> <li><code>benchmark_summary.json</code> - Condensed summary for documentation updates</li> <li><code>benchmark_output.txt</code> - Human-readable benchmark output with tables and analysis</li> </ul>"},{"location":"development/benchmarks/#ml-results-files","title":"ML Results Files","text":"<ul> <li>ML benchmarks output results directly to console with rich formatting</li> <li>Results are not automatically saved to files (manual documentation updates required)</li> </ul>"},{"location":"development/benchmarks/#documentation-integration","title":"Documentation Integration","text":"<p>The bioinformatics benchmark system automatically updates <code>docs/benchmarks/bioinformatics_benchmarks.md</code> with the latest performance data, ensuring documentation stays current with benchmark results. ML benchmarks are documented separately in <code>docs/benchmarks/ml_benchmarks.md</code> and require manual updates.</p>"},{"location":"development/benchmarks/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":""},{"location":"development/benchmarks/#common-issues","title":"Common Issues","text":"<ol> <li>Dataset not found: Ensure datasets are in the correct directory and use <code>--auto-convert</code> to convert h5ad files</li> <li>Benchmark failures: Check that SLAF files exist and are properly formatted</li> <li>Memory issues: Some benchmarks require significant memory for large datasets</li> <li>ML benchmark dependencies: Ensure all ML dependencies are installed for standalone ML benchmarks</li> </ol>"},{"location":"development/benchmarks/#debug-mode","title":"Debug Mode","text":"<pre><code># Run bioinformatics benchmarks with verbose output for debugging\nslaf benchmark run --datasets pbmc3k_processed --types cell_filtering --verbose\n\n# Run ML benchmarks with debug output\npython benchmarks/benchmark_dataloaders_external.py --debug\n</code></pre>"},{"location":"development/benchmarks/#contributing","title":"\ud83d\udcdd Contributing","text":""},{"location":"development/benchmarks/#adding-bioinformatics-benchmarks","title":"Adding Bioinformatics Benchmarks","text":"<p>When adding new bioinformatics benchmarks:</p> <ol> <li>Create a new benchmark module following the existing pattern</li> <li>Add the benchmark type to the CLI in <code>slaf/cli.py</code></li> <li>Update this documentation with the new benchmark type</li> <li>Test with <code>slaf benchmark run --types your_new_benchmark</code></li> </ol>"},{"location":"development/benchmarks/#adding-ml-benchmarks","title":"Adding ML Benchmarks","text":"<p>When adding new ML benchmarks:</p> <ol> <li>Create a new standalone benchmark script following the existing pattern</li> <li>Add appropriate documentation in <code>docs/benchmarks/ml_benchmarks.md</code></li> <li>Test the standalone script directly</li> <li>Consider integration with CLI system in the future</li> </ol>"},{"location":"development/benchmarks/#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<p>The benchmark system uses a modular design with two distinct approaches:</p>"},{"location":"development/benchmarks/#bioinformatics-benchmarks-cli-integrated_1","title":"Bioinformatics Benchmarks (CLI Integrated)","text":"<ul> <li>CLI Interface: Unified command-line interface in <code>slaf/cli.py</code></li> <li>Benchmark Runner: Main orchestration in <code>benchmarks/benchmark.py</code></li> <li>Individual Modules: Specialized benchmark tests in separate files</li> <li>Utilities: Shared functions in <code>benchmarks/benchmark_utils.py</code></li> <li>Documentation: Automatic updates to <code>docs/benchmarks/bioinformatics_benchmarks.md</code></li> </ul>"},{"location":"development/benchmarks/#ml-benchmarks-standalone","title":"ML Benchmarks (Standalone)","text":"<ul> <li>Standalone Scripts: Independent benchmark scripts with rich console output</li> <li>External Comparisons: <code>benchmark_dataloaders_external.py</code> for competitor analysis</li> <li>Internal Analysis: <code>benchmark_dataloaders_internal.py</code> for tokenization strategies</li> <li>Pipeline Analysis: <code>benchmark_prefetcher.py</code> for prefetcher performance</li> <li>Documentation: Manual updates to <code>docs/benchmarks/ml_benchmarks.md</code></li> </ul>"},{"location":"development/benchmarks/#future-integration","title":"\ud83d\udd04 Future Integration","text":"<p>The ML benchmarks are currently standalone but may be integrated with the CLI system in the future to provide:</p> <ul> <li>Unified benchmark execution</li> <li>Automatic result aggregation</li> <li>Integrated documentation updates</li> <li>Consistent output formatting</li> </ul> <p>For now, ML benchmarks provide immediate value as standalone tools for development and performance analysis.</p>"},{"location":"development/contributing/","title":"Contributing to SLAF","text":"<p>Thank you for your interest in contributing to SLAF! This guide will help you get started.</p>"},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ - SLAF requires Python 3.10 or higher</li> <li>Git - For version control</li> <li>uv - For dependency management (recommended)</li> </ul>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<pre><code># 1. Fork the repository on GitHub\n# Go to https://github.com/slaf-project/slaf and click \"Fork\"\n\n# 2. Clone your fork\ngit clone https://github.com/YOUR_USERNAME/slaf.git\ncd slaf\n\n# 3. Add upstream remote\ngit remote add upstream https://github.com/slaf-project/slaf.git\n\n# 4. Install development dependencies\nuv pip install -e \".[dev,test,docs]\"\n\n# 5. Install pre-commit hooks (runs linting/formatting automatically)\nuv run pre-commit install\n\n# 6. Run tests to verify setup\npytest tests/\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Follow the existing code style (enforced by pre-commit hooks)</li> <li>Add tests for new functionality</li> <li>Update documentation as needed</li> <li>Add type hints to new functions</li> <li>For API changes, follow the docstring template</li> </ul>"},{"location":"development/contributing/#3-commit-your-changes","title":"3. Commit Your Changes","text":"<pre><code>git add .\ngit commit -m \"feat: add your feature description\"\n</code></pre>"},{"location":"development/contributing/#4-push-and-create-a-pull-request","title":"4. Push and Create a Pull Request","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a pull request on GitHub.</p>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#python","title":"Python","text":"<ul> <li>Follow PEP 8 style guidelines</li> <li>Use type hints for all function parameters and return values</li> <li>Write docstrings using Google style</li> <li>Keep functions focused and small</li> <li>Use meaningful variable names</li> </ul>"},{"location":"development/contributing/#testing","title":"Testing","text":"<pre><code># Run all tests\nuv run pytest tests/\n\n# Run specific test file\nuv run pytest tests/test_slaf.py\n\n# Run with coverage\nuv run pytest pytest --cov=slaf tests/\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#building-documentation","title":"Building Documentation","text":"<pre><code># Serve docs locally for development\nslaf docs --serve\n\n# Build docs for testing\nslaf docs --build\n</code></pre>"},{"location":"development/contributing/#working-with-examples","title":"Working with Examples","text":"<p>Our examples are written in Marimo notebooks. Marimo provides an excellent interactive environment for data science and machine learning workflows.</p>"},{"location":"development/contributing/#interactive-development","title":"Interactive Development","text":"<pre><code># Edit examples interactively\ncd examples\nmarimo edit\n\n# Run a specific example\nmarimo edit examples/01-getting-started.py\n</code></pre>"},{"location":"development/contributing/#exporting-examples-for-documentation","title":"Exporting Examples for Documentation","text":"<p>After editing examples, export them to HTML for the documentation:</p> <pre><code># Export all examples to HTML\nslaf examples --export\n\n# Export a specific example\nmarimo export html examples/01-getting-started.py -o examples/01-getting-started.html\n\n# List available examples\nslaf examples --list\n</code></pre>"},{"location":"development/contributing/#programmatic-export","title":"Programmatic Export","text":"<p>You can also export notebooks programmatically:</p> <pre><code>import marimo\n\n# Export notebook to HTML\nmarimo.export_html(\"examples/01-getting-started.py\", \"examples/01-getting-started.html\")\n</code></pre>"},{"location":"development/contributing/#example-structure","title":"Example Structure","text":"<p>Our examples follow a consistent structure:</p> <ul> <li>01-getting-started.py: Comprehensive introduction to SLAF</li> <li>02-lazy-processing.py: Demonstrates lazy evaluation and processing</li> <li>03-ml-training-pipeline.py: Shows ML training workflows</li> </ul>"},{"location":"development/contributing/#best-practices-for-examples","title":"Best Practices for Examples","text":"<p>For Interactive Use:</p> <ul> <li>Use descriptive cell names</li> <li>Include markdown cells for explanations</li> <li>Add progress indicators for long-running operations</li> <li>Use the variables panel to explore data</li> </ul> <p>For Documentation:</p> <ul> <li>Keep examples focused and concise</li> <li>Include clear explanations in markdown cells</li> <li>Use consistent formatting</li> <li>Test examples with different datasets</li> </ul> <p>For Export:</p> <ul> <li>Ensure all dependencies are available</li> <li>Test the exported HTML in different browsers</li> <li>Optimize for readability in static format</li> <li>Include navigation if exporting multiple notebooks</li> </ul>"},{"location":"development/contributing/#embedding-examples-in-documentation","title":"Embedding Examples in Documentation","text":"<p>To include examples in documentation:</p> <ol> <li>Export the notebooks to HTML using Marimo's built-in export</li> <li>Place the HTML files in your documentation directory</li> <li>Include them using iframes:</li> </ol> <pre><code>&lt;iframe src=\"01-getting-started.html\" width=\"100%\" height=\"800px\"&gt;&lt;/iframe&gt;\n</code></pre>"},{"location":"development/contributing/#troubleshooting-examples","title":"Troubleshooting Examples","text":"<p>Common Issues:</p> <ol> <li>Import errors: Ensure all dependencies are installed</li> <li>Data not found: Check file paths and dataset availability</li> <li>Memory issues: Use lazy evaluation for large datasets</li> <li>Export problems: Verify Marimo version and export options</li> </ol> <p>Getting Help:</p> <ul> <li>Check the Marimo documentation</li> <li>Review the SLAF API reference</li> <li>Open an issue on GitHub for specific problems</li> </ul>"},{"location":"development/contributing/#running-benchmarks","title":"Running Benchmarks","text":"<p>SLAF includes a comprehensive benchmarking system to measure performance improvements. The benchmark suite compares SLAF vs h5ad across multiple domains including cell filtering, gene filtering, expression queries, and more.</p>"},{"location":"development/contributing/#quick-start","title":"Quick Start","text":"<pre><code># Run all benchmarks on a dataset\nslaf benchmark run --datasets pbmc3k --auto-convert\n\n# Run specific benchmark types\nslaf benchmark run --datasets pbmc3k --types cell_filtering,expression_queries\n\n# Run complete workflow (benchmarks + summary + docs)\nslaf benchmark all --datasets pbmc3k --auto-convert\n</code></pre>"},{"location":"development/contributing/#available-commands","title":"Available Commands","text":"<ul> <li><code>run</code>: Run benchmarks on specified datasets</li> <li><code>summary</code>: Generate documentation summary from results</li> <li><code>docs</code>: Update performance.md with benchmark data</li> <li><code>all</code>: Run complete workflow (benchmarks + summary + docs)</li> </ul>"},{"location":"development/contributing/#benchmark-types","title":"Benchmark Types","text":"<p>Available benchmark types include:</p> <ul> <li><code>cell_filtering</code>: Metadata-only cell filtering operations</li> <li><code>gene_filtering</code>: Gene filtering and selection</li> <li><code>expression_queries</code>: Expression matrix slicing and queries</li> <li><code>scanpy_preprocessing</code>: Scanpy preprocessing pipeline operations</li> <li><code>anndata_ops</code>: Basic AnnData operations</li> </ul>"},{"location":"development/contributing/#ml-benchmarks-standalone","title":"ML Benchmarks (Standalone)","text":"<p>For ML-specific benchmarks, run the standalone scripts:</p> <pre><code># External dataloader comparisons\nuv run python benchmarks/benchmark_dataloaders_external.py\n\n# Internal tokenization strategies\nuv run python benchmarks/benchmark_dataloaders_internal.py\n\n# Prefetcher performance analysis\nuv run python benchmarks/benchmark_prefetcher.py\n</code></pre> <p>For detailed information about the benchmark system, including advanced usage, troubleshooting, and contributing new benchmarks, see the Benchmarks Guide.</p>"},{"location":"development/contributing/#cicd","title":"CI/CD","text":"<p>The project uses GitHub Actions for automated testing and deployment:</p> <ul> <li>Tests: Run on every push and pull request</li> <li>Documentation: Automatically deployed to GitHub Pages on main branch</li> <li>Coverage: Requires minimum 70% code coverage</li> <li>Security: Automated vulnerability scanning</li> </ul> <p>All checks run automatically - you don't need to run them locally unless you want to catch issues early.</p>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>\ud83d\udcd6 Documentation: Check the API Reference</li> <li>\ud83d\udcac GitHub Issues: Report bugs on GitHub</li> </ul>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing to SLAF, you agree that your contributions will be licensed under the same license as the project.</p>"},{"location":"development/docstring_template/","title":"SLAF Docstring Template","text":"<p>This document defines the standard docstring format for all SLAF classes and methods.</p>"},{"location":"development/docstring_template/#class-docstring-template","title":"Class Docstring Template","text":"<pre><code>class ClassName:\n    \"\"\"\n    Brief description of what the class does.\n\n    Longer description explaining the purpose, key features, and use cases.\n    Include important design decisions or architectural notes.\n\n    Examples:\n        &gt;&gt;&gt; # Basic instantiation\n        &gt;&gt;&gt; obj = ClassName(param1=\"value1\", param2=\"value2\")\n        &gt;&gt;&gt; print(f\"Object created: {obj}\")\n        Object created: &lt;ClassName object&gt;\n\n        &gt;&gt;&gt; # Common usage pattern\n        &gt;&gt;&gt; obj = ClassName(param1=\"value1\")\n        &gt;&gt;&gt; result = obj.method1(param2=\"value2\")\n        &gt;&gt;&gt; print(f\"Result: {result}\")\n        Result: &lt;expected output&gt;\n\n        &gt;&gt;&gt; # Advanced usage with multiple methods\n        &gt;&gt;&gt; obj = ClassName(param1=\"value1\")\n        &gt;&gt;&gt; obj.configure(option1=True, option2=False)\n        &gt;&gt;&gt; results = obj.process_data()\n        &gt;&gt;&gt; print(f\"Processed {len(results)} items\")\n        Processed 100 items\n    \"\"\"\n</code></pre>"},{"location":"development/docstring_template/#__init__-method-template","title":"<code>__init__</code> Method Template","text":"<pre><code>def __init__(self, param1: type, param2: type = default_value):\n    \"\"\"\n    Initialize the class instance.\n\n    Args:\n        param1: Description of the first parameter. Include any constraints,\n                expected format, or important notes about the parameter.\n        param2: Description of the second parameter with default value.\n                Explain when to use the default vs custom values.\n\n    Raises:\n        ValueError: When parameters are invalid or missing required data.\n        FileNotFoundError: When required files don't exist.\n        TypeError: When parameter types are incorrect.\n\n    Examples:\n        &gt;&gt;&gt; # Basic instantiation\n        &gt;&gt;&gt; obj = ClassName(\"path/to/data\")\n        &gt;&gt;&gt; print(f\"Initialized with shape: {obj.shape}\")\n        Initialized with shape: (1000, 20000)\n\n        &gt;&gt;&gt; # With custom parameters\n        &gt;&gt;&gt; obj = ClassName(\"path/to/data\", param2=\"custom_value\")\n        &gt;&gt;&gt; print(f\"Custom config: {obj.param2}\")\n        Custom config: custom_value\n\n        &gt;&gt;&gt; # Error handling\n        &gt;&gt;&gt; try:\n        ...     obj = ClassName(\"nonexistent/path\")\n        ... except FileNotFoundError as e:\n        ...     print(f\"Error: {e}\")\n        Error: SLAF config not found at nonexistent/path/config.json\n    \"\"\"\n</code></pre>"},{"location":"development/docstring_template/#method-docstring-template","title":"Method Docstring Template","text":"<pre><code>def method_name(self, param1: type, param2: type = default_value) -&gt; ReturnType:\n    \"\"\"\n    Brief description of what the method does.\n\n    Longer description explaining the method's purpose, behavior, and any\n    important implementation details or side effects.\n\n    Args:\n        param1: Description of the first parameter. Include constraints,\n                expected format, or important notes.\n        param2: Description of the second parameter with default value.\n                Explain when to use default vs custom values.\n\n    Returns:\n        Description of the return value, including its type and structure.\n        If the method returns multiple types, explain when each occurs.\n\n    Raises:\n        ValueError: When parameters are invalid or data is malformed.\n        KeyError: When required keys are missing.\n        RuntimeError: When the operation cannot be completed.\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; obj = ClassName(\"path/to/data\")\n        &gt;&gt;&gt; result = obj.method_name(\"value1\")\n        &gt;&gt;&gt; print(f\"Result: {result}\")\n        Result: &lt;expected output&gt;\n\n        &gt;&gt;&gt; # With multiple parameters\n        &gt;&gt;&gt; result = obj.method_name(\"value1\", param2=\"custom_value\")\n        &gt;&gt;&gt; print(f\"Custom result: {result}\")\n        Custom result: &lt;expected output&gt;\n\n        &gt;&gt;&gt; # Error handling\n        &gt;&gt;&gt; try:\n        ...     result = obj.method_name(\"invalid_value\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Invalid value provided\n\n        &gt;&gt;&gt; # Chaining methods\n        &gt;&gt;&gt; obj = ClassName(\"path/to/data\")\n        &gt;&gt;&gt; filtered = obj.filter_cells(cell_type=\"T cells\")\n        &gt;&gt;&gt; subset = filtered.query(\"SELECT * FROM cells LIMIT 10\")\n        &gt;&gt;&gt; print(f\"Filtered subset: {len(subset)} cells\")\n        Filtered subset: 10 cells\n    \"\"\"\n</code></pre>"},{"location":"development/docstring_template/#property-docstring-template","title":"Property Docstring Template","text":"<pre><code>@property\ndef property_name(self) -&gt; ReturnType:\n    \"\"\"\n    Brief description of the property.\n\n    Longer description explaining what the property represents,\n    when it's computed, and any important notes about its behavior.\n\n    Returns:\n        Description of the property value and its type.\n\n    Examples:\n        &gt;&gt;&gt; # Accessing the property\n        &gt;&gt;&gt; obj = ClassName(\"path/to/data\")\n        &gt;&gt;&gt; print(f\"Shape: {obj.shape}\")\n        Shape: (1000, 20000)\n\n        &gt;&gt;&gt; # Property behavior\n        &gt;&gt;&gt; obj = ClassName(\"path/to/data\")\n        &gt;&gt;&gt; print(f\"Initial shape: {obj.shape}\")\n        Initial shape: (1000, 20000)\n        &gt;&gt;&gt; # After filtering\n        &gt;&gt;&gt; obj.filter_cells(cell_type=\"T cells\")\n        &gt;&gt;&gt; print(f\"After filtering: {obj.shape}\")\n        After filtering: (250, 20000)\n    \"\"\"\n</code></pre>"},{"location":"development/docstring_template/#module-level-function-template","title":"Module-Level Function Template","text":"<pre><code>def function_name(param1: type, param2: type = default_value) -&gt; ReturnType:\n    \"\"\"\n    Brief description of what the function does.\n\n    Longer description explaining the function's purpose, behavior,\n    and any important implementation details.\n\n    Args:\n        param1: Description of the first parameter.\n        param2: Description of the second parameter with default value.\n\n    Returns:\n        Description of the return value and its type.\n\n    Raises:\n        ValueError: When parameters are invalid.\n        FileNotFoundError: When required files don't exist.\n\n    Examples:\n        &gt;&gt;&gt; # Basic usage\n        &gt;&gt;&gt; result = function_name(\"input_data\")\n        &gt;&gt;&gt; print(f\"Result: {result}\")\n        Result: &lt;expected output&gt;\n\n        &gt;&gt;&gt; # With custom parameters\n        &gt;&gt;&gt; result = function_name(\"input_data\", param2=\"custom_value\")\n        &gt;&gt;&gt; print(f\"Custom result: {result}\")\n        Custom result: &lt;expected output&gt;\n\n        &gt;&gt;&gt; # Error handling\n        &gt;&gt;&gt; try:\n        ...     result = function_name(\"invalid_input\")\n        ... except ValueError as e:\n        ...     print(f\"Error: {e}\")\n        Error: Invalid input provided\n    \"\"\"\n</code></pre>"},{"location":"development/docstring_template/#guidelines-for-examples","title":"Guidelines for Examples","text":""},{"location":"development/docstring_template/#1-instantiation-examples","title":"1. Instantiation Examples","text":"<ul> <li>Always show the basic instantiation pattern</li> <li>Include common parameter combinations</li> <li>Show error handling for invalid inputs</li> <li>Demonstrate different ways to create the object</li> </ul>"},{"location":"development/docstring_template/#2-method-usage-examples","title":"2. Method Usage Examples","text":"<ul> <li>Show the object being instantiated first</li> <li>Demonstrate the method call with realistic parameters</li> <li>Include chaining of multiple methods</li> <li>Show error handling for invalid method calls</li> </ul>"},{"location":"development/docstring_template/#3-realistic-data","title":"3. Realistic Data","text":"<ul> <li>Use realistic parameter values</li> <li>Show expected output formats</li> <li>Include common use cases and edge cases</li> <li>Demonstrate the full workflow from instantiation to result</li> </ul>"},{"location":"development/docstring_template/#4-error-handling","title":"4. Error Handling","text":"<ul> <li>Show common error scenarios</li> <li>Demonstrate proper exception handling</li> <li>Include validation examples</li> <li>Show how to debug common issues</li> </ul>"},{"location":"development/docstring_template/#google-style-docstring-format","title":"Google Style Docstring Format","text":"<p>All docstrings should follow Google style format:</p> <pre><code>def function_name(param1: str, param2: int = 10) -&gt; bool:\n    \"\"\"\n    Brief description.\n\n    Longer description with more details about the function's behavior,\n    implementation details, and important notes.\n\n    Args:\n        param1: Description of the first parameter.\n        param2: Description of the second parameter with default value.\n\n    Returns:\n        Description of the return value.\n\n    Raises:\n        ValueError: When parameters are invalid.\n        FileNotFoundError: When required files don't exist.\n\n    Examples:\n        &gt;&gt;&gt; # Example usage\n        &gt;&gt;&gt; result = function_name(\"test\", 5)\n        &gt;&gt;&gt; print(result)\n        True\n    \"\"\"\n</code></pre>"},{"location":"development/docstring_template/#implementation-checklist","title":"Implementation Checklist","text":"<p>When updating docstrings, ensure:</p> <ul> <li> Class docstrings include purpose, features, and examples</li> <li> <code>__init__</code> methods document all parameters with types</li> <li> All methods include Args, Returns, Raises sections</li> <li> Examples show instantiation + method usage</li> <li> Error handling examples are included</li> <li> Realistic parameter values and expected outputs</li> <li> Google style format is followed consistently</li> <li> Type hints are included for all parameters and returns</li> </ul>"},{"location":"development/maintaining/","title":"For Maintainers","text":"<p>This guide covers release management, CI/CD, and maintenance tasks for SLAF maintainers.</p>"},{"location":"development/maintaining/#cicd-pipeline","title":"CI/CD Pipeline","text":"<p>The project uses GitHub Actions for automated testing and deployment:</p>"},{"location":"development/maintaining/#automated-checks","title":"Automated Checks","text":"<ul> <li>Lint and Format: Code formatting and style checks</li> <li>Type Check: Static type checking with mypy</li> <li>Test Suite: Full test suite with coverage reporting</li> <li>Security: Vulnerability scanning with bandit and safety</li> <li>Build: Package building and validation</li> <li>Integration Tests: Additional integration tests</li> </ul>"},{"location":"development/maintaining/#deployment","title":"Deployment","text":"<ul> <li>Documentation: Automatically deployed to GitHub Pages on main branch</li> <li>Coverage: Uploaded to Codecov with 70% minimum requirement</li> <li>Security Reports: Generated as artifacts</li> </ul>"},{"location":"development/maintaining/#release-management","title":"Release Management","text":""},{"location":"development/maintaining/#pypi-setup-one-time","title":"PyPI Setup (One-time)","text":"<ol> <li> <p>Create PyPI Account:</p> </li> <li> <p>Go to PyPI</p> </li> <li> <p>Create an account and verify email</p> </li> <li> <p>Create API Token:</p> </li> <li> <p>Go to PyPI API Tokens</p> </li> <li>Create token with \"Entire account\" scope</li> <li> <p>Copy the token</p> </li> <li> <p>Add to GitHub Secrets:</p> </li> <li>Go to your GitHub repo \u2192 <code>Settings</code> \u2192 <code>Secrets and variables</code> \u2192 <code>Actions</code></li> <li>Add secret: <code>PYPI_API_TOKEN</code> with your PyPI token</li> </ol>"},{"location":"development/maintaining/#release-process","title":"Release Process","text":"<p>The release process is automated through GitHub Actions:</p> <ol> <li> <p>Prepare Release:</p> </li> <li> <p>Go to <code>Actions</code> \u2192 <code>Prepare Release</code></p> </li> <li>Choose release type: <code>patch</code>, <code>minor</code>, or <code>major</code></li> <li> <p>The workflow will:</p> <ul> <li>Calculate the next version number</li> <li>Update <code>pyproject.toml</code> and <code>uv.lock</code></li> <li>Generate a changelog with recent commits</li> <li>Create and push a release branch</li> <li>Provide direct links to create a PR</li> </ul> </li> <li> <p>Create Pull Request:</p> </li> <li> <p>Click the provided link in the workflow output</p> </li> <li>Or go to: <code>https://github.com/slaf-project/slaf/compare/main...release-{VERSION}</code></li> <li> <p>Use the default title and description below</p> </li> <li> <p>Create Release Tag:</p> </li> </ol> <pre><code>git pull origin main\ngit tag v0.2.1  # Use the version from the PR\ngit push origin v0.2.1\n</code></pre> <ol> <li>Automatic Publishing:</li> <li>The release workflow automatically:<ul> <li>Builds the package</li> <li>Runs tests</li> <li>Publishes to PyPI</li> <li>Creates a GitHub release</li> </ul> </li> </ol>"},{"location":"development/maintaining/#cli-release-commands","title":"CLI Release Commands","text":"<p>For manual release management:</p> <pre><code># Prepare release (updates version and changelog)\nslaf release prepare --type patch\n\n# Run tests\nslaf release test\n\n# Build package\nslaf release build\n\n# Check package\nslaf release check\n</code></pre>"},{"location":"development/maintaining/#default-pr-title-and-message","title":"Default PR Title and Message","text":"<p>When creating a release PR, use these defaults:</p> <p>Title:</p> <pre><code>Release 0.1.1\n</code></pre> <p>Description:</p> <pre><code>## Release 0.1.1\n\nThis PR prepares the release for version 0.1.1.\n\n### Changes:\n\n- Updated version in pyproject.toml\n- Updated uv.lock\n- Generated changelog\n\n### Next steps:\n\n1. Review the changes\n2. Merge this PR\n3. Create a tag: `git tag v0.1.1`\n4. Push the tag: `git push origin v0.1.1`\n\nThe release workflow will automatically publish to PyPI when the tag is pushed.\n</code></pre>"},{"location":"development/maintaining/#package-configuration","title":"Package Configuration","text":""},{"location":"development/maintaining/#version-management","title":"Version Management","text":"<p>The package version is managed in <code>pyproject.toml</code>:</p> <pre><code>[project]\nname = \"slaf\"\nversion = \"0.2.0\"  # Updated automatically during release\n</code></pre>"},{"location":"development/maintaining/#dependencies","title":"Dependencies","text":"<p>Dependencies are organized in optional groups:</p> <pre><code>[project.optional-dependencies]\ndev = [\"pytest&gt;=8.0.0\", \"ruff==0.12.2\", \"mypy&gt;=1.8.0\", ...]\ndocs = [\"mkdocs&gt;=1.5.0\", \"mkdocs-material&gt;=9.5.0\", ...]\ntest = [\"pytest&gt;=8.0.0\", \"pytest-cov&gt;=6.2.0\", \"coverage&gt;=7.9.1\"]\n</code></pre>"},{"location":"development/maintaining/#documentation-management","title":"Documentation Management","text":""},{"location":"development/maintaining/#local-development","title":"Local Development","text":"<pre><code># Serve docs locally\nslaf docs --serve\n\n# Build docs for testing\nslaf docs --build\n</code></pre>"},{"location":"development/maintaining/#examples-management","title":"Examples Management","text":"<pre><code># List available examples\nslaf examples --list\n\n# Export all examples to HTML\nslaf examples --export\n\n# Export specific example\nslaf examples --export getting-started\n</code></pre>"},{"location":"development/maintaining/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/maintaining/#common-issues","title":"Common Issues","text":"<ol> <li>Release fails: Check PyPI API token in GitHub secrets</li> <li>Tests fail: Review coverage reports and add tests</li> <li>Build fails: Check package configuration in pyproject.toml</li> <li>Documentation fails: Verify mkdocs configuration</li> </ol>"},{"location":"development/marimo_template/","title":"Marimo Notebook AI Prompt","text":"<p>Write Python code that is compatible with marimo notebooks. Follow these requirements:</p>"},{"location":"development/marimo_template/#core-requirements","title":"Core Requirements","text":"<ol> <li>Import and setup: Always start with:</li> </ol> <pre><code>import marimo as mo\n\n__generated_with = \"0.14.0\"\napp = marimo.App(width=\"medium\")\n</code></pre> <ol> <li> <p>Cell structure: Each cell is a function named <code>def _():</code> with the <code>@app.cell</code> decorator</p> </li> <li> <p>Variable scope:</p> </li> <li>Cells using variables from other cells accept them as function arguments: <code>def _(x, y):</code></li> <li>Cells creating variables return them: <code>return x, y</code></li> <li>All variable names are unique to a single cell - you cannot reassign a variable that was created in another cell</li> </ol>"},{"location":"development/marimo_template/#key-patterns","title":"Key Patterns","text":""},{"location":"development/marimo_template/#workaround-for-readability","title":"Workaround for readability","text":"<p>Since variables can't be reused across cells, create a function inside the cell and call it:</p> <pre><code>@app.cell\ndef _(data):\n    def process_data():\n        # Use readable variable names inside the function\n        cleaned_data = data.dropna()\n        filtered_data = cleaned_data[cleaned_data &gt; 0]\n        result = filtered_data.mean()\n        return result\n\n    processed_result = process_data()\n    return processed_result,\n</code></pre>"},{"location":"development/marimo_template/#no-conditional-returns","title":"No conditional returns","text":"<p>Never use if/else to return different variables from a cell. Handle conditionals inside functions:</p> <pre><code>@app.cell\ndef _(condition):\n    def get_output():\n        if condition:\n            return \"option_a\"\n        else:\n            return \"option_b\"\n\n    output = get_output()\n    return output,\n</code></pre>"},{"location":"development/marimo_template/#additional-notes","title":"Additional Notes","text":"<ul> <li>Use marimo UI components like <code>mo.md()</code>, <code>mo.ui.slider()</code>, etc.</li> <li>Each cell should be self-contained while maintaining proper dependencies</li> <li>Always return variables that other cells need to access</li> </ul> <p>This covers the key marimo constraints that make it different from regular Python notebooks.</p>"},{"location":"development/polars-integration-phase1/","title":"Polars Integration - Phase 1","text":""},{"location":"development/polars-integration-phase1/#overview","title":"Overview","text":"<p>Phase 1 of the Polars integration adds Polars LazyFrame support alongside the existing SQL backend in SLAF. This phase focuses on:</p> <ol> <li>Adding Polars dependencies without breaking existing functionality</li> <li>Creating PolarsQueryOptimizer that reuses existing query optimization logic</li> <li>Extending SLAFArray with Polars LazyFrame support</li> <li>Adding comprehensive testing and benchmarking</li> <li>Maintaining backward compatibility with existing SQL interface</li> </ol>"},{"location":"development/polars-integration-phase1/#key-components","title":"Key Components","text":""},{"location":"development/polars-integration-phase1/#1-polarsqueryoptimizer-slafcorepolars_optimizerpy","title":"1. PolarsQueryOptimizer (<code>slaf/core/polars_optimizer.py</code>)","text":"<p>The <code>PolarsQueryOptimizer</code> class adapts the existing SQL query optimizations to Polars expressions:</p> <ul> <li>Consecutive ID Detection: Uses <code>BETWEEN</code> instead of <code>IN</code> for consecutive ranges</li> <li>Adaptive Batching: Groups scattered IDs into optimal batches</li> <li>Slice Optimization: Converts numpy-style slicing to efficient Polars filters</li> <li>Aggregation Expressions: Builds optimized Polars aggregation expressions</li> </ul> <pre><code>from slaf.core.polars_optimizer import PolarsQueryOptimizer\n\n# Build optimized filter for consecutive IDs\nfilter_expr = PolarsQueryOptimizer.build_optimized_polars_filter(\n    list(range(100, 200)), \"cell\"\n)\n\n# Build submatrix filters\ncell_filter, gene_filter = PolarsQueryOptimizer.build_submatrix_filter(\n    cell_selector=slice(0, 100),\n    gene_selector=slice(0, 5000),\n    cell_count=1000,\n    gene_count=20000\n)\n</code></pre>"},{"location":"development/polars-integration-phase1/#2-slafarray-extensions-slafcoreslafpy","title":"2. SLAFArray Extensions (<code>slaf/core/slaf.py</code>)","text":"<p>SLAFArray now includes Polars support alongside existing SQL functionality:</p> <ul> <li><code>polars_query(expression_builder)</code>: Execute Polars queries with custom expressions</li> <li><code>get_expression_lazyframe()</code>: Get Polars LazyFrame for expression data</li> <li><code>_polars_available</code>: Property indicating Polars backend availability</li> <li>Graceful fallback: Falls back to SQL-only mode if Polars setup fails</li> </ul> <pre><code># Get Polars LazyFrame\nlf = slaf_array.get_expression_lazyframe()\n\n# Apply custom operations\nfiltered_lf = lf.filter(pl.col(\"value\") &gt; 0)\nresult = filtered_lf.collect()\n</code></pre>"},{"location":"development/polars-integration-phase1/#3-performance-metrics-slafcorepolars_optimizerpy","title":"3. Performance Metrics (<code>slaf/core/polars_optimizer.py</code>)","text":"<p>The <code>PolarsPerformanceMetrics</code> class tracks and compares Polars vs SQL performance:</p> <pre><code>from slaf.core.polars_optimizer import PolarsPerformanceMetrics\n\nmetrics = PolarsPerformanceMetrics()\nmetrics.record_polars_query(\"consecutive\", 100, 0.1)\ncomparison = metrics.compare_sql_vs_polars()\n</code></pre>"},{"location":"development/polars-integration-phase1/#installation","title":"Installation","text":"<p>Add Polars to your dependencies:</p> <pre><code>pip install polars&gt;=0.22.0\n</code></pre> <p>Or update your <code>pyproject.toml</code>:</p> <pre><code>dependencies = [\n    # ... existing dependencies ...\n    \"polars&gt;=0.22.0\",  # Add Polars for lazy evaluation\n]\n</code></pre>"},{"location":"development/polars-integration-phase1/#testing","title":"Testing","text":""},{"location":"development/polars-integration-phase1/#unit-tests-teststest_polars_integrationpy","title":"Unit Tests (<code>tests/test_polars_integration.py</code>)","text":"<p>Comprehensive test suite covering:</p> <ul> <li>Polars availability detection</li> <li>Query optimizer functionality</li> <li>Submatrix filter building</li> <li>Aggregation expression building</li> <li>Performance metrics tracking</li> <li>Error handling</li> </ul>"},{"location":"development/polars-integration-phase1/#benchmarks-benchmarksbenchmark_polars_integrationpy","title":"Benchmarks (<code>benchmarks/benchmark_polars_integration.py</code>)","text":"<p>Performance benchmarks for:</p> <ul> <li>Query optimization strategies</li> <li>Submatrix filter building</li> <li>Aggregation expression building</li> <li>Memory efficiency</li> <li>Performance metrics tracking</li> </ul>"},{"location":"development/polars-integration-phase1/#examples-examples04-polars-integrationpy","title":"Examples (<code>examples/04-polars-integration.py</code>)","text":"<p>Demonstration script showing:</p> <ul> <li>Polars backend availability</li> <li>Query optimization examples</li> <li>Submatrix filter examples</li> <li>Aggregation expression examples</li> <li>SLAFArray integration examples</li> </ul>"},{"location":"development/polars-integration-phase1/#usage-examples","title":"Usage Examples","text":""},{"location":"development/polars-integration-phase1/#basic-polars-query","title":"Basic Polars Query","text":"<pre><code>from slaf.core.slaf import SLAFArray\nimport polars as pl\n\n# Load SLAF dataset\nslaf_array = SLAFArray(\"path/to/data.slaf\")\n\n# Get Polars LazyFrame\nlf = slaf_array.get_expression_lazyframe()\n\n# Apply operations\nfiltered_lf = lf.filter(pl.col(\"value\") &gt; 0)\naggregated_lf = filtered_lf.group_by(\"cell_integer_id\").agg(\n    pl.col(\"value\").sum().alias(\"total_counts\")\n)\n\n# Materialize when needed\nresult = aggregated_lf.collect()\n</code></pre>"},{"location":"development/polars-integration-phase1/#custom-polars-query","title":"Custom Polars Query","text":"<pre><code># Use polars_query for custom operations\nlf = slaf_array.polars_query(\n    lambda df: df.filter(pl.col(\"value\") &gt; 0)\n                 .group_by(\"cell_integer_id\")\n                 .agg(pl.col(\"value\").sum().alias(\"total_counts\"))\n)\nresult = lf.collect()\n</code></pre>"},{"location":"development/polars-integration-phase1/#optimized-filtering","title":"Optimized Filtering","text":"<pre><code>from slaf.core.polars_optimizer import PolarsQueryOptimizer\n\n# Build optimized filter\nfilter_expr = PolarsQueryOptimizer.build_optimized_polars_filter(\n    list(range(100, 200)), \"cell\"\n)\n\n# Apply to LazyFrame\nlf = slaf_array.get_expression_lazyframe()\nfiltered_lf = lf.filter(filter_expr)\n</code></pre>"},{"location":"development/polars-integration-phase1/#performance-considerations","title":"Performance Considerations","text":""},{"location":"development/polars-integration-phase1/#query-optimization","title":"Query Optimization","text":"<p>The PolarsQueryOptimizer preserves the performance improvements from the SQL backend:</p> <ul> <li>Consecutive ID Detection: Uses <code>BETWEEN</code> for consecutive ranges</li> <li>Adaptive Batching: Groups scattered IDs into optimal batches</li> <li>Range Query Optimization: Converts slices to efficient range filters</li> <li>Boolean Mask Handling: Converts boolean masks to index lists</li> </ul>"},{"location":"development/polars-integration-phase1/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>Lazy Evaluation: Operations are not executed until materialization</li> <li>Expression Building: Complex operations built as expressions, not materialized data</li> <li>Selective Materialization: Only materialize when explicitly requested</li> </ul>"},{"location":"development/polars-integration-phase1/#migration-path","title":"Migration Path","text":""},{"location":"development/polars-integration-phase1/#phase-1-current","title":"Phase 1 (Current)","text":"<ul> <li>\u2705 Add Polars dependencies</li> <li>\u2705 Create PolarsQueryOptimizer</li> <li>\u2705 Extend SLAFArray with Polars support</li> <li>\u2705 Add comprehensive testing</li> <li>\u2705 Maintain backward compatibility</li> </ul>"},{"location":"development/polars-integration-phase1/#phase-15-next","title":"Phase 1.5 (Next)","text":"<ul> <li>\ud83d\udd04 Add unit tests and benchmarks</li> <li>\ud83d\udd04 Test with actual SLAF data</li> <li>\ud83d\udd04 Compare performance with SQL backend</li> <li>\ud83d\udd04 Optimize based on results</li> </ul>"},{"location":"development/polars-integration-phase1/#phase-2-future","title":"Phase 2 (Future)","text":"<ul> <li>\ud83d\udd32 Switch LazyExpressionMatrix to use Polars internally</li> <li>\ud83d\udd32 Convert transformation pipeline to Polars expressions</li> <li>\ud83d\udd32 Update aggregation methods to use Polars</li> </ul>"},{"location":"development/polars-integration-phase1/#phase-3-future","title":"Phase 3 (Future)","text":"<ul> <li>\ud83d\udd32 Remove unused SQL code</li> <li>\ud83d\udd32 Add Polars-specific optimizations</li> <li>\ud83d\udd32 Consider exposing Polars interface for advanced users</li> </ul>"},{"location":"development/polars-integration-phase1/#backward-compatibility","title":"Backward Compatibility","text":"<p>All existing functionality remains unchanged:</p> <ul> <li>SQL queries: <code>slaf_array.query(\"SELECT ...\")</code> still works</li> <li>LazyAnnData interface: No changes to user-facing API</li> <li>LazyExpressionMatrix interface: No changes to user-facing API</li> <li>Scanpy integration: No changes to preprocessing functions</li> </ul>"},{"location":"development/polars-integration-phase1/#error-handling","title":"Error Handling","text":"<p>The implementation includes robust error handling:</p> <ul> <li>Graceful fallback: Falls back to SQL-only mode if Polars setup fails</li> <li>Import error handling: Skips Polars features if not available</li> <li>Performance monitoring: Tracks and reports performance issues</li> <li>Comprehensive testing: Tests both success and failure scenarios</li> </ul>"},{"location":"development/polars-integration-phase1/#next-steps","title":"Next Steps","text":"<ol> <li>Install Polars: <code>pip install polars&gt;=0.22.0</code></li> <li>Run tests: <code>pytest tests/test_polars_integration.py</code></li> <li>Run benchmarks: <code>python benchmarks/benchmark_polars_integration.py</code></li> <li>Test with real data: Use actual SLAF datasets</li> <li>Compare performance: Benchmark Polars vs SQL performance</li> <li>Proceed to Phase 2: Integrate Polars into LazyExpressionMatrix</li> </ol>"},{"location":"development/polars-integration-phase1/#contributing","title":"Contributing","text":"<p>When contributing to Phase 1:</p> <ol> <li>Maintain backward compatibility: Don't break existing SQL functionality</li> <li>Add tests: Include tests for new Polars functionality</li> <li>Update benchmarks: Add benchmarks for performance-critical code</li> <li>Document changes: Update this documentation</li> <li>Follow error handling patterns: Use graceful fallbacks</li> </ol>"},{"location":"development/polars-integration-phase1/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/polars-integration-phase1/#polars-not-available","title":"Polars Not Available","text":"<p>If you see \"Polars backend not available\" errors:</p> <ol> <li>Install Polars: <code>pip install polars&gt;=0.22.0</code></li> <li>Check installation: <code>python -c \"import polars; print(polars.__version__)\"</code></li> <li>Restart your Python environment</li> </ol>"},{"location":"development/polars-integration-phase1/#performance-issues","title":"Performance Issues","text":"<p>If Polars queries are slower than SQL:</p> <ol> <li>Check query optimization: Use <code>PolarsQueryOptimizer</code> methods</li> <li>Profile queries: Use <code>PolarsPerformanceMetrics</code></li> <li>Compare strategies: Test different optimization approaches</li> <li>Report issues: Include performance data in bug reports</li> </ol>"},{"location":"development/polars-integration-phase1/#memory-issues","title":"Memory Issues","text":"<p>If you encounter memory issues:</p> <ol> <li>Use LazyFrames: Don't materialize unnecessarily</li> <li>Monitor memory usage: Use memory profiling tools</li> <li>Optimize queries: Use efficient Polars expressions</li> <li>Consider chunking: Process data in smaller chunks</li> </ol>"},{"location":"examples/getting-started/","title":"Getting Started Examples","text":""},{"location":"examples/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the User Guide for detailed concepts</li> <li>Check the API Reference for complete documentation</li> </ul>"},{"location":"examples/lazy-processing/","title":"Lazy Processing Example","text":""},{"location":"examples/lazy-processing/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the User Guide for detailed concepts</li> <li>Check the API Reference for complete documentation</li> </ul>"},{"location":"examples/ml-training/","title":"ML Training Example","text":""},{"location":"examples/ml-training/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the User Guide for detailed concepts</li> <li>Check the API Reference for complete documentation</li> </ul>"},{"location":"examples/sql-queries/","title":"SQL Queries","text":"<p>SLAF provides native SQL querying capabilities for single-cell data, allowing you to perform complex analyses using familiar SQL syntax.</p>"},{"location":"examples/sql-queries/#database-schema","title":"Database Schema","text":"<p>SLAF stores data in three main tables:</p> <ul> <li><code>cells</code>: Cell metadata with <code>cell_id</code> (string) and <code>cell_integer_id</code> (integer)</li> <li><code>genes</code>: Gene metadata with <code>gene_id</code> (string) and <code>gene_integer_id</code> (integer)</li> <li><code>expression</code>: Sparse expression data with <code>cell_integer_id</code>, <code>gene_integer_id</code>, and <code>value</code></li> </ul> <p>The expression table uses integer IDs for efficiency, so you need to JOIN with metadata tables to get string identifiers.</p>"},{"location":"examples/sql-queries/#basic-sql-queries","title":"Basic SQL Queries","text":""},{"location":"examples/sql-queries/#simple-select-queries","title":"Simple SELECT Queries","text":"<pre><code>import slaf\n\n# Load your data\nslaf_array = slaf.SLAFArray(\"data.slaf\")\n\n# Basic cell query\nresults = slaf_array.query(\"\"\"\n    SELECT cell_id, cell_type, total_counts\n    FROM cells\n    WHERE batch = 'batch1'\n    LIMIT 10\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#gene-expression-queries","title":"Gene Expression Queries","text":"<pre><code># Query gene expression data\nresults = slaf_array.query(\"\"\"\n    SELECT\n        c.cell_id,\n        c.cell_type,\n        g.gene_id,\n        e.value\n    FROM cells c\n    JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n    WHERE g.gene_id IN ('CD3D', 'CD3E', 'CD3G')\n    AND c.cell_type = 'T cells'\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#aggregation-and-grouping","title":"Aggregation and Grouping","text":""},{"location":"examples/sql-queries/#cell-type-analysis","title":"Cell Type Analysis","text":"<pre><code># Analyze by cell type\nresults = slaf_array.query(\"\"\"\n    SELECT\n        cell_type,\n        COUNT(*) as cell_count,\n        AVG(total_counts) as avg_counts,\n        STDDEV(total_counts) as std_counts\n    FROM cells\n    GROUP BY cell_type\n    ORDER BY cell_count DESC\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#gene-expression-statistics","title":"Gene Expression Statistics","text":"<pre><code># Gene expression statistics\nresults = slaf_array.query(\"\"\"\n    SELECT\n        g.gene_id,\n        COUNT(e.value) as expressed_cells,\n        AVG(e.value) as mean_expression,\n        MAX(e.value) as max_expression\n    FROM genes g\n    JOIN expression e ON g.gene_integer_id = e.gene_integer_id\n    GROUP BY g.gene_id\n    HAVING expressed_cells &gt; 100\n    ORDER BY mean_expression DESC\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#advanced-queries","title":"Advanced Queries","text":""},{"location":"examples/sql-queries/#subqueries","title":"Subqueries","text":"<pre><code># Find cells with high expression of specific genes\nresults = slaf_array.query(\"\"\"\n    SELECT DISTINCT c.cell_id, c.cell_type\n    FROM cells c\n    WHERE c.cell_integer_id IN (\n        SELECT e.cell_integer_id\n        FROM expression e\n        JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n        WHERE g.gene_id = 'CD3D'\n        AND e.value &gt; 10\n    )\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#window-functions","title":"Window Functions","text":"<pre><code># Rank genes by expression within each cell type\nresults = slaf_array.query(\"\"\"\n    SELECT\n        c.cell_type,\n        g.gene_id,\n        e.value,\n        RANK() OVER (\n            PARTITION BY c.cell_type\n            ORDER BY e.value DESC\n        ) as rank_in_cell_type\n    FROM cells c\n    JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#ctes-common-table-expressions","title":"CTEs (Common Table Expressions)","text":"<pre><code># Using CTEs for complex analysis\nresults = slaf_array.query(\"\"\"\n    WITH cell_stats AS (\n        SELECT\n            cell_type,\n            AVG(total_counts) as avg_counts\n        FROM cells\n        GROUP BY cell_type\n    ),\n    high_expression_genes AS (\n        SELECT DISTINCT g.gene_integer_id, g.gene_id\n        FROM genes g\n        JOIN expression e ON g.gene_integer_id = e.gene_integer_id\n        WHERE e.value &gt; 5\n    )\n    SELECT\n        c.cell_type,\n        g.gene_id,\n        e.value,\n        cs.avg_counts\n    FROM cells c\n    JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n    JOIN high_expression_genes g ON e.gene_integer_id = g.gene_integer_id\n    JOIN cell_stats cs ON c.cell_type = cs.cell_type\n    WHERE e.value &gt; cs.avg_counts\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#performance-tips","title":"Performance Tips","text":""},{"location":"examples/sql-queries/#use-integer-ids-for-joins","title":"Use Integer IDs for Joins","text":"<p>For better performance, use integer IDs in JOIN conditions:</p> <pre><code># Efficient: Use integer IDs for joins\nresults = slaf_array.query(\"\"\"\n    SELECT c.cell_id, g.gene_id, e.value\n    FROM expression e\n    JOIN cells c ON e.cell_integer_id = c.cell_integer_id\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n    WHERE e.value &gt; 0\n\"\"\")\n\n# Less efficient: Using string IDs in WHERE clauses\nresults = slaf_array.query(\"\"\"\n    SELECT c.cell_id, g.gene_id, e.value\n    FROM expression e\n    JOIN cells c ON e.cell_integer_id = c.cell_integer_id\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n    WHERE c.cell_id = 'cell_001'  -- Requires string comparison\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#optimize-for-large-datasets","title":"Optimize for Large Datasets","text":"<pre><code># For large datasets, filter early and use integer IDs\nresults = slaf_array.query(\"\"\"\n    SELECT\n        c.cell_id,\n        g.gene_id,\n        e.value\n    FROM expression e\n    JOIN cells c ON e.cell_integer_id = c.cell_integer_id\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n    WHERE e.cell_integer_id BETWEEN 0 AND 1000  -- Early filtering\n    AND e.value &gt; 0\n    ORDER BY e.cell_integer_id, e.gene_integer_id\n\"\"\")\n</code></pre>"},{"location":"examples/sql-queries/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the User Guide for detailed concepts</li> <li>Check the API Reference for complete documentation</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with SLAF in minutes! Experience the joy of lightning-fast SQL queries on your single-cell data.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<p>Install SLAF using your preferred package manager:</p> <pre><code># Using uv (recommended)\nuv add slafdb\n\n# Or pip\npip install slafdb\n</code></pre> <p>For development dependencies (including documentation):</p> <pre><code># Using uv (recommended)\nuv pip install -e \".[docs,dev,test]\"\n\n# Or using pip\npip install slafdb[docs,dev,test]\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-slaf-experience","title":"Your First SLAF Experience","text":"<p>Let's simulate a single-cell dataset similar to what you'd get from a real experiment.</p>"},{"location":"getting-started/quickstart/#option-1-create-a-synthetic-dataset-recommended-for-first-time","title":"Option 1: Create a Synthetic Dataset (Recommended for First Time)","text":"<p>This creates a realistic single-cell dataset from scratch:</p> <pre><code>import numpy as np\nimport polars as pl\nimport scanpy as sc\nfrom scipy.sparse import csr_matrix\nfrom slaf.data.converter import SLAFConverter\n\n# Set random seed for reproducible results\nnp.random.seed(42)\n\n# Create a realistic single-cell dataset\nn_cells, n_genes = 1000, 2000\n\n# Create sparse matrix with realistic sparsity\ndensity = 0.1  # 10% sparsity - typical for single-cell data\nn_nonzero = int(n_cells * n_genes * density)\n\n# Generate realistic expression data (log-normal distribution)\ndata = np.random.lognormal(0, 1, n_nonzero).astype(np.float32)\nrow_indices = np.random.randint(0, n_cells, n_nonzero)\ncol_indices = np.random.randint(0, n_genes, n_nonzero)\n\n# Create sparse matrix\nX = csr_matrix((data, (row_indices, col_indices)), shape=(n_cells, n_genes))\n\n# Create cell metadata\nobs = pl.DataFrame({\n    \"cell_id\": [f\"cell_{i:04d}\" for i in range(n_cells)],\n    \"cell_type\": np.random.choice([\"T_cell\", \"B_cell\", \"NK_cell\", \"Monocyte\"], n_cells),\n    \"batch\": np.random.choice([\"batch1\", \"batch2\", \"batch3\"], n_cells),\n    \"total_counts\": X.sum(axis=1).A1,\n    \"n_genes_by_counts\": (X &gt; 0).sum(axis=1).A1,\n    \"high_mito\": np.random.choice([True, False], n_cells, p=[0.1, 0.9]),\n})\n\n# Create gene metadata\nvar = pl.DataFrame({\n    \"gene_id\": [f\"ENSG_{i:08d}\" for i in range(n_genes)],\n    \"gene_symbol\": [f\"GENE_{i:06d}\" for i in range(n_genes)],\n    \"highly_variable\": np.random.choice([True, False], n_genes, p=[0.2, 0.8]),\n    \"total_counts\": X.sum(axis=0).A1,\n    \"n_cells_by_counts\": (X &gt; 0).sum(axis=0).A1,\n})\n\n# Create AnnData object\nadata = sc.AnnData(X=X, obs=obs, var=var)\n\nprint(f\"\u2705 Created dataset: {adata.n_obs:,} cells \u00d7 {adata.n_vars:,} genes\")\nprint(f\"   Sparsity: {1 - adata.X.nnz / (adata.n_obs * adata.n_vars):.1%}\")\n</code></pre>"},{"location":"getting-started/quickstart/#option-2-use-a-real-dataset-pbmc3k","title":"Option 2: Use a Real Dataset (PBMC3K)","text":"<p>For a more authentic experience, you can use the popular PBMC3K dataset:</p> <pre><code>import scanpy as sc\nfrom slaf.data.converter import SLAFConverter\n\n# Download PBMC3K dataset (this will take a moment)\nprint(\"Downloading PBMC3K dataset...\")\nadata = sc.datasets.pbmc3k()\nadata.var_names_make_unique()\n\nprint(f\"\u2705 Downloaded PBMC3K: {adata.n_obs:,} cells \u00d7 {adata.n_vars:,} genes\")\nprint(f\"   Sparsity: {1 - adata.X.nnz / (adata.n_obs * adata.n_vars):.1%}\")\n\n# Add some basic metadata if not present\nif \"total_counts\" not in adata.obs.columns:\n    adata.obs[\"total_counts\"] = adata.X.sum(axis=1).A1\nif \"n_genes_by_counts\" not in adata.obs.columns:\n    adata.obs[\"n_genes_by_counts\"] = (adata.X &gt; 0).sum(axis=1).A1\nif \"batch\" not in adata.obs.columns:\n    adata.obs[\"batch\"] = \"pbmc3k\"\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-convert-to-slaf-format","title":"Step 2: Convert to SLAF Format","text":"<pre><code># Convert to SLAF format\nconverter = SLAFConverter()\nslaf_path = \"my_dataset.slaf\"\nconverter.convert_anndata(adata, slaf_path)\n\nprint(f\"\u2705 Converted to SLAF format: {slaf_path}\")\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-load-and-explore-your-data","title":"Step 3: Load and Explore Your Data","text":"<pre><code>import slaf\n\n# Load your SLAF dataset\nslaf_array = slaf.SLAFArray(slaf_path)\n\n# Check basic info\nprint(f\"Dataset shape: {slaf_array.shape}\")\nprint(f\"Number of cells: {slaf_array.shape[0]:,}\")\nprint(f\"Number of genes: {slaf_array.shape[1]:,}\")\n\n# View cell metadata\nprint(\"\\nCell metadata columns:\")\nprint(list(slaf_array.obs.columns))\n\n# View gene metadata\nprint(\"\\nGene metadata columns:\")\nprint(list(slaf_array.var.columns))\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-sql-queries","title":"Your First SQL Queries","text":"<p>Now experience the power of SQL on your single-cell data!</p>"},{"location":"getting-started/quickstart/#basic-queries","title":"Basic Queries","text":"<pre><code># Count cells by cell type\ncell_types = slaf_array.query(\"\"\"\n    SELECT cell_type, COUNT(*) as count\n    FROM cells\n    GROUP BY cell_type\n    ORDER BY count DESC\n\"\"\")\nprint(\"Cell type distribution:\")\nprint(cell_types)\n\n# Find cells with high total counts\nhigh_count_cells = slaf_array.query(\"\"\"\n    SELECT cell_id, cell_type, total_counts\n    FROM cells\n    WHERE total_counts &gt; 1000\n    ORDER BY total_counts DESC\n    LIMIT 5\n\"\"\")\nprint(\"\\nCells with high total counts:\")\nprint(high_count_cells)\n</code></pre>"},{"location":"getting-started/quickstart/#advanced-queries-with-joins","title":"Advanced Queries with Joins","text":"<pre><code># Find highly variable genes with their expression stats\nvariable_genes = slaf_array.query(\"\"\"\n    SELECT\n        g.gene_id,\n        g.gene_symbol,\n        g.total_counts as gene_total_counts,\n        COUNT(e.value) as cells_expressed,\n        AVG(e.value) as avg_expression,\n        MAX(e.value) as max_expression\n    FROM genes g\n    LEFT JOIN expression e ON g.gene_integer_id = e.gene_integer_id\n    WHERE g.highly_variable = true\n    GROUP BY g.gene_id, g.gene_symbol, g.total_counts\n    ORDER BY g.total_counts DESC\n    LIMIT 10\n\"\"\")\nprint(\"Top highly variable genes:\")\nprint(variable_genes)\n</code></pre>"},{"location":"getting-started/quickstart/#complex-analysis","title":"Complex Analysis","text":"<pre><code># Analyze expression patterns by cell type\ncell_type_analysis = slaf_array.query(\"\"\"\n    SELECT\n        c.cell_type,\n        COUNT(DISTINCT c.cell_id) as num_cells,\n        AVG(c.total_counts) as avg_total_counts,\n        AVG(c.n_genes_by_counts) as avg_genes_per_cell,\n        COUNT(CASE WHEN c.high_mito = true THEN 1 END) as high_mito_cells\n    FROM cells c\n    GROUP BY c.cell_type\n    ORDER BY num_cells DESC\n\"\"\")\nprint(\"Cell type analysis:\")\nprint(cell_type_analysis)\n</code></pre>"},{"location":"getting-started/quickstart/#lazy-evaluation-with-anndata-integration","title":"Lazy Evaluation with Anndata Integration","text":"<p>SLAF works seamlessly with Scanpy for familiar workflows with lazy evaluation:</p> <pre><code>from slaf.integrations.anndata import read_slaf\n\n# Load as lazy AnnData\nadata = read_slaf(slaf_path)\nprint(f\"\u2705 Loaded: {adata.shape[0]:,} cells \u00d7 \"\n      f\"{adata.shape[1]:,} genes\")\n\n# Lazy slicing - no data loaded yet\nt_cells = adata[adata.obs.cell_type == \"T_cell\", :]\n\n# Only load data when you need it\nexpression_matrix = t_cells.X.compute()\nprint(f\"Computed expression: {expression_matrix.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#working-with-metadata-and-layers","title":"Working with Metadata and Layers","text":"<p>SLAF provides lazy views for all AnnData metadata objects, allowing you to read, modify, and create metadata efficiently:</p> <pre><code>from slaf.integrations.anndata import read_slaf\n\n# Load as lazy AnnData\nadata = read_slaf(slaf_path)\n\n# Access and modify obs columns (cell metadata)\nprint(\"Available obs columns:\", list(adata.obs.columns))\nadata.obs[\"new_cluster\"] = cluster_labels  # Create new column\nprint(f\"Cluster distribution:\\n{adata.obs['new_cluster'].value_counts()}\")\n\n# Access and modify var columns (gene metadata)\nprint(\"Available var columns:\", list(adata.var.columns))\nadata.var[\"is_highly_variable\"] = hvg_flags  # Create new column\n\n# Store multi-dimensional arrays (obsm - cell embeddings)\nimport numpy as np\numap_coords = compute_umap(adata)  # Your function returning (n_cells, 2) array\nadata.obsm[\"X_umap\"] = umap_coords\nprint(f\"UMAP shape: {adata.obsm['X_umap'].shape}\")\n\n# Store gene-level embeddings (varm)\npca_loadings = compute_pca_loadings(adata)  # Your function returning (n_genes, 50) array\nadata.varm[\"PCs\"] = pca_loadings\nprint(f\"PCA loadings shape: {adata.varm['PCs'].shape}\")\n\n# Store unstructured metadata (uns)\nadata.uns[\"neighbors\"] = {\"params\": {\"n_neighbors\": 15, \"metric\": \"euclidean\"}}\nadata.uns[\"pca\"] = {\"variance_ratio\": variance_ratios.tolist()}\nprint(\"Unstructured metadata keys:\", list(adata.uns.keys()))\n\n# Access layers (alternative expression matrices)\nif \"spliced\" in adata.layers:\n    spliced_matrix = adata.layers[\"spliced\"].compute()\n    print(f\"Spliced layer shape: {spliced_matrix.shape}\")\n\n# Create new layers\nnormalized = adata.X.copy()\n# ... apply normalization ...\nadata.layers[\"normalized\"] = normalized\nprint(\"Available layers:\", list(adata.layers.keys()))\n</code></pre> <p>Key Features:</p> <ul> <li>DataFrame-like access: <code>obs</code> and <code>var</code> behave like pandas DataFrames</li> <li>Dictionary-like interface: <code>layers</code>, <code>obsm</code>, <code>varm</code>, and <code>uns</code> use dict-like syntax</li> <li>Lazy evaluation: Metadata is accessed on-demand</li> <li>Immutability protection: Columns/keys converted from h5ad are protected from accidental deletion</li> </ul>"},{"location":"getting-started/quickstart/#lazy-scanpy-preprocessing","title":"Lazy Scanpy Preprocessing","text":"<p>SLAF provides lazy versions of scanpy preprocessing functions that work efficiently on large datasets:</p> <pre><code>from slaf.integrations import scanpy as slaf_scanpy\nfrom slaf.integrations.anndata import read_slaf\n\n# Load as lazy AnnData\nadata = read_slaf(slaf_path)\n\n# Calculate QC metrics (lazy SQL aggregation)\ncell_qc, gene_qc = slaf_scanpy.pp.calculate_qc_metrics(\n    adata, inplace=False\n)\nprint(\"Cell QC metrics:\")\nprint(cell_qc.head())\n\n# Filter cells (lazy - no data loaded)\nadata_filtered = slaf_scanpy.pp.filter_cells(\n    adata,\n    min_genes=200,\n    min_counts=1000,\n    inplace=False\n)\nprint(f\"After filtering: {adata_filtered.shape}\")\n\n# Filter genes (lazy)\nadata_filtered = slaf_scanpy.pp.filter_genes(\n    adata_filtered,\n    min_cells=30,\n    inplace=False\n)\nprint(f\"After gene filtering: {adata_filtered.shape}\")\n\n# Normalize total (lazy transformation)\nadata_norm = slaf_scanpy.pp.normalize_total(\n    adata_filtered,\n    target_sum=1e4,\n    inplace=False\n)\n\n# Apply log1p transformation (lazy)\nadata_log = slaf_scanpy.pp.log1p(\n    adata_norm, inplace=False\n)\n\n# Find highly variable genes (lazy SQL)\nhvg_stats = slaf_scanpy.pp.highly_variable_genes(\n    adata_log,\n    min_mean=0.0125,\n    max_mean=3,\n    min_disp=0.5,\n    inplace=False\n)\nprint(f\"Highly variable genes: {hvg_stats['highly_variable'].sum()}\")\n\n# All operations are lazy - compute when needed\nexpression = adata_log.X.compute()\nprint(f\"Computed expression: {expression.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#chaining-transformations","title":"Chaining Transformations","text":"<p>You can chain lazy transformations efficiently:</p> <pre><code># Build complete preprocessing pipeline\nadata_processed = slaf_scanpy.pp.normalize_total(\n    adata, target_sum=1e4, inplace=False\n)\nadata_processed = slaf_scanpy.pp.log1p(\n    adata_processed, inplace=False\n)\n\n# Slice after transformations\nsubset = adata_processed[:1000, :500]\n\n# Compute only when needed\nresult = subset.X.compute()\nprint(f\"Processed subset: {result.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#key-benefits","title":"Key Benefits","text":"<ul> <li>Memory Efficient: Operations stored as instructions</li> <li>SQL Performance: QC metrics use SQL aggregation</li> <li>Composable: Chain transformations easily</li> <li>Lazy: Compute only when you call <code>.compute()</code></li> </ul>"},{"location":"getting-started/quickstart/#efficient-filtering","title":"Efficient Filtering","text":"<p>SLAF provides optimized filtering methods:</p> <pre><code># Filter cells by metadata\nt_cells = slaf_array.filter_cells(cell_type=\"T_cell\", total_counts=\"&gt;1000\")\nprint(f\"Found {len(t_cells)} T cells with high counts\")\n\n# Filter genes\nvariable_genes = slaf_array.filter_genes(highly_variable=True)\nprint(f\"Found {len(variable_genes)} highly variable genes\")\n\n# Get expression submatrix\nexpression = slaf_array.get_submatrix(\n    cell_selector=t_cells,\n    gene_selector=variable_genes\n)\nprint(f\"Expression submatrix: {expression.shape}\")\n</code></pre>"},{"location":"getting-started/quickstart/#ml-training-with-dataloaders","title":"ML Training with Dataloaders","text":"<p>SLAF provides efficient tokenization and dataloaders for training foundation models:</p>"},{"location":"getting-started/quickstart/#tokenization","title":"Tokenization","text":"<pre><code>from slaf.ml import SLAFTokenizer\n\n# Create tokenizer for GeneFormer style tokenization\ntokenizer = SLAFTokenizer(\n    slaf_array=slaf_array,\n    tokenizer_type=\"geneformer\",\n    vocab_size=50000,\n    n_expression_bins=10\n)\n\n# Geneformer tokenization (gene sequence only)\ngene_sequences = [[1, 2, 3], [4, 5, 6]]  # Example gene IDs\ninput_ids, attention_mask = tokenizer.tokenize(\n    gene_sequences,\n    max_genes=2048\n)\n\n# Create tokenizer for scGPT style tokenization\ntokenizer = SLAFTokenizer(\n    slaf_array=slaf_array,\n    tokenizer_type=\"scgpt\",\n    vocab_size=50000,\n    n_expression_bins=10\n)\n\n# scGPT tokenization (gene-expression pairs)\ngene_sequences = [[1, 2, 3], [4, 5, 6]]  # Gene IDs\nexpr_sequences = [[0.5, 0.8, 0.2], [0.9, 0.1, 0.7]]  # Expression values\ninput_ids, attention_mask = tokenizer.tokenize(\n    gene_sequences,\n    expr_sequences=expr_sequences,\n    max_genes=1024\n)\n</code></pre>"},{"location":"getting-started/quickstart/#dataloader-for-training","title":"DataLoader for Training","text":"<pre><code>from slaf.ml import SLAFDataLoader\n\n# Create DataLoader (uses MoS by default for high entropy)\ndataloader = SLAFDataLoader(\n    slaf_array=slaf_array,\n    tokenizer_type=\"geneformer\",  # or \"scgpt\"\n    batch_size=32,\n    max_genes=2048\n)\n\n# Use with PyTorch training\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]\n    attention_mask = batch[\"attention_mask\"]\n    cell_ids = batch[\"cell_ids\"]\n\n    # Your training loop here\n    loss = model(input_ids, attention_mask=attention_mask)\n    loss.backward()\n</code></pre>"},{"location":"getting-started/quickstart/#using-the-slaf-cli","title":"Using the SLAF CLI","text":"<p>SLAF includes a powerful command-line interface for common data operations:</p>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<pre><code># Show general help\nslaf --help\n\n# Show help for specific command\nslaf convert --help\nslaf query --help\n</code></pre>"},{"location":"getting-started/quickstart/#basic-commands","title":"Basic Commands","text":"<pre><code># Check SLAF version\nslaf version\n\n# Show info about your dataset\nslaf info my_dataset.slaf\n\n# Execute a SQL query\nslaf query my_dataset.slaf \"SELECT COUNT(*) FROM cells\"\n\n# Convert other formats to SLAF\nslaf convert data.h5ad output.slaf\n</code></pre>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":"<p>Converting Datasets:</p> <pre><code># Convert an AnnData file\nslaf convert pbmc3k.h5ad pbmc3k.slaf\n\n# Convert with verbose output to see details\nslaf convert pbmc3k.h5ad pbmc3k.slaf --verbose\n</code></pre> <p>Exploring Datasets:</p> <pre><code># Get basic info about a dataset\nslaf info pbmc3k.slaf\n\n# Run a simple query\nslaf query pbmc3k.slaf \"SELECT cell_type, COUNT(*) FROM cells GROUP BY cell_type\"\n\n# Export query results to CSV\nslaf query pbmc3k.slaf \"SELECT * FROM cells WHERE cell_type = 'T cells'\" --output t_cells.csv\n</code></pre> <p>Data Analysis Pipeline:</p> <pre><code># Convert input data\nslaf convert input.h5ad output.slaf\n\n# Verify conversion\nslaf info output.slaf\n\n# Run analysis queries\nslaf query output.slaf \"SELECT cell_type, AVG(total_counts) FROM cells GROUP BY cell_type\"\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Read the User Guide for detailed concepts</li> <li>Explore Examples for real-world use cases</li> <li>Check the API Reference for complete documentation</li> <li>Report bugs or make feature requests on GitHub</li> </ul>"},{"location":"slides/intro-to-slaf/","title":"Intro to slaf","text":""},{"location":"slides/intro-to-slaf/#slaf","title":"SLAF","text":""},{"location":"slides/intro-to-slaf/#sparse-lazy-array-format","title":"Sparse Lazy Array Format","text":""},{"location":"slides/intro-to-slaf/#the-single-cell-format-for-the-virtual-cell-era","title":"The Single-Cell Format for the Virtual Cell Era","text":"<p>Github: github.com/slaf-project/slaf Docs: slaf-project.github.io/slaf/</p>"},{"location":"slides/intro-to-slaf/#the-promise-of-the-virtual-cell","title":"The Promise of the Virtual Cell","text":"Figure 1. The Virtual Cell: a unified, queryable representation of all single-cell data, enabling prediction, explanation, and discovery.      Figure 5. Virtual Cells power new applications: prediction, explanation, and discovery across tissues, modalities, and experiments.      <ul> <li>Virtual Cells unify all single-cell data into a single, queryable format.</li> <li>Enable prediction of cell states, explanation of biological phenomena, and discovery of new cell types.</li> <li>Break down barriers between datasets, tissues, and modalities.</li> </ul>"},{"location":"slides/intro-to-slaf/#single-cell-datasets-have-scaled-2000-in-5-years","title":"Single-Cell Datasets Have Scaled 2000\u00d7 in 5 Years","text":"<ul> <li>2019: Typical study: 50K cells</li> <li>2024: Modern experiment: 5M+ cells</li> <li>2025: Atlas-scale datasets: 100M+ cells</li> </ul>"},{"location":"slides/intro-to-slaf/#the-storage-crisis","title":"The Storage Crisis","text":""},{"location":"slides/intro-to-slaf/#current-formats-are-not-cloud-native","title":"Current Formats Are Not Cloud-Native","text":""},{"location":"slides/intro-to-slaf/#traditional-single-cell-workflows-hit-fundamental-bottlenecks","title":"Traditional single-cell workflows hit fundamental bottlenecks:","text":"<ul> <li>Network: Days to transfer 100M-cell datasets</li> <li>Memory: Out-of-memory errors on standard hardware</li> <li>Infrastructure: Requires dedicated engineering teams</li> <li>Duplication: Multiple copies per experiment/node</li> </ul>"},{"location":"slides/intro-to-slaf/#three-personas-three-pain-points","title":"Three Personas, Three Pain Points","text":"\ud83d\udd2c No-Code Users Scientific Insights  - Want self-service analysis on massive datasets - Blocked by infrastructure complexity   - Need interactive dashboards that scale  \ud83d\udcbb Some-Code Users Exploratory Analysis  - Building scanpy pipelines on 10M+ cell datasets - Facing OOM errors and data transfer bottlenecks - Can't do self-service without infrastructure engineers  \ud83e\udd16 ML Engineers Foundation Models  - Training transformer models on single-cell data - Copying data per node on attached storage - Need to maximize GPU utilization and experiment velocity"},{"location":"slides/intro-to-slaf/#meanwhile-other-domains-have-evolved","title":"Meanwhile, Other Domains Have Evolved","text":""},{"location":"slides/intro-to-slaf/#cloud-native-formats-revolutionized-pb-scale-data","title":"Cloud-Native Formats Revolutionized PB-Scale Data","text":"<ul> <li>Imaging: Zarr + Dask enabled distributed microscopy</li> <li>Tabular: Parquet + Polars enabled real-time analytics</li> <li>Genomics: CRAM + HTSlib enabled cloud-native sequencing</li> <li>Single-cell transcriptomics: Still stuck with 2015 technology</li> </ul>"},{"location":"slides/intro-to-slaf/#slaf-bringing-cloud-native-to-single-cell","title":"SLAF: Bringing Cloud-Native to Single-Cell","text":""},{"location":"slides/intro-to-slaf/#built-on-proven-technologies","title":"Built on Proven Technologies","text":"<ul> <li>Lance: Cloud-native columnar storage</li> <li>Polars: High-performance dataframe engine</li> <li>DuckDB: Embedded OLAP database</li> <li>Dask: Lazy computation graphs</li> </ul>"},{"location":"slides/intro-to-slaf/#slaf-combines-these-innovations-for-single-cell-data","title":"SLAF combines these innovations for single-cell data","text":""},{"location":"slides/intro-to-slaf/#slaf-architecture","title":"SLAF Architecture","text":""},{"location":"slides/intro-to-slaf/#three-core-innovations","title":"Three Core Innovations","text":""},{"location":"slides/intro-to-slaf/#1-olap-powered-sql","title":"1. OLAP-Powered SQL","text":"<p>Embedded, in-process query engines for pushdown optimization</p>"},{"location":"slides/intro-to-slaf/#2-memory-efficient","title":"2. Memory Efficient","text":"<p>Lazy evaluation - only load what you need</p>"},{"location":"slides/intro-to-slaf/#3-scanpy-compatible","title":"3. Scanpy Compatible","text":"<p>Drop-in replacement with familiar numpy-like slicing</p>"},{"location":"slides/intro-to-slaf/#slaf-not-just-storage-but-experience","title":"SLAF: Not Just Storage, But Experience","text":""},{"location":"slides/intro-to-slaf/#you-never-left-scanpy","title":"\"You Never Left Scanpy\"","text":"<pre><code>from slaf.integrations import read_slaf\n\n# Load as lazy AnnData\nadata = read_slaf(\"s3://bucket/large_dataset.slaf\")\n\n# Operations are lazy until you call .compute()\nsubset = adata[adata.obs.cell_type == \"T cells\", :]\nfirst_ten_cells = subset[:10, :]\nexpression = first_ten_cells.X.compute()  # Only now is data loaded\n</code></pre>"},{"location":"slides/intro-to-slaf/#familiar-scanpy-idioms-cloud-native-performance","title":"Familiar scanpy idioms, cloud-native performance","text":""},{"location":"slides/intro-to-slaf/#sql-powered-analytics","title":"SQL-Powered Analytics","text":""},{"location":"slides/intro-to-slaf/#leverage-pushdown-filtering","title":"Leverage Pushdown Filtering","text":"**Do this:** <pre><code>from slaf import SLAFArray\n\nslaf_array = SLAFArray(\"s3://bucket/large_dataset.slaf\")\n\nresults = slaf_array.query(\"\"\"\nSELECT cell_type, AVG(total_counts) as avg_counts\nFROM cells\nWHERE batch = \"batch1\"\nAND cell_type IN (\"T cells\", \"B cells\")\nGROUP BY cell_type\nORDER BY avg_counts DESC\n\"\"\")\n\n````\n&lt;/div&gt;\n\n&lt;div class=\"instead-of\"&gt;\n**Instead of:**\n```python\nimport scanpy as sc\n\n# Download large dataset\n!aws s3 cp s3://bucket/large_dataset.h5ad .\n\n# Load entire dataset into memory\nadata = sc.read_h5ad(\"large_dataset.h5ad\")\n\n# Filter in memory\nsubset = adata[adata.obs.batch == \"batch1\"]\nsubset = subset[subset.obs.cell_type.isin([\"T cells\", \"B cells\"])]\n\n# Aggregate in memory\nresults = subset.obs.groupby(\"cell_type\")[\"total_counts\"].mean()\n````\n\n&lt;/div&gt;\n\n&lt;/div&gt;\n\n---\n\n# Foundation Model Ready\n\n## Stream Tokenized Batches to GPU\n\n```python\nfrom slaf.ml.dataloaders import SLAFDataLoader\n\n# Create production-ready DataLoader\ndataloader = SLAFDataLoader(\n    slaf_array=slaf_array,\n    tokenizer_type=\"geneformer\",\n    batch_size=32,\n    max_genes=2048,\n    vocab_size=50000\n)\n\n# Stream batches for training\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]      # Already tokenized\n    attention_mask = batch[\"attention_mask\"]\n    cell_ids = batch[\"cell_ids\"]\n    # Your training code here\n</code></pre>  ### **No manual tokenization, no data duplication**  ---  # Performance Benchmarks  ## Metadata Queries: 10-100\u00d7 Faster  | Scenario          | Traditional (ms) | SLAF (ms) | Speedup  | | ----------------- | ---------------- | --------- | -------- | | Quality filtering | 610.6            | 4.1       | **150\u00d7** | | Batch filtering   | 264.1            | 2.0       | **130\u00d7** | | Complex queries   | 190.4            | 1.7       | **112\u00d7** |  ### **Memory efficiency: 100\u00d7 better**  ---  # Performance Benchmarks  ## Expression Reads: 1-10\u00d7 Faster  ### **Preprocessing computations: 1-10\u00d7 faster**  ### **Dataloader throughput: 6.4\u00d7 faster than naive PyTorch**  ### **Streaming to GPU: 9,496 cells/second with tokenization**  ---  # SLAF vs State-of-the-Art  ## ML Training Benchmarks  ### **SLAF DataLoader**: 9,496 cells/second  ### **Traditional PyTorch**: 1,480 cells/second  ### **Speedup**: **6.4\u00d7 faster**  ### **Enables efficient training of transformer foundation models**  ---  # Real-World Impact  ## Enabling New Workflows  ### **Atlas Builders**: Serve 100M+ cell datasets globally  ### **Tool Builders**: Build interactive dashboards on commodity hardware  ### **Foundation Model Builders**: Train models without data duplication  ### **Bioinformaticians**: Self-service analysis without infrastructure bottlenecks  ---  # Getting Started  ## Three Ways to Use SLAF  ### 1. **Convert existing data**  <pre><code>pip install slaf\nslaf convert data.h5ad data.slaf\n</code></pre>  ### 2. **Use with scanpy**  <pre><code>from slaf.integrations import read_slaf\nadata = read_slaf(\"data.slaf\")\n</code></pre>  ### 3. **Train foundation models**  <pre><code>from slaf.ml.dataloaders import SLAFDataLoader\ndataloader = SLAFDataLoader(slaf_array, tokenizer_type=\"geneformer\")\n</code></pre>  ---  # The Future of Single-Cell Data  ## SLAF Enables the Virtual Cell Era  - **Cloud-native storage** for PB-scale datasets - **Lazy computation** for memory-efficient analysis - **SQL-powered queries** for complex analytics - **Foundation model ready** dataloaders - **Scanpy compatibility** for familiar workflows  ### **Single-cell data, reimagined for the cloud era**  ---  # Thank You  ## Questions?  ### **SLAF**: The Single-Cell Data Format for the Virtual Cell Era  **Documentation**: https://slaf-project.github.io/slaf/   **GitHub**: https://github.com/slaf-project/slaf   **Blog**: https://slaf-project.github.io/slaf/blog/  ---  # Appendix: Technical Details  ## SLAF Architecture Deep Dive  ### **Storage Layer**  - Lance tables for columnar storage - Optimized compression for expression data - Metadata stored separately for fast queries  ### **Query Layer**  - DuckDB for SQL queries - Polars for dataframe operations - Pushdown optimization for performance  ### **Compute Layer**  - Dask for lazy computation graphs - Scanpy-compatible interfaces - Pre-built dataloaders for ML training"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the SLAF User Guide!</p> <p>Choose a topic below to learn more:</p> <ul> <li>How SLAF Works: Understand the main ideas behind SLAF.</li> <li>Migrating to SLAF: How to load and manage your data.</li> <li>SQL Queries: Query your data using SQL.</li> </ul> <p>Use the navigation bar above to explore more sections.</p>"},{"location":"user-guide/how-slaf-works/","title":"How SLAF Works","text":"<p>SLAF (Sparse Lazy Array Format) is a high-performance format for single-cell data that combines the power of SQL with lazy evaluation. It's designed to solve the performance and scalability challenges of traditional single-cell data formats.</p>"},{"location":"user-guide/how-slaf-works/#key-design-principles","title":"Key Design Principles","text":""},{"location":"user-guide/how-slaf-works/#1-sql-native-design-with-relational-schema","title":"1. SQL-Native Design with Relational Schema","text":"<p>SLAF is built on a SQL-native relational schema that enables direct SQL queries while providing lazy AnnData/Scanpy equivalences for seamless migration:</p>"},{"location":"user-guide/how-slaf-works/#relational-schema","title":"Relational Schema","text":"<p>SLAF stores single-cell data in core tables with an extensible schema:</p> <ul> <li><code>cells</code> table: Cell metadata, QC metrics, and annotations with <code>cell_id</code> (string) and <code>cell_integer_id</code> (integer). Also stores multi-dimensional arrays (obsm) like UMAP coordinates and PCA embeddings as <code>FixedSizeListArray</code> columns.</li> <li><code>genes</code> table: Gene metadata, annotations, and feature information with <code>gene_id</code> (string) and <code>gene_integer_id</code> (integer). Also stores multi-dimensional arrays (varm) like PCA loadings as <code>FixedSizeListArray</code> columns.</li> <li><code>expression</code> table: Sparse expression matrix with <code>cell_integer_id</code>, <code>gene_integer_id</code>, and <code>value</code> columns</li> <li><code>layers</code> table (optional): Alternative expression matrices (e.g., <code>spliced</code>, <code>unspliced</code>, <code>counts</code>) stored in wide format with one column per layer, sharing the same <code>cell_integer_id</code> and <code>gene_integer_id</code> structure as the expression table</li> </ul> <p>The expression and layers tables use integer IDs for efficiency, so you need to JOIN with metadata tables to get string identifiers. Multi-dimensional arrays (obsm/varm) are stored alongside scalar metadata in the same tables using Arrow's native <code>FixedSizeListArray</code> type for efficient vector operations.</p> <p>This relational design enables direct SQL queries for everything:</p> <pre><code># Direct SQL for complex aggregations\nresults = slaf.query(\"\"\"\n    SELECT cell_type,\n           COUNT(*) as cell_count,\n           AVG(total_counts) as avg_counts,\n           SUM(e.value) as total_expression\n    FROM cells c\n    JOIN expression e ON c.cell_integer_id = e.cell_integer_id\n    WHERE batch = 'batch1' AND n_genes_by_counts &gt;= 500\n    GROUP BY cell_type\n    ORDER BY cell_count DESC\n\"\"\")\n\n# Window functions for advanced analysis\nranked_genes = slaf.query(\"\"\"\n    SELECT g.gene_id,\n           c.cell_type,\n           e.value,\n           ROW_NUMBER() OVER (\n               PARTITION BY c.cell_type\n               ORDER BY e.value DESC\n           ) as rank\n    FROM expression e\n    JOIN cells c ON e.cell_integer_id = c.cell_integer_id\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n    WHERE g.gene_id IN ('MS4A1', 'CD3D', 'CD8A')\n\"\"\")\n</code></pre>"},{"location":"user-guide/how-slaf-works/#lazy-anndatascanpy-equivalences","title":"Lazy AnnData/Scanpy Equivalences","text":"<p>For users migrating from Scanpy, SLAF provides drop-in lazy equivalents:</p> <pre><code># Load as LazyAnnData (no data loaded yet)\nadata = read_slaf(\"data.slaf\")\n\n# Use familiar Scanpy-style operations\nsubset = adata[adata.obs.cell_type == \"T cells\", :] # This is lazy\n\n# Access expression data lazily\nexpression = subset.X.compute()  # Only loads the subset\n\n# Use Scanpy preprocessing (lazy)\nfrom slaf.scanpy import pp\npp.normalize_total(adata, target_sum=1e4, inplace=True)\npp.log1p(adata)\npp.highly_variable_genes(adata)\nexpression = adata[cell_ids, gene_ids].X.compute()\n</code></pre>"},{"location":"user-guide/how-slaf-works/#seamless-switching-between-interfaces","title":"Seamless Switching Between Interfaces","text":"<p>You can switch between SQL and AnnData interfaces as needed:</p> <pre><code># Start with AnnData interface\nlazy_adata = read_slaf(\"data.slaf\")\nt_cells = lazy_adata[lazy_adata.obs.cell_type == \"T cells\", :]\n\n# Switch to SQL for complex operations\nt_cells_slaf = t_cells.slaf  # Access underlying SLAFArray object\ncomplex_query_result = t_cells_slaf.query(\"\"\"\n    SELECT g.gene_id,\n           COUNT(*) as expressing_cells,\n           AVG(e.value) as mean_expression\n    FROM expression e\n    JOIN genes g ON e.gene_integer_id = g.gene_integer_id\n    WHERE e.cell_integer_id IN (\n        SELECT cell_integer_id FROM cells\n        WHERE cell_type = 'T cells'\n    )\n    GROUP BY g.gene_id\n    HAVING expressing_cells &gt;= 10\n    ORDER BY mean_expression DESC\n\"\"\")\n\n# Back to AnnData for visualization\nimport scanpy as sc\nt_cells_as_adata = t_cells.compute()  # Convert to native scanpy\nsc.pl.umap(t_cells_as_adata, color='leiden')\n</code></pre> <p>Benefits:</p> <ul> <li>SQL-native: Direct access to relational database capabilities</li> <li>SQL-native: Complex aggregations and window functions</li> <li>Migration-friendly: Drop-in replacement for existing Scanpy workflows</li> <li>Flexible: Switch between SQL and AnnData interfaces as needed</li> </ul>"},{"location":"user-guide/how-slaf-works/#2-polars-like-olap-database-with-pushdown-filters","title":"2. Polars-Like: OLAP Database with Pushdown Filters","text":"<p>SLAF leverages modern OLAP databases and pushdown filters on optimized storage formats rather than in-memory operations:</p> <ul> <li><code>cells</code> table: Cell metadata, QC metrics, and multi-dimensional arrays (obsm)</li> <li><code>genes</code> table: Gene metadata, annotations, and multi-dimensional arrays (varm)</li> <li><code>expression</code> table: Sparse expression matrix data</li> <li><code>layers</code> table: Alternative expression matrices (optional, wide format)</li> </ul> <p>Like Polars, SLAF pushes complex operations down to the query engine:</p> <pre><code># Metadata-only filtering without loading expression data\nfiltered_cells = slaf.filter_cells(n_genes_by_counts=\"&gt;=500\")\nhigh_quality = slaf.filter_cells(\n    n_genes_by_counts=\"&gt;=1000\",\n    pct_counts_mt=\"&lt;=10\"\n)\n</code></pre> <p>Benefits:</p> <ul> <li>Memory efficient: Only load metadata when filtering</li> <li>Faster metadata filtering vs h5ad as datasets scale</li> </ul>"},{"location":"user-guide/how-slaf-works/#3-zarr-like-lazy-loading-of-sparse-matrices","title":"3. Zarr-Like: Lazy Loading of Sparse Matrices","text":"<p>SLAF provides lazy loading of sparse matrices from cloud storage with concurrent access patterns:</p> <pre><code># No data loaded yet - just metadata\nadata = read_slaf(\"data.slaf\")\n\n# Lazy slicing like Zarr chunked arrays\nsubset = adata[adata.obs.cell_type == \"T cells\", :]\nsingle_cell = adata.get_cell_expression(\"AAACCTGAGAAACCAT-1\")\ngene_expression = adata.get_gene_expression(\"MS4A1\")\n\n# Data loaded only when .compute() is called\nexpression = subset.X.compute()\n</code></pre> <p>Benefits:</p> <ul> <li>Memory efficient for submatrix operations</li> <li>Concurrent access: Multiple readers can access different slices</li> <li>Cloud-native: Direct access to data in S3/GCS without downloading</li> <li>Chunked processing: Handle datasets larger than RAM</li> </ul>"},{"location":"user-guide/how-slaf-works/#4-dask-like-lazy-computation-graphs","title":"4. Dask-Like: Lazy Computation Graphs","text":"<p>SLAF enables building computational graphs of operations that execute lazily on demand:</p> <pre><code># Build lazy computation graph\nadata = LazyAnnData(\"data.slaf\")\n\n# Each operation is lazy - no data loaded yet\npp.calculate_qc_metrics(adata, inplace=True)\npp.filter_cells(adata, min_counts=500, min_genes=200, inplace=True)\npp.normalize_total(adata, target_sum=1e4, inplace=True)\npp.log1p(adata)\n\n# Execute only on the slice of interest\nexpression = adata.X[cell_ids, gene_ids].compute()\n</code></pre> <p>Benefits:</p> <ul> <li>Complex pipelines: Build preprocessing workflows impossible with eager processing</li> <li>Composable: Chain operations without intermediate materialization</li> <li>Memory efficient: Only process the slice you need</li> <li>Scalable: Handle datasets that would cause memory explosions</li> </ul>"},{"location":"user-guide/how-slaf-works/#5-advanced-query-optimization","title":"5. Advanced Query Optimization","text":"<p>SLAF includes sophisticated query optimization to overcome current limitations of LanceDB:</p> <pre><code># Adaptive batching for large scattered ID sets\nbatched_query = QueryOptimizer.build_optimized_query(\n    entity_ids=large_id_list,\n    entity_type=\"cell\",\n    use_adaptive_batching=True\n)\n\n# CTE optimization for complex queries\ncte_query = QueryOptimizer.build_cte_query(\n    entity_ids=scattered_ids,\n    entity_type=\"gene\"\n)\n</code></pre> <p>Key optimizations:</p> <ul> <li>Submatrix optimization: Efficient slicing for complex selectors</li> <li>Adaptive batching: Optimize query patterns based on ID distribution</li> <li>Range vs IN optimization: Choose BETWEEN vs IN clauses intelligently</li> </ul>"},{"location":"user-guide/how-slaf-works/#6-foundation-model-training-support","title":"6. Foundation Model Training Support","text":"<p>SLAF's versatile SQL combined with OLAP-optimized query engine enables window function queries that directly support tokenization and streaming dataloaders:</p> <pre><code># Streaming tokenization for transformer models\nfrom slaf.ml.dataloaders import SLAFDataLoader\n\n# Create production-ready DataLoader\ndataloader = SLAFDataLoader(\n    slaf_array=slaf_array,\n    tokenizer_type=\"geneformer\",\n    batch_size=32,\n    max_genes=2048,\n    vocab_size=50000\n)\n\n# Stream batches for training\nfor batch in dataloader:\n    input_ids = batch[\"input_ids\"]      # Already tokenized\n    attention_mask = batch[\"attention_mask\"]\n    cell_ids = batch[\"cell_ids\"]\n    # Your training code here\n\n# High-throughput dataloading\n# 15k cells/sec peak throughput\n# 30M tokens/sec for large batches\n</code></pre> <p>Benefits:</p> <ul> <li>Streaming architecture: Supports asynchronous pre-fetching and concurrent streaming</li> <li>GPU-optimized: Batch sizes up to 2048 cells with high GPU utilization</li> <li>Multi-node ready: Shard-aware streaming for distributed training</li> <li>Foundation model support: Direct integration with scGPT, Geneformer, etc.</li> </ul>"},{"location":"user-guide/how-slaf-works/#comparison-with-other-formats","title":"Comparison with Other Formats","text":"Feature SLAF H5AD Zarr TileDB SOMA Storage Cloud-Native \u2705 \u274c \u2705 \u2705 Sparse Arrays \u2705 \u2705 \u274c \u2705 Chunked Reads \u2705 \u274c \u2705 \u2705 Schema Evolution \u2705 \u274c \u2705 \u2705 Compute SQL Queries \u2705 \u274c \u274c \u2705 Optimized Query Engine \u2705 \u274c \u274c \u2705 Random Access \u2705 \u2705 \u2705 \u2705 Use Cases Scanpy Integration \u2705 \u2705 \u2705 \u274c Lazy Computation \u2705 \u274c \u274c \u274c Tokenizers \u2705 \u274c \u274c \u274c Dataloaders \u2705 \u274c \u274c \u274c Embeddings Support \ud83d\udd04 \u274c \u274c \u274c Visualization Support \ud83d\udd04 \u2705 \u274c \u274c <p>Legend:</p> <ul> <li>\u2705 = Supported</li> <li>\u274c = Not supported</li> <li>\ud83d\udd04 = Coming soon</li> </ul>"},{"location":"user-guide/how-slaf-works/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Migrating to SLAF</li> <li>Explore SQL Queries</li> <li>See Examples for real-world usage</li> </ul>"},{"location":"user-guide/migrating-to-slaf/","title":"Migrating to SLAF","text":""},{"location":"user-guide/migrating-to-slaf/#quick-start","title":"Quick Start","text":"<p>Convert your single-cell data to SLAF format with just one command:</p> <pre><code># Convert any supported format (auto-detection)\nslaf convert data.h5ad output.slaf\nslaf convert filtered_feature_bc_matrix/ output.slaf\nslaf convert data.h5 output.slaf\nslaf convert data.tiledb output.slaf\n</code></pre> <p>That's it! SLAF automatically detects your file format and converts it with optimized settings.</p>"},{"location":"user-guide/migrating-to-slaf/#multi-file-conversion","title":"Multi-File Conversion","text":"<p>Convert multiple files to a single SLAF dataset:</p> <pre><code># Convert multiple files from a directory\nslaf convert data_folder/ output.slaf\n\n# Convert specific files\nslaf convert file1.h5ad file2.h5ad file3.h5ad output.slaf\n\n# Auto-detection works for all formats\nslaf convert 10x_data_folder/ output.slaf\n</code></pre> <p>SLAF automatically:</p> <ul> <li>\u2705 Validates all files are compatible</li> <li>\u2705 Assigns unique cell IDs across all files</li> <li>\u2705 Tracks which file each cell came from</li> <li>\u2705 Combines metadata intelligently</li> </ul>"},{"location":"user-guide/migrating-to-slaf/#appending-to-existing-datasets","title":"Appending to Existing Datasets","text":"<p>Add new data to an existing SLAF dataset:</p> <pre><code># Append a single file\nslaf append new_data.h5ad existing.slaf\n\n# Append multiple files from a directory\nslaf append new_data_folder/ existing.slaf\n\n# Skip validation if already validated (faster)\nslaf append new_data.h5ad existing.slaf --skip-validation\n</code></pre> <p>Perfect for:</p> <ul> <li>Incremental data collection - Add new batches as they arrive</li> <li>Data updates - Append new samples to existing datasets</li> <li>Combining datasets - Merge related datasets over time</li> </ul>"},{"location":"user-guide/migrating-to-slaf/#supported-formats","title":"Supported Formats","text":"<p>SLAF supports conversion from these common single-cell formats:</p> <ul> <li>AnnData (.h5ad files) - The standard single-cell format</li> <li>10x MTX (filtered_feature_bc_matrix directories) - Cell Ranger output</li> <li>10x H5 (.h5 files) - Cell Ranger H5 output format</li> <li>TileDB SOMA (.tiledb directories) - High-performance single-cell format</li> </ul>"},{"location":"user-guide/migrating-to-slaf/#python-api","title":"Python API","text":"<pre><code>from slaf.data import SLAFConverter\n\n# Basic conversion (auto-detection)\nconverter = SLAFConverter()\nconverter.convert(\"data.h5ad\", \"output.slaf\")\nconverter.convert(\"filtered_feature_bc_matrix/\", \"output.slaf\")\nconverter.convert(\"data.h5\", \"output.slaf\")\nconverter.convert(\"data.tiledb\", \"output.slaf\")\n\n# Multi-file conversion\nconverter.convert(\"data_folder/\", \"output.slaf\")  # Directory of files\nconverter.convert([\"file1.h5ad\", \"file2.h5ad\"], \"output.slaf\")  # List of files\n\n# Append to existing dataset\nconverter.append(\"new_data.h5ad\", \"existing.slaf\")\nconverter.append(\"new_data_folder/\", \"existing.slaf\")\n\n# Convert existing AnnData object\nimport scanpy as sc\nadata = sc.read_h5ad(\"data.h5ad\")\nconverter.convert_anndata(adata, \"output.slaf\")\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#large-datasets","title":"Large Datasets","text":"<p>For large datasets (&gt;100k cells), you can optimize performance:</p> <pre><code># Use larger chunks for speed (if you have enough RAM)\nslaf convert large_data.h5ad output.slaf --chunk-size 100000\n\n# Create indices for faster queries\nslaf convert large_data.h5ad output.slaf --create-indices\n\n# Control fragment size (important for HuggingFace uploads)\n# Default: 100M rows per fragment (stays under 10k file limit)\nslaf convert large_data.h5ad output.slaf --max-rows-per-file 100000000\n</code></pre> <pre><code># Python API for large datasets\nconverter = SLAFConverter(\n    chunk_size=100000,\n    create_indices=True,\n    max_rows_per_file=100_000_000,  # Default: 100M (stays under HuggingFace's 10k limit)\n)\nconverter.convert(\"large_data.h5ad\", \"output.slaf\")\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#validation-and-quality-control","title":"Validation and Quality Control","text":"<p>The <code>slaf validate-input-files</code> command helps you catch compatibility issues before conversion:</p>"},{"location":"user-guide/migrating-to-slaf/#basic-validation","title":"Basic Validation","text":"<pre><code># Validate a single file\nslaf validate-input-files data.h5ad\n\n# Validate multiple files from a directory\nslaf validate-input-files data_folder/\n\n# Validate specific files\nslaf validate-input-files file1.h5ad file2.h5ad file3.h5ad\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#verbose-output","title":"Verbose Output","text":"<pre><code># Get detailed information about files being validated\nslaf validate-input-files data_folder/ --verbose\n\n# Output shows:\n# \ud83d\udcc1 Found 3 h5ad files\n#   1. batch_001.h5ad\n#   2. batch_002.h5ad\n#   3. batch_003.h5ad\n# \u2705 All files are compatible for conversion\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#format-specific-validation","title":"Format-Specific Validation","text":"<pre><code># Validate 10x MTX directories\nslaf validate-input-files filtered_feature_bc_matrix/ --format 10x_mtx\n\n# Validate 10x H5 files\nslaf validate-input-files data.h5 --format 10x_h5\n\n# Validate TileDB SOMA files\nslaf validate-input-files experiment.tiledb --format tiledb\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#what-validation-checks","title":"What Validation Checks","text":"<p>The validation command performs comprehensive compatibility checks:</p> <ul> <li>\u2705 File Integrity: Files exist, are readable, and not empty</li> <li>\u2705 Format Consistency: All files use the same format (h5ad, 10x_mtx, etc.)</li> <li>\u2705 Gene Compatibility: All files have identical gene sets</li> <li>\u2705 Metadata Schema: Cell metadata columns are compatible across files</li> <li>\u2705 Value Types: Expression data types are consistent (uint16, float32, etc.)</li> <li>\u2705 File Sizes: Ensures files contain actual data (not empty)</li> </ul>"},{"location":"user-guide/migrating-to-slaf/#common-validation-scenarios","title":"Common Validation Scenarios","text":"<pre><code># Validate before multi-file conversion\nslaf validate-input-files batch1/ batch2/ batch3/\nslaf convert batch1/ output.slaf  # Safe to proceed\n\n# Validate before appending\nslaf validate-input-files new_batch/\nslaf append existing.slaf new_batch/  # Safe to proceed\n\n# Check specific format compatibility\nslaf validate-input-files 10x_data/ --format 10x_mtx\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#error-examples","title":"Error Examples","text":"<p>When validation fails, you get clear error messages:</p> <pre><code># Gene mismatch error\n\u274c Validation failed: File batch_002.h5ad is incompatible:\n  Missing genes: GENE_001, GENE_002, GENE_003\n  Extra genes: GENE_999, GENE_1000\n\n# Schema mismatch error\n\u274c Validation failed: File batch_003.h5ad has incompatible cell metadata schema:\n  Missing columns: ['cell_type', 'batch']\n  Extra columns: ['cluster_id']\n\n# Format mismatch error\n\u274c Validation failed: Multiple formats detected in directory\n  Found: h5ad, 10x_mtx\n  All files must use the same format\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#integration-with-conversion","title":"Integration with Conversion","text":"<p>Validation runs automatically during conversion, but you can skip it for performance:</p> <pre><code># Automatic validation (default)\nslaf convert data_folder/ output.slaf\n\n# Skip validation (faster, but less safe)\nslaf convert data_folder/ output.slaf --skip-validation\nslaf append new_data.h5ad existing.slaf --skip-validation\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#advanced-options","title":"Advanced Options","text":"<p>Most users won't need these, but they're available if needed:</p>"},{"location":"user-guide/migrating-to-slaf/#cli-options","title":"CLI Options","text":"<pre><code># Specify format explicitly (if auto-detection fails)\nslaf convert data.h5 output.slaf --format 10x_h5\nslaf convert data.tiledb output.slaf --format tiledb\n\n# Use non-chunked processing (not recommended for large datasets)\nslaf convert small_data.h5ad output.slaf --no-chunked\n\n# Disable storage optimization (larger files but includes string IDs)\nslaf convert data.h5ad output.slaf --no-optimize-storage\n\n# Verbose output\nslaf convert data.h5ad output.slaf --verbose\n\n# Skip validation (if already validated)\nslaf convert data_folder/ output.slaf --skip-validation\nslaf append new_data.h5ad existing.slaf --skip-validation\n\n# Control fragment size (for HuggingFace uploads or large datasets)\n# Default: 100M rows per fragment (stays under HuggingFace's 10k file limit)\nslaf convert large_data.h5ad output.slaf --max-rows-per-file 200000000\n\n# TileDB-specific options\nslaf convert data.tiledb output.slaf --tiledb-collection RNA\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#python-api-options","title":"Python API Options","text":"<pre><code># Custom settings\nconverter = SLAFConverter(\n    chunk_size=50000,           # Cells per chunk\n    create_indices=True,        # Faster queries\n    optimize_storage=True,      # Smaller files (default)\n    use_optimized_dtypes=True,  # Better compression (default)\n    tiledb_collection_name=\"RNA\",  # TileDB collection name (default: \"RNA\")\n    max_rows_per_file=100_000_000,  # Max rows per fragment (default: 100M)\n                                    # Increase for fewer fragments (e.g., for HuggingFace)\n)\n\nconverter.convert(\"data.h5ad\", \"output.slaf\")\nconverter.convert(\"data.tiledb\", \"output.slaf\")\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#tiledb-soma-conversion","title":"TileDB SOMA Conversion","text":"<p>SLAF provides excellent support for TileDB SOMA format, which is increasingly popular for large-scale single-cell datasets:</p>"},{"location":"user-guide/migrating-to-slaf/#basic-tiledb-conversion","title":"Basic TileDB Conversion","text":"<pre><code># Auto-detect TileDB format\nslaf convert experiment.tiledb output.slaf\n\n# Specify collection name (default: \"RNA\")\nslaf convert experiment.tiledb output.slaf --tiledb-collection RNA\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#python-api-for-tiledb","title":"Python API for TileDB","text":"<pre><code>from slaf.data import SLAFConverter\n\n# Basic TileDB conversion\nconverter = SLAFConverter()\nconverter.convert(\"experiment.tiledb\", \"output.slaf\")\n\n# With custom collection name\nconverter = SLAFConverter(tiledb_collection_name=\"RNA\")\nconverter.convert(\"experiment.tiledb\", \"output.slaf\")\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#tiledb-benefits","title":"TileDB Benefits","text":"<ul> <li>Memory Efficient: TileDB's chunked storage works seamlessly with SLAF's chunked processing</li> <li>Large Datasets: Optimized for datasets with millions of cells</li> <li>Data Preservation: Maintains exact floating-point precision from TileDB</li> <li>Fast Conversion: Leverages TileDB's efficient data access patterns</li> </ul>"},{"location":"user-guide/migrating-to-slaf/#what-slaf-does","title":"What SLAF Does","text":"<p>SLAF converts your data to an optimized format that:</p> <ul> <li>Enables fast SQL queries on your data</li> <li>Works with any size dataset (memory-efficient processing)</li> <li>Preserves all metadata including:</li> <li>Cell and gene annotations (<code>obs</code> and <code>var</code> columns)</li> <li>Alternative expression matrices (<code>layers</code> like <code>spliced</code>, <code>unspliced</code>, <code>counts</code>)</li> <li>Multi-dimensional arrays (<code>obsm</code> like UMAP coordinates, PCA embeddings)</li> <li>Gene-level embeddings (<code>varm</code> like PCA loadings)</li> <li>Unstructured metadata (<code>uns</code> like analysis parameters)</li> </ul>"},{"location":"user-guide/migrating-to-slaf/#workflow-examples","title":"Workflow Examples","text":""},{"location":"user-guide/migrating-to-slaf/#multi-file-workflow","title":"Multi-File Workflow","text":"<pre><code># 1. Validate files first (recommended)\nslaf validate-input-files batch1/ batch2/ batch3/\n\n# 2. Convert all batches to single SLAF\nslaf convert batch1/ initial.slaf\n\n# 3. Append additional batches\nslaf append batch2/ initial.slaf\nslaf append batch3/ initial.slaf\n\n# 4. Explore the combined dataset\nslaf info initial.slaf\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#incremental-data-collection","title":"Incremental Data Collection","text":"<pre><code># Start with first batch\nslaf convert batch_001/ dataset.slaf\n\n# Add new batches as they arrive\nslaf append batch_002/ dataset.slaf\nslaf append batch_003/ dataset.slaf\nslaf append batch_004/ dataset.slaf\n\n# Each append maintains data integrity and source tracking\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#quality-control-workflow","title":"Quality Control Workflow","text":"<pre><code># 1. Validate all files before conversion\nslaf validate-input-files all_batches/\n\n# 2. Convert with validation (automatic)\nslaf convert all_batches/ combined.slaf\n\n# 3. Check source file tracking\nslaf query combined.slaf \"SELECT source_file, COUNT(*) FROM cells GROUP BY source_file\"\n</code></pre>"},{"location":"user-guide/migrating-to-slaf/#next-steps","title":"Next Steps","text":"<p>After converting your data:</p> <ol> <li>Explore: <code>slaf info output.slaf</code></li> <li>Query: <code>slaf query output.slaf \"SELECT * FROM expression LIMIT 10\"</code></li> <li>Use in Python: <code>import slaf; data = slaf.SLAFArray(\"output.slaf\")</code></li> <li>Check Source Files: <code>slaf query output.slaf \"SELECT DISTINCT source_file FROM cells\"</code></li> </ol> <p>See the Getting Started guide for more examples.</p>"}]}