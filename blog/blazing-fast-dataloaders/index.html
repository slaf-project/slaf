<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Sparse Lazy Array Format - High-performance single-cell data storage and analysis"><meta name=author content="Pavan Ramkumar"><link href=https://slaf-project.github.io/slaf/blog/blazing-fast-dataloaders/ rel=canonical><link href=../introducing-slaf/ rel=prev><link href=../blazing-fast-dataloaders-2/ rel=next><link rel=icon href=../../assets/slaf-icon-transparent-dark-mono.svg><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>Blazing Fast Dataloaders - SLAF Documentation</title><link rel=stylesheet href=../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../assets/_mkdocstrings.css><link rel=stylesheet href=../../stylesheets/extra.css><link rel=stylesheet href=../../stylesheets/logo-theme.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config",""),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#64x-faster-dataloaders-deconstructing-pytorch-for-single-cell-genomics class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="SLAF Documentation" class="md-header__button md-logo" aria-label="SLAF Documentation" data-md-component=logo> <img src=../../assets/slaf-icon-transparent-dark-mono.svg alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> SLAF Documentation </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Blazing Fast Dataloaders </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/slaf-project/slaf title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> slaf-project/slaf </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="SLAF Documentation" class="md-nav__button md-logo" aria-label="SLAF Documentation" data-md-component=logo> <img src=../../assets/slaf-icon-transparent-dark-mono.svg alt=logo> </a> SLAF Documentation </label> <div class=md-nav__source> <a href=https://github.com/slaf-project/slaf title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> slaf-project/slaf </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-nav__item> <a href=../../getting-started/quickstart/ class=md-nav__link> <span class=md-ellipsis> Quick Start </span> </a> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> User Guide </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> User Guide </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../user-guide/how-slaf-works/ class=md-nav__link> <span class=md-ellipsis> How SLAF Works </span> </a> </li> <li class=md-nav__item> <a href=../../user-guide/migrating-to-slaf/ class=md-nav__link> <span class=md-ellipsis> Migrating to SLAF </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex> <span class=md-ellipsis> Examples </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Examples </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../examples/getting-started/ class=md-nav__link> <span class=md-ellipsis> Getting Started </span> </a> </li> <li class=md-nav__item> <a href=../../examples/lazy-processing/ class=md-nav__link> <span class=md-ellipsis> Lazy Processing </span> </a> </li> <li class=md-nav__item> <a href=../../examples/ml-training/ class=md-nav__link> <span class=md-ellipsis> ML Training </span> </a> </li> <li class=md-nav__item> <a href=../../examples/sql-queries/ class=md-nav__link> <span class=md-ellipsis> SQL Queries </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex> <span class=md-ellipsis> Benchmarks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Benchmarks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../benchmarks/ class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=../../benchmarks/bioinformatics_benchmarks/ class=md-nav__link> <span class=md-ellipsis> For Bioinformaticians </span> </a> </li> <li class=md-nav__item> <a href=../../benchmarks/ml_benchmarks/ class=md-nav__link> <span class=md-ellipsis> For ML Engineers </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_6> <label class=md-nav__link for=__nav_6 id=__nav_6_label tabindex> <span class=md-ellipsis> API Reference </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_6_label aria-expanded=false> <label class=md-nav__title for=__nav_6> <span class="md-nav__icon md-icon"></span> API Reference </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../api/core/ class=md-nav__link> <span class=md-ellipsis> Core </span> </a> </li> <li class=md-nav__item> <a href=../../api/data/ class=md-nav__link> <span class=md-ellipsis> Data </span> </a> </li> <li class=md-nav__item> <a href=../../api/integrations/ class=md-nav__link> <span class=md-ellipsis> Integrations </span> </a> </li> <li class=md-nav__item> <a href=../../api/ml/ class=md-nav__link> <span class=md-ellipsis> ML </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_7> <label class=md-nav__link for=__nav_7 id=__nav_7_label tabindex> <span class=md-ellipsis> Development </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_7_label aria-expanded=false> <label class=md-nav__title for=__nav_7> <span class="md-nav__icon md-icon"></span> Development </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../development/contributing/ class=md-nav__link> <span class=md-ellipsis> For Contributors </span> </a> </li> <li class=md-nav__item> <a href=../../development/maintaining/ class=md-nav__link> <span class=md-ellipsis> For Maintainers </span> </a> </li> <li class=md-nav__item> <a href=../../development/benchmarks/ class=md-nav__link> <span class=md-ellipsis> Benchmarks </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_8 checked> <label class=md-nav__link for=__nav_8 id=__nav_8_label tabindex> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_8_label aria-expanded=true> <label class=md-nav__title for=__nav_8> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Blog Home </span> </a> </li> <li class=md-nav__item> <a href=../huggingface/ class=md-nav__link> <span class=md-ellipsis> SLAF on Hugging Face </span> </a> </li> <li class=md-nav__item> <a href=../introducing-slaf/ class=md-nav__link> <span class=md-ellipsis> Introducing SLAF </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Blazing Fast Dataloaders </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Blazing Fast Dataloaders </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#streaming-transcriptomics-data-to-modern-gpus class=md-nav__link> <span class=md-ellipsis> Streaming Transcriptomics Data to Modern GPUs </span> </a> </li> <li class=md-nav__item> <a href=#the-standard-approach-and-its-limitations class=md-nav__link> <span class=md-ellipsis> The Standard Approach and Its Limitations </span> </a> </li> <li class=md-nav__item> <a href=#the-journey-from-naive-to-optimized class=md-nav__link> <span class=md-ellipsis> The Journey: From Naive to Optimized </span> </a> <nav class=md-nav aria-label="The Journey: From Naive to Optimized"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#phase-1-sql-window-functions class=md-nav__link> <span class=md-ellipsis> Phase 1: SQL Window Functions </span> </a> </li> <li class=md-nav__item> <a href=#phase-2-switch-from-range-scans-to-random-reads class=md-nav__link> <span class=md-ellipsis> Phase 2: Switch from Range Scans to Random Reads </span> </a> </li> <li class=md-nav__item> <a href=#phase-3-switching-window-functions-from-pandas-to-polars class=md-nav__link> <span class=md-ellipsis> Phase 3: Switching Window Functions from Pandas to Polars </span> </a> </li> <li class=md-nav__item> <a href=#understanding-the-scaling-laws class=md-nav__link> <span class=md-ellipsis> Understanding the Scaling Laws </span> </a> </li> <li class=md-nav__item> <a href=#phase-4-the-deconstructed-dataloader class=md-nav__link> <span class=md-ellipsis> Phase 4: The deconstructed dataloader </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#the-optimized-pipeline class=md-nav__link> <span class=md-ellipsis> The Optimized Pipeline </span> </a> </li> <li class=md-nav__item> <a href=#performance-comparison class=md-nav__link> <span class=md-ellipsis> Performance Comparison </span> </a> </li> <li class=md-nav__item> <a href=#results-and-future-directions class=md-nav__link> <span class=md-ellipsis> Results and Future Directions </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../blazing-fast-dataloaders-2/ class=md-nav__link> <span class=md-ellipsis> Mixture of Scanners </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#streaming-transcriptomics-data-to-modern-gpus class=md-nav__link> <span class=md-ellipsis> Streaming Transcriptomics Data to Modern GPUs </span> </a> </li> <li class=md-nav__item> <a href=#the-standard-approach-and-its-limitations class=md-nav__link> <span class=md-ellipsis> The Standard Approach and Its Limitations </span> </a> </li> <li class=md-nav__item> <a href=#the-journey-from-naive-to-optimized class=md-nav__link> <span class=md-ellipsis> The Journey: From Naive to Optimized </span> </a> <nav class=md-nav aria-label="The Journey: From Naive to Optimized"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#phase-1-sql-window-functions class=md-nav__link> <span class=md-ellipsis> Phase 1: SQL Window Functions </span> </a> </li> <li class=md-nav__item> <a href=#phase-2-switch-from-range-scans-to-random-reads class=md-nav__link> <span class=md-ellipsis> Phase 2: Switch from Range Scans to Random Reads </span> </a> </li> <li class=md-nav__item> <a href=#phase-3-switching-window-functions-from-pandas-to-polars class=md-nav__link> <span class=md-ellipsis> Phase 3: Switching Window Functions from Pandas to Polars </span> </a> </li> <li class=md-nav__item> <a href=#understanding-the-scaling-laws class=md-nav__link> <span class=md-ellipsis> Understanding the Scaling Laws </span> </a> </li> <li class=md-nav__item> <a href=#phase-4-the-deconstructed-dataloader class=md-nav__link> <span class=md-ellipsis> Phase 4: The deconstructed dataloader </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#the-optimized-pipeline class=md-nav__link> <span class=md-ellipsis> The Optimized Pipeline </span> </a> </li> <li class=md-nav__item> <a href=#performance-comparison class=md-nav__link> <span class=md-ellipsis> Performance Comparison </span> </a> </li> <li class=md-nav__item> <a href=#results-and-future-directions class=md-nav__link> <span class=md-ellipsis> Results and Future Directions </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=64x-faster-dataloaders-deconstructing-pytorch-for-single-cell-genomics>6.4x Faster DataLoaders: Deconstructing PyTorch for Single-Cell Genomics<a class=headerlink href=#64x-faster-dataloaders-deconstructing-pytorch-for-single-cell-genomics title="Permanent link">&para;</a></h1> <p>Single-cell transcriptomics datasets have reached escape velocity. With falling assay and sequencing costs, a modern experiment can yield counts for upwards of 5M cells × 20k genes, representing a 100-fold increase in scale from just 5 years ago.</p> <p>As we've seen in other domains, this is also the precise moment where <a href=https://en.wikipedia.org/wiki/Jevons_paradox>Jevons paradox</a> (the principle that demand will rise to meet surplus in supply) and Sutton's <a href=http://www.incompleteideas.net/IncIdeas/BitterLesson.html>bitter lesson</a> (the principle that large enough data and large enough models can outperform alternatives derived from decades of domain-specific creativity) kick in.</p> <p>Cue: Atlas-scale datasets (<a href=https://www.biorxiv.org/content/10.1101/2025.02.20.639398v1>Tahoe-100M</a>, <a href=https://www.humancellatlas.org/ >Human Cell Atlas</a>, <a href=https://www.biorxiv.org/content/10.1101/2025.02.27.640494v1>scBaseCamp</a>), transformer-based foundation models (<a href=https://www.nature.com/articles/s41592-024-02201-0>scGPT</a>, <a href=https://www.biorxiv.org/content/10.1101/2025.04.25.650731v1>Transcriptformer</a>, <a href=https://www.biorxiv.org/content/10.1101/2025.06.26.661135v2>STATE</a>), and the <a href=https://virtualcellchallenge.org/ >virtual cell era</a>.</p> <p>To keep pace with this deluge, I developed <a href=https://slaf-project.github.io/slaf/ >Sparse Lazy Array Format (SLAF)</a>. SLAF is a ground-up rethink of storage and compute for single cell data.</p> <ul> <li>SLAF is a columnar, cloud-native, vector-first storage container (on top of an open standard like <a href=https://lancedb.github.io/lance/ >Lance</a>).</li> <li>SLAF provides lazy compute interfaces that resemble familiar scanpy idioms, and state-of-the art data loaders for training transformer-based foundation models.</li> </ul> <p>Learn about SLAF <a href=https://slaf-project.github.io/slaf/blog/introducing-slaf/ >here</a> and <a href=https://github.com/slaf-project/slaf/ >start using it</a> for your single-cell workflows today.</p> <p>In this blog post, we'll go behind the scenes of SLAF's data loader, and the various optimizations that improve throughput ~6.4x with respect to a naive PyTorch implementation on SLAF, and ~100x with respect to status quo approaches for single-cell data loading. Look for comprehensive dataloader benchmarks <a href=https://slaf-project.github.io/slaf/benchmarks/ml_benchmarks/ >here</a>.</p> <h2 id=streaming-transcriptomics-data-to-modern-gpus>Streaming Transcriptomics Data to Modern GPUs<a class=headerlink href=#streaming-transcriptomics-data-to-modern-gpus title="Permanent link">&para;</a></h2> <p>Training foundation models on single-cell RNA-seq data requires streaming massive datasets to GPU clusters. Let's do some napkin math:</p> <div class="admonition info"> <p class=admonition-title>Performance Requirements</p> <ul> <li><strong>Dataset</strong>: 100M cells, ~60B non-zero expression values</li> <li><strong>Model</strong>: 1.2B parameter transformer (scGPT)</li> <li><strong>Batch size</strong>: 32 cells</li> <li><strong>Training loop throughput</strong>: 20 batches/second on H100 GPU</li> <li><strong>Per-GPU requirement</strong>: 640 cells/second</li> <li><strong>8 × H100 node requirement</strong>: ~5-6K cells/second</li> </ul> </div> <div class="admonition warning"> <p class=admonition-title>SLAF Constraints</p> <p>SLAF serves three user personas:</p> <p>(1) <strong>dashboard builders for no-code users</strong></p> <p>(2) <strong>computational biologists building scanpy pipelines</strong></p> <p>(3) <strong>ML engineers training foundation models</strong></p> <p>To support the first two personas, SLAF relies on performance-optimized dataframes (polars) and OLAP query engines (duckdb), both of which benefit from pre-sorted cell and gene IDs in Lance tables.</p> <p>Given the mission of "store once, query-in-place", pre-randomization and pre-tokenization weren't realistic options.</p> </div> <p><strong>The challenge</strong>: Stream 5-6K cells/second from cloud-native Lance storage, doing on-the-fly randomization and tokenization, providing GPU-ready cell sentences.</p> <h2 id=the-standard-approach-and-its-limitations>The Standard Approach and Its Limitations<a class=headerlink href=#the-standard-approach-and-its-limitations title="Permanent link">&para;</a></h2> <p>Let's start with a traditional PyTorch approach:</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=k>class</span><span class=w> </span><span class=nc>SingleCellDataset</span><span class=p>(</span><span class=n>IterableDataset</span><span class=p>):</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__iter__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>        <span class=c1># Load one cell at a time</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>        <span class=k>for</span> <span class=n>cell_id</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>num_cells</span><span class=p>):</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5 href=#__codelineno-0-5></a>            <span class=c1># Load cell data from storage</span>
</span><span id=__span-0-6><a id=__codelineno-0-6 name=__codelineno-0-6 href=#__codelineno-0-6></a>            <span class=n>cell_data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_cell</span><span class=p>(</span><span class=n>cell_integer_id</span><span class=p>)</span>
</span><span id=__span-0-7><a id=__codelineno-0-7 name=__codelineno-0-7 href=#__codelineno-0-7></a>            <span class=c1># Apply window functions for gene ranking</span>
</span><span id=__span-0-8><a id=__codelineno-0-8 name=__codelineno-0-8 href=#__codelineno-0-8></a>            <span class=n>ranked_genes</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rank_genes</span><span class=p>(</span><span class=n>cell_data</span><span class=p>)</span>
</span><span id=__span-0-9><a id=__codelineno-0-9 name=__codelineno-0-9 href=#__codelineno-0-9></a>            <span class=c1># Tokenize the sequence</span>
</span><span id=__span-0-10><a id=__codelineno-0-10 name=__codelineno-0-10 href=#__codelineno-0-10></a>            <span class=n>tokens</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>tokenize</span><span class=p>(</span><span class=n>ranked_genes</span><span class=p>)</span>
</span><span id=__span-0-11><a id=__codelineno-0-11 name=__codelineno-0-11 href=#__codelineno-0-11></a>            <span class=k>yield</span> <span class=n>tokens</span>
</span><span id=__span-0-12><a id=__codelineno-0-12 name=__codelineno-0-12 href=#__codelineno-0-12></a>
</span><span id=__span-0-13><a id=__codelineno-0-13 name=__codelineno-0-13 href=#__codelineno-0-13></a><span class=c1># Use standard DataLoader with multiprocessing</span>
</span><span id=__span-0-14><a id=__codelineno-0-14 name=__codelineno-0-14 href=#__codelineno-0-14></a><span class=n>dataloader</span> <span class=o>=</span> <span class=n>DataLoader</span><span class=p>(</span>
</span><span id=__span-0-15><a id=__codelineno-0-15 name=__codelineno-0-15 href=#__codelineno-0-15></a>    <span class=n>dataset</span><span class=p>,</span>
</span><span id=__span-0-16><a id=__codelineno-0-16 name=__codelineno-0-16 href=#__codelineno-0-16></a>    <span class=n>batch_size</span><span class=o>=</span><span class=mi>32</span><span class=p>,</span>
</span><span id=__span-0-17><a id=__codelineno-0-17 name=__codelineno-0-17 href=#__codelineno-0-17></a>    <span class=n>num_workers</span><span class=o>=</span><span class=mi>8</span><span class=p>,</span>      <span class=c1># Multiprocessing for parallelism</span>
</span><span id=__span-0-18><a id=__codelineno-0-18 name=__codelineno-0-18 href=#__codelineno-0-18></a>    <span class=n>prefetch_factor</span><span class=o>=</span><span class=mi>4</span>   <span class=c1># Asynchronously prefetch subsequent batches</span>
</span><span id=__span-0-19><a id=__codelineno-0-19 name=__codelineno-0-19 href=#__codelineno-0-19></a><span class=p>)</span>
</span></code></pre></div> <div class="language-python highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=k>def</span><span class=w> </span><span class=nf>load_cell</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cell_id</span><span class=p>):</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Load cell data using SQL query to expression lance table&quot;&quot;&quot;</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>    <span class=n>sql_query</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&quot;&quot;&quot;</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a><span class=s2>    SELECT cell_integer_id, gene_integer_id, value</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=s2>    FROM expression</span>
</span><span id=__span-1-6><a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a><span class=s2>    WHERE cell_integer_id = </span><span class=si>{</span><span class=n>cell_id</span><span class=si>}</span>
</span><span id=__span-1-7><a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=s2>    &quot;&quot;&quot;</span>
</span><span id=__span-1-8><a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a>    <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>expression_dataset</span><span class=o>.</span><span class=n>sql</span><span class=p>(</span><span class=n>sql_query</span><span class=p>)</span><span class=o>.</span><span class=n>to_pandas</span><span class=p>()</span>
</span><span id=__span-1-9><a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>
</span><span id=__span-1-10><a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=k>def</span><span class=w> </span><span class=nf>rank_genes</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cell_data</span><span class=p>):</span>
</span><span id=__span-1-11><a id=__codelineno-1-11 name=__codelineno-1-11 href=#__codelineno-1-11></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Rank genes using pandas-based windowing&quot;&quot;&quot;</span>
</span><span id=__span-1-12><a id=__codelineno-1-12 name=__codelineno-1-12 href=#__codelineno-1-12></a>    <span class=n>cell_data</span><span class=p>[</span><span class=s1>&#39;gene_rank&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>cell_data</span><span class=o>.</span><span class=n>groupby</span><span class=p>(</span><span class=s1>&#39;cell_integer_id&#39;</span><span class=p>)[</span><span class=s1>&#39;value&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>rank</span><span class=p>(</span>
</span><span id=__span-1-13><a id=__codelineno-1-13 name=__codelineno-1-13 href=#__codelineno-1-13></a>        <span class=n>method</span><span class=o>=</span><span class=s1>&#39;dense&#39;</span><span class=p>,</span> <span class=n>ascending</span><span class=o>=</span><span class=kc>False</span>
</span><span id=__span-1-14><a id=__codelineno-1-14 name=__codelineno-1-14 href=#__codelineno-1-14></a>    <span class=p>)</span>
</span><span id=__span-1-15><a id=__codelineno-1-15 name=__codelineno-1-15 href=#__codelineno-1-15></a>    <span class=k>return</span> <span class=n>cell_data</span><span class=p>[</span><span class=n>cell_data</span><span class=p>[</span><span class=s1>&#39;gene_rank&#39;</span><span class=p>]</span> <span class=o>&lt;=</span> <span class=bp>self</span><span class=o>.</span><span class=n>max_genes</span><span class=p>]</span>
</span></code></pre></div> <div class="admonition warning"> <p class=admonition-title>Result: 4,408 cells/second</p> </div> <p>This naive approach doesn't scale because it ignores several critical aspects unique to our data format:</p> <div class="admonition info"> <p class=admonition-title>Key Limitations</p> <ol> <li><strong>Storage format mismatch</strong>: Lance tables store expression counts as <code>(cell_integer_id, gene_integer_id, value)</code> per record, but models need one cell per data point</li> <li><strong>Inefficient I/O</strong>: Sample-by-sample loading ignores Lance's optimized <code>to_batches()</code> generator for contiguous reads</li> <li><strong>Wrong shuffling granularity</strong>: Standard shuffling breaks cell data integrity</li> <li><strong>Missing vectorization</strong>: No batch processing for ranking and tokenization</li> <li><strong>Multiprocessing overhead</strong>: PyTorch's approach conflicts with Lance's multi-threaded I/O</li> </ol> </div> <p>As we'll see, fully embracing these deviations meant writing our own async prefetcher and vectorized transforms—which is why state-of-the-art solutions like Mosaic ML's <a href=https://github.com/mosaicml/streaming>mosaicml-streaming</a> and <a href=https://docs.ray.io/en/latest/data/data.html>Ray Data</a> didn't work out of the box for SLAF.</p> <h2 id=the-journey-from-naive-to-optimized>The Journey: From Naive to Optimized<a class=headerlink href=#the-journey-from-naive-to-optimized title="Permanent link">&para;</a></h2> <h3 id=phase-1-sql-window-functions>Phase 1: SQL Window Functions<a class=headerlink href=#phase-1-sql-window-functions title="Permanent link">&para;</a></h3> <p>We started with the pushdown promise of OLAP databases, leveraging SQL window functions to combine cell loading and gene ranking by pushing compute down to Lance tables.</p> <div class="language-sql highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=k>WITH</span><span class=w> </span><span class=n>ranked_genes</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a><span class=w>    </span><span class=k>SELECT</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a><span class=w>        </span><span class=n>cell_integer_id</span><span class=p>,</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a><span class=w>        </span><span class=n>gene_integer_id</span><span class=p>,</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a><span class=w>        </span><span class=n>value</span><span class=p>,</span>
</span><span id=__span-2-6><a id=__codelineno-2-6 name=__codelineno-2-6 href=#__codelineno-2-6></a><span class=w>        </span><span class=n>ROW_NUMBER</span><span class=p>()</span><span class=w> </span><span class=n>OVER</span><span class=w> </span><span class=p>(</span>
</span><span id=__span-2-7><a id=__codelineno-2-7 name=__codelineno-2-7 href=#__codelineno-2-7></a><span class=w>            </span><span class=n>PARTITION</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>cell_integer_id</span>
</span><span id=__span-2-8><a id=__codelineno-2-8 name=__codelineno-2-8 href=#__codelineno-2-8></a><span class=w>            </span><span class=k>ORDER</span><span class=w> </span><span class=k>BY</span><span class=w> </span><span class=n>value</span><span class=w> </span><span class=k>DESC</span><span class=p>,</span><span class=w> </span><span class=n>gene_integer_id</span><span class=w> </span><span class=k>ASC</span>
</span><span id=__span-2-9><a id=__codelineno-2-9 name=__codelineno-2-9 href=#__codelineno-2-9></a><span class=w>            </span><span class=k>LIMIT</span><span class=w> </span><span class=err>{</span><span class=n>max_genes</span><span class=err>}</span>
</span><span id=__span-2-10><a id=__codelineno-2-10 name=__codelineno-2-10 href=#__codelineno-2-10></a><span class=w>        </span><span class=p>)</span><span class=w> </span><span class=k>as</span><span class=w> </span><span class=n>gene_rank</span>
</span><span id=__span-2-11><a id=__codelineno-2-11 name=__codelineno-2-11 href=#__codelineno-2-11></a><span class=w>    </span><span class=k>FROM</span><span class=w> </span><span class=n>expression</span>
</span><span id=__span-2-12><a id=__codelineno-2-12 name=__codelineno-2-12 href=#__codelineno-2-12></a><span class=w>    </span><span class=k>WHERE</span><span class=w> </span><span class=n>cell_integer_id</span><span class=w> </span><span class=k>BETWEEN</span><span class=w> </span><span class=err>{</span><span class=n>cell_start</span><span class=err>}</span><span class=w> </span><span class=k>AND</span><span class=w> </span><span class=err>{</span><span class=n>cell_start</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>batch_size</span><span class=err>}</span>
</span><span id=__span-2-13><a id=__codelineno-2-13 name=__codelineno-2-13 href=#__codelineno-2-13></a><span class=p>)</span>
</span></code></pre></div> <div class="admonition warning"> <p class=admonition-title>Result: 4,582 cells/second</p> </div> <p><strong>Problem</strong>: Range scans are expensive for massive tables without indexes. For datasets of this scale (~60B rows), indexes add 200% storage overhead, so we want to do as much as possible without indexes.</p> <h3 id=phase-2-switch-from-range-scans-to-random-reads>Phase 2: Switch from Range Scans to Random Reads<a class=headerlink href=#phase-2-switch-from-range-scans-to-random-reads title="Permanent link">&para;</a></h3> <p>After recognizing the range scan bottleneck, I realized we could exploit lancedb's 100x faster-than-parquet random access capability. I switched to PyArrow's <code>take()</code> method using an inverted index for cell start and end positions.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=k>def</span><span class=w> </span><span class=nf>load_cells_efficient</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>cell_ids</span><span class=p>):</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Load cells using Lance take() for efficient row access&quot;&quot;&quot;</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    <span class=c1># Convert cell IDs to integer IDs</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>    <span class=n>integer_ids</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_normalize_entity_ids</span><span class=p>(</span><span class=n>cell_ids</span><span class=p>,</span> <span class=s2>&quot;cell&quot;</span><span class=p>)</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>    <span class=c1># Get row indices using RowIndexMapper</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>    <span class=n>row_indices</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>row_mapper</span><span class=o>.</span><span class=n>get_cell_row_ranges</span><span class=p>(</span><span class=n>integer_ids</span><span class=p>)</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>    <span class=c1># Load data with Lance take() - much faster than SQL queries</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>    <span class=n>expression_data</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>expression</span><span class=o>.</span><span class=n>take</span><span class=p>(</span><span class=n>row_indices</span><span class=p>)</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a>    <span class=c1># Convert to Polars DataFrame</span>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a>    <span class=k>return</span> <span class=n>pl</span><span class=o>.</span><span class=n>from_arrow</span><span class=p>(</span><span class=n>expression_data</span><span class=p>)</span>
</span></code></pre></div> <div class="admonition success"> <p class=admonition-title>Result: 7,086 cells/second</p> </div> <p><strong>Problem</strong>: Pandas-based windowing operations become the bottleneck.</p> <p>We could stop here because we've reached our original criteria of streaming 5-6k cells / second, but let's keep going!</p> <h3 id=phase-3-switching-window-functions-from-pandas-to-polars>Phase 3: Switching Window Functions from Pandas to Polars<a class=headerlink href=#phase-3-switching-window-functions-from-pandas-to-polars title="Permanent link">&para;</a></h3> <p>With Rust-based acceleration of data frame operations, and drop-in replacement to Pandas, Polars was the natural choice for window functions.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>result</span> <span class=o>=</span> <span class=n>filtered_df</span><span class=o>.</span><span class=n>with_columns</span><span class=p>([</span>
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>    <span class=n>pl</span><span class=o>.</span><span class=n>col</span><span class=p>(</span><span class=s2>&quot;value&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>rank</span><span class=p>(</span><span class=n>method</span><span class=o>=</span><span class=s2>&quot;dense&quot;</span><span class=p>,</span> <span class=n>descending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a>    <span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=s2>&quot;cell_integer_id&quot;</span><span class=p>)</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>    <span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>&quot;gene_rank&quot;</span><span class=p>)</span>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=p>])</span>
</span></code></pre></div> <div class="admonition success"> <p class=admonition-title>Result: 16,620 cells/second (much better!)</p> </div> <h3 id=understanding-the-scaling-laws>Understanding the Scaling Laws<a class=headerlink href=#understanding-the-scaling-laws title="Permanent link">&para;</a></h3> <p>Before going further, I needed to understand why larger batches were performing better. I began measuring how different operations scaled with batch size, and discovered a series of <strong>scaling laws</strong> that would guide the final architecture. These laws describe how the per-cell cost of operations changes as batch size increases: crucial information for designing an optimal prefetching strategy.</p> <div class="admonition info"> <p class=admonition-title>The Scaling Law Discovery</p> <p>As I optimized the pipeline, I began to notice patterns in how different operations scaled with batch size. This led me to measure the actual scaling characteristics of our key operations. What I found was a series of sub-linear scaling laws that would guide the final architecture:</p> <p><strong>Window Function Scaling (Gene Ranking)</strong></p> <table> <thead> <tr> <th>Batch Size (cells)</th> <th>Runtime (μs)</th> <th>Per-cell Cost (μs)</th> </tr> </thead> <tbody> <tr> <td>32</td> <td>1,983</td> <td>62.0</td> </tr> <tr> <td>64</td> <td>2,237</td> <td>35.0</td> </tr> <tr> <td>128</td> <td>3,888</td> <td>30.4</td> </tr> <tr> <td>256</td> <td>6,173</td> <td>24.1</td> </tr> <tr> <td>512</td> <td>10,533</td> <td>20.6</td> </tr> <tr> <td>1024</td> <td>19,523</td> <td>19.1</td> </tr> </tbody> </table> <pre class=mermaid><code>xychart
    title "Window Function Scaling: Per-cell Cost vs Batch Size"
    x-axis "Batch Size (cells)" [32, 64, 128, 256, 512, 1024]
    y-axis "Per-cell Cost (μs)" 0 --&gt; 70
    line [62.0, 35.0, 30.4, 24.1, 20.6, 19.1]</code></pre> <p><strong>Lance Data Loading Scaling</strong></p> <table> <thead> <tr> <th>Batches per Chunk</th> <th>Runtime (μs)</th> <th>Per-row Cost (μs)</th> <th>Rows Processed</th> </tr> </thead> <tbody> <tr> <td>10</td> <td>58,587</td> <td>715.2</td> <td>81,920</td> </tr> <tr> <td>25</td> <td>97,178</td> <td>474.5</td> <td>204,800</td> </tr> <tr> <td>50</td> <td>50,104</td> <td>122.3</td> <td>409,600</td> </tr> <tr> <td>100</td> <td>43,335</td> <td>52.9</td> <td>819,200</td> </tr> <tr> <td>200</td> <td>51,451</td> <td>31.4</td> <td>1,638,400</td> </tr> </tbody> </table> <pre class=mermaid><code>xychart
    title "Lance Data Loading Scaling: Per-row Cost vs Chunk Size"
    x-axis "Batches per Chunk" [10, 25, 50, 100, 200]
    y-axis "Per-row Cost (μs)" 0 --&gt; 800
    line [715.2, 474.5, 122.3, 52.9, 31.4]</code></pre> <p><strong>Block Shuffling Scaling</strong></p> <table> <thead> <tr> <th>Unique Cells</th> <th>Runtime (μs)</th> <th>Per-cell Cost (μs)</th> </tr> </thead> <tbody> <tr> <td>100</td> <td>950</td> <td>9.5</td> </tr> <tr> <td>500</td> <td>3,087</td> <td>6.2</td> </tr> <tr> <td>1000</td> <td>11,573</td> <td>11.6</td> </tr> <tr> <td>2000</td> <td>13,044</td> <td>6.5</td> </tr> <tr> <td>5000</td> <td>29,567</td> <td>5.9</td> </tr> </tbody> </table> <pre class=mermaid><code>xychart
    title "Block Shuffling Scaling: Per-cell Cost vs Number of Cells"
    x-axis "Unique Cells" [100, 500, 1000, 2000, 5000]
    y-axis "Per-cell Cost (μs)" 0 --&gt; 15
    line [9.5, 6.2, 11.6, 6.5, 5.9]</code></pre> <p>These scaling laws revealed critical insights:</p> <ul> <li><strong>Window functions</strong>: 0.31x scaling factor (larger batches are 3x more efficient)</li> <li><strong>Lance loading</strong>: 0.04x scaling factor (larger chunks are 25x more efficient)</li> <li><strong>Block shuffling</strong>: 0.62x scaling factor (more cells are 1.6x more efficient)</li> </ul> <p>The solution became clear: async prefetching and preprocessing at a different batch size than the training loop, leveraging these sub-linear scaling characteristics.</p> </div> <h3 id=phase-4-the-deconstructed-dataloader>Phase 4: The deconstructed dataloader<a class=headerlink href=#phase-4-the-deconstructed-dataloader title="Permanent link">&para;</a></h3> <p>Armed with this insight, I looked around for libraries that provide the flexibility to decouple async prefetching batch size from training batch size in the dataloader ecosystem without having the write my own async prefetcher and queue management systems. I looked into Mosaic ML's streaming library and Ray Data.</p> <ul> <li> <p><code>mosaic-ml-streaming</code> was dead on arrival because it expects users to convert to Mosaic's custom format or other standards like jsonl. This violates our self-imposed constraint of "store once, query in place" and wouldn't be a drop-in replacement for PyTorch over SLAF.</p> </li> <li> <p>Ray Data actually supports most of our requirements - it has direct Lance integration with <code>read_lance()</code>, supports Arrow tables and custom transformations, and can handle different batch sizes for prefetching vs training. However, Ray's <code>transform_data</code> method is opinionated about input types, accepting pandas dataframes and Arrow Tables but not polars dataframes. Ultimately, we chose to build a custom solution to avoid the conversion overhead between <code>pyarrow.RecordBatch</code> → <code>pyarrow.Table</code> → Polars → <code>ray.data.Dataset</code>, and to have tighter control over the specific optimizations like block shuffling and window functions in polars. Once Ray's <code>transform_data</code> supported polars dataframes directly, we could switch.</p> </li> </ul> <p>Given this state, I decided to bite the bullet and write my own thread-based async prefetcher. The hardest part was unlearning PyTorch's received wisdom about the roles of the Dataset and DataLoader classes.</p> <table> <thead> <tr> <th>Component</th> <th>Traditional PyTorch</th> <th>Our Deconstructed Approach</th> </tr> </thead> <tbody> <tr> <td><strong>Dataset</strong></td> <td>"I just return one sample at a time"</td> <td>"I'm a streaming interface that pre-fetches and pre-processes many training batches at once"</td> </tr> <tr> <td><strong>DataLoader</strong></td> <td>"I handle batching, shuffling, and parallelization"</td> <td>"I'm a simple iterator with async prefetching"</td> </tr> <tr> <td><strong>Tokenizer</strong></td> <td>"I process sequences on-demand in the training loop"</td> <td>"I batch-process sequences in background threads outside the training loop"</td> </tr> </tbody> </table> <p>Once I reframed it this way, the rest was straightforward.</p> <div class="admonition info"> <p class=admonition-title>1. Contiguous Reads with Two-Level Hierarchy</p> <p>Pre-fetching large batches enables switching from random reads (<code>take()</code>) to contiguous reads (<code>to_batches()</code>), then block-shuffling cells within the prefetch batch. Random reads are always slower than contiguous reads. The upward pressure on prefetch batch size comes from minimizing disk reads, while downward pressure comes from I/O scaling laws and vectorized operation scaling laws.</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=k>class</span><span class=w> </span><span class=nc>PrefetchBatchProcessor</span><span class=p>:</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>slaf_array</span><span class=p>,</span> <span class=n>window</span><span class=p>,</span> <span class=n>shuffle</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>):</span>
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>        <span class=c1># Create Lance dataset and batch generator for contiguous reads</span>
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>        <span class=bp>self</span><span class=o>.</span><span class=n>expression_dataset</span> <span class=o>=</span> <span class=n>lance</span><span class=o>.</span><span class=n>dataset</span><span class=p>(</span><span class=sa>f</span><span class=s2>&quot;</span><span class=si>{</span><span class=n>slaf_array</span><span class=o>.</span><span class=n>slaf_path</span><span class=si>}</span><span class=s2>/expression.lance&quot;</span><span class=p>)</span>
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>        <span class=bp>self</span><span class=o>.</span><span class=n>batch_generator</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>expression_dataset</span><span class=o>.</span><span class=n>to_batches</span><span class=p>()</span>  <span class=c1># Contiguous reads</span>
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>    <span class=k>def</span><span class=w> </span><span class=nf>load_prefetch_batch</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>        <span class=c1># Load ~1.5M records (multiple Lance batches)</span>
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>        <span class=n>batch_dfs</span> <span class=o>=</span> <span class=p>[]</span>
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>batches_per_chunk</span><span class=p>):</span>  <span class=c1># 50 batches</span>
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a>            <span class=n>batch</span> <span class=o>=</span> <span class=nb>next</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>batch_generator</span><span class=p>)</span>  <span class=c1># Contiguous read from Lance</span>
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a>            <span class=n>batch_dfs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>pl</span><span class=o>.</span><span class=n>from_arrow</span><span class=p>(</span><span class=n>batch</span><span class=p>))</span>
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a>
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a>        <span class=c1># Apply window functions and shuffling at prefetch level</span>
</span><span id=__span-5-15><a id=__codelineno-5-15 name=__codelineno-5-15 href=#__codelineno-5-15></a>        <span class=n>combined_df</span> <span class=o>=</span> <span class=n>pl</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span><span class=n>batch_dfs</span><span class=p>)</span>
</span><span id=__span-5-16><a id=__codelineno-5-16 name=__codelineno-5-16 href=#__codelineno-5-16></a>        <span class=n>ranked_df</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>apply_window_functions</span><span class=p>(</span><span class=n>combined_df</span><span class=p>)</span>
</span><span id=__span-5-17><a id=__codelineno-5-17 name=__codelineno-5-17 href=#__codelineno-5-17></a>        <span class=n>shuffled_chunks</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>apply_block_shuffling</span><span class=p>(</span><span class=n>ranked_df</span><span class=p>)</span>
</span><span id=__span-5-18><a id=__codelineno-5-18 name=__codelineno-5-18 href=#__codelineno-5-18></a>
</span><span id=__span-5-19><a id=__codelineno-5-19 name=__codelineno-5-19 href=#__codelineno-5-19></a>        <span class=k>return</span> <span class=n>shuffled_chunks</span>  <span class=c1># Multiple training batches</span>
</span></code></pre></div> <p><strong>Key benefits</strong>: I/O efficiency through contiguous reads, vectorized processing with sub-linear scaling, and memory efficiency through chunked processing.</p> <div class="admonition info"> <p class=admonition-title>2. Single-Threaded Async Prefetching</p> <p>Our choice of single-threaded prefetching aligns with a broader trend in the PyTorch community away from multiprocessing toward threading for I/O-bound workloads. Recent work by NVIDIA has shown that <a href=https://developer.nvidia.com/blog/improved-data-loading-with-threads/ >threading can significantly outperform multiprocessing</a> for GPU-accelerated workloads. Modern frameworks like <a href=https://docs.ray.io/en/latest/train/user-guides/data-loading-preprocessing.html#prefetching-batches>Ray Train</a> have embraced threading-based prefetching.</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=k>class</span><span class=w> </span><span class=nc>AsyncPrefetcher</span><span class=p>:</span>
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>    <span class=k>def</span><span class=w> </span><span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>batch_processor</span><span class=p>,</span> <span class=n>max_queue_size</span><span class=o>=</span><span class=mi>500</span><span class=p>):</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>        <span class=bp>self</span><span class=o>.</span><span class=n>queue</span> <span class=o>=</span> <span class=n>Queue</span><span class=p>(</span><span class=n>maxsize</span><span class=o>=</span><span class=n>max_queue_size</span><span class=p>)</span>
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>        <span class=bp>self</span><span class=o>.</span><span class=n>worker_thread</span> <span class=o>=</span> <span class=n>threading</span><span class=o>.</span><span class=n>Thread</span><span class=p>(</span><span class=n>target</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>_prefetch_worker</span><span class=p>)</span>
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>    <span class=k>def</span><span class=w> </span><span class=nf>_prefetch_worker</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>        <span class=k>while</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>should_stop</span><span class=p>:</span>
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>            <span class=c1># Load prefetch batch (CPU-bound operations)</span>
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>            <span class=n>batch</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>batch_processor</span><span class=o>.</span><span class=n>load_prefetch_batch</span><span class=p>()</span>
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>            <span class=c1># Put in queue for training loop</span>
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>            <span class=bp>self</span><span class=o>.</span><span class=n>queue</span><span class=o>.</span><span class=n>put</span><span class=p>(</span><span class=n>batch</span><span class=p>)</span>
</span></code></pre></div> <p><strong>Key benefits</strong>: No serialization overhead, better cache locality, simpler coordination, and optimal Lance I/O utilization.</p> <div class="admonition info"> <p class=admonition-title>3. Vectorized Window Functions in Polars</p> <p>The breakthrough came from Polars' vectorized window functions. As batch size increases, the per-cell cost of window functions decreases, making larger prefetch batches more efficient.</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=c1># Vectorized gene ranking across all cells in a fragment</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class=n>grouped</span> <span class=o>=</span> <span class=p>(</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>    <span class=n>fragment_df</span><span class=o>.</span><span class=n>with_columns</span><span class=p>([</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>        <span class=n>pl</span><span class=o>.</span><span class=n>col</span><span class=p>(</span><span class=s2>&quot;value&quot;</span><span class=p>)</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>        <span class=o>.</span><span class=n>rank</span><span class=p>(</span><span class=n>method</span><span class=o>=</span><span class=s2>&quot;dense&quot;</span><span class=p>,</span> <span class=n>descending</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>        <span class=o>.</span><span class=n>over</span><span class=p>(</span><span class=s2>&quot;cell_integer_id&quot;</span><span class=p>)</span>
</span><span id=__span-7-7><a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a>        <span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>&quot;gene_rank&quot;</span><span class=p>)</span>
</span><span id=__span-7-8><a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a>    <span class=p>])</span>
</span><span id=__span-7-9><a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a>    <span class=o>.</span><span class=n>filter</span><span class=p>(</span><span class=n>pl</span><span class=o>.</span><span class=n>col</span><span class=p>(</span><span class=s2>&quot;gene_rank&quot;</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=n>max_genes</span><span class=p>)</span>
</span><span id=__span-7-10><a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a>    <span class=o>.</span><span class=n>group_by</span><span class=p>(</span><span class=s2>&quot;cell_integer_id&quot;</span><span class=p>)</span>
</span><span id=__span-7-11><a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a>    <span class=o>.</span><span class=n>agg</span><span class=p>([</span>
</span><span id=__span-7-12><a id=__codelineno-7-12 name=__codelineno-7-12 href=#__codelineno-7-12></a>        <span class=n>pl</span><span class=o>.</span><span class=n>col</span><span class=p>(</span><span class=s2>&quot;gene_integer_id&quot;</span><span class=p>)</span><span class=o>.</span><span class=n>alias</span><span class=p>(</span><span class=s2>&quot;gene_sequence&quot;</span><span class=p>),</span>
</span><span id=__span-7-13><a id=__codelineno-7-13 name=__codelineno-7-13 href=#__codelineno-7-13></a>    <span class=p>])</span>
</span><span id=__span-7-14><a id=__codelineno-7-14 name=__codelineno-7-14 href=#__codelineno-7-14></a><span class=p>)</span>
</span></code></pre></div> <p><strong>Key benefits</strong>: Columnar processing on memory-contiguous chunks, Rust acceleration for near-native performance, sub-linear scaling with batch size, and memory efficiency without intermediate copies.</p> <div class="admonition info"> <p class=admonition-title>4. Block Shuffling for Structured Data</p> <p>Traditional shuffling operates at the sample level, but our data format requires block shuffling to keep all records of a cell together:</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=k>def</span><span class=w> </span><span class=nf>efficient_block_shuffle</span><span class=p>(</span><span class=n>df</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>seed</span><span class=p>):</span>
</span><span id=__span-8-2><a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Shuffle at the cell level, then create batches&quot;&quot;&quot;</span>
</span><span id=__span-8-3><a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a>    <span class=c1># Partition by cell_integer_id (fast since data is pre-sorted)</span>
</span><span id=__span-8-4><a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a>    <span class=n>chunks</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>partition_by</span><span class=p>(</span><span class=s2>&quot;cell_integer_id&quot;</span><span class=p>,</span> <span class=n>as_dict</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span><span id=__span-8-5><a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a>
</span><span id=__span-8-6><a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a>    <span class=c1># Shuffle the list of chunks</span>
</span><span id=__span-8-7><a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a>    <span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span><span id=__span-8-8><a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a>    <span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>chunks</span><span class=p>)</span>
</span><span id=__span-8-9><a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a>
</span><span id=__span-8-10><a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a>    <span class=c1># Create batches of shuffled cells</span>
</span><span id=__span-8-11><a id=__codelineno-8-11 name=__codelineno-8-11 href=#__codelineno-8-11></a>    <span class=k>return</span> <span class=p>[</span>
</span><span id=__span-8-12><a id=__codelineno-8-12 name=__codelineno-8-12 href=#__codelineno-8-12></a>        <span class=n>pl</span><span class=o>.</span><span class=n>concat</span><span class=p>(</span><span class=n>chunks</span><span class=p>[</span><span class=n>i</span> <span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=n>batch_size</span><span class=p>])</span>
</span><span id=__span-8-13><a id=__codelineno-8-13 name=__codelineno-8-13 href=#__codelineno-8-13></a>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>chunks</span><span class=p>),</span> <span class=n>batch_size</span><span class=p>)</span>
</span><span id=__span-8-14><a id=__codelineno-8-14 name=__codelineno-8-14 href=#__codelineno-8-14></a>    <span class=p>]</span>
</span></code></pre></div> <p><strong>Key benefits</strong>: Memory efficiency by shuffling cell IDs instead of entire DataFrames, cache-friendly data locality preservation, and vectorized operations leveraging Polars' optimized filter operations.</p> <div class="admonition info"> <p class=admonition-title>5. Vectorized Tokenization</p> <p>Batch tokenization provides significant speedups through vectorized operations and efficient memory management:</p> </div> <div class="language-python highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a><span class=k>def</span><span class=w> </span><span class=nf>batch_tokenize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>gene_sequences</span><span class=p>,</span> <span class=n>expr_sequences</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>max_genes</span><span class=o>=</span><span class=mi>2048</span><span class=p>):</span>
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a><span class=w>    </span><span class=sd>&quot;&quot;&quot;Vectorized tokenization across entire prefetch batch&quot;&quot;&quot;</span>
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>    <span class=n>batch_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>gene_sequences</span><span class=p>)</span>
</span><span id=__span-9-4><a id=__codelineno-9-4 name=__codelineno-9-4 href=#__codelineno-9-4></a>
</span><span id=__span-9-5><a id=__codelineno-9-5 name=__codelineno-9-5 href=#__codelineno-9-5></a>    <span class=c1># Pre-allocate numpy array for speed</span>
</span><span id=__span-9-6><a id=__codelineno-9-6 name=__codelineno-9-6 href=#__codelineno-9-6></a>    <span class=n>token_array</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>full</span><span class=p>((</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>max_genes</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;PAD&quot;</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span>
</span><span id=__span-9-7><a id=__codelineno-9-7 name=__codelineno-9-7 href=#__codelineno-9-7></a>
</span><span id=__span-9-8><a id=__codelineno-9-8 name=__codelineno-9-8 href=#__codelineno-9-8></a>    <span class=c1># Vectorized processing for each sequence</span>
</span><span id=__span-9-9><a id=__codelineno-9-9 name=__codelineno-9-9 href=#__codelineno-9-9></a>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>gene_sequence</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>gene_sequences</span><span class=p>):</span>
</span><span id=__span-9-10><a id=__codelineno-9-10 name=__codelineno-9-10 href=#__codelineno-9-10></a>        <span class=c1># Convert gene IDs to tokens (fast mapping)</span>
</span><span id=__span-9-11><a id=__codelineno-9-11 name=__codelineno-9-11 href=#__codelineno-9-11></a>        <span class=n>gene_tokens</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>gene_sequence</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span> <span class=o>+</span> <span class=mi>4</span>
</span><span id=__span-9-12><a id=__codelineno-9-12 name=__codelineno-9-12 href=#__codelineno-9-12></a>
</span><span id=__span-9-13><a id=__codelineno-9-13 name=__codelineno-9-13 href=#__codelineno-9-13></a>        <span class=c1># Build sequence: [CLS] genes [SEP]</span>
</span><span id=__span-9-14><a id=__codelineno-9-14 name=__codelineno-9-14 href=#__codelineno-9-14></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>gene_tokens</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span><span id=__span-9-15><a id=__codelineno-9-15 name=__codelineno-9-15 href=#__codelineno-9-15></a>            <span class=n>tokens</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span>
</span><span id=__span-9-16><a id=__codelineno-9-16 name=__codelineno-9-16 href=#__codelineno-9-16></a>                <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;CLS&quot;</span><span class=p>]],</span>
</span><span id=__span-9-17><a id=__codelineno-9-17 name=__codelineno-9-17 href=#__codelineno-9-17></a>                <span class=n>gene_tokens</span><span class=p>,</span>
</span><span id=__span-9-18><a id=__codelineno-9-18 name=__codelineno-9-18 href=#__codelineno-9-18></a>                <span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;SEP&quot;</span><span class=p>]],</span>
</span><span id=__span-9-19><a id=__codelineno-9-19 name=__codelineno-9-19 href=#__codelineno-9-19></a>            <span class=p>])</span>
</span><span id=__span-9-20><a id=__codelineno-9-20 name=__codelineno-9-20 href=#__codelineno-9-20></a>        <span class=k>else</span><span class=p>:</span>
</span><span id=__span-9-21><a id=__codelineno-9-21 name=__codelineno-9-21 href=#__codelineno-9-21></a>            <span class=n>tokens</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;CLS&quot;</span><span class=p>],</span> <span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;SEP&quot;</span><span class=p>]],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span>
</span><span id=__span-9-22><a id=__codelineno-9-22 name=__codelineno-9-22 href=#__codelineno-9-22></a>
</span><span id=__span-9-23><a id=__codelineno-9-23 name=__codelineno-9-23 href=#__codelineno-9-23></a>        <span class=c1># Pad/truncate to max_genes</span>
</span><span id=__span-9-24><a id=__codelineno-9-24 name=__codelineno-9-24 href=#__codelineno-9-24></a>        <span class=n>tokens</span> <span class=o>=</span> <span class=n>tokens</span><span class=p>[:</span><span class=n>max_genes</span><span class=p>]</span>
</span><span id=__span-9-25><a id=__codelineno-9-25 name=__codelineno-9-25 href=#__codelineno-9-25></a>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>max_genes</span><span class=p>:</span>
</span><span id=__span-9-26><a id=__codelineno-9-26 name=__codelineno-9-26 href=#__codelineno-9-26></a>            <span class=n>padding</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>full</span><span class=p>(</span><span class=n>max_genes</span> <span class=o>-</span> <span class=nb>len</span><span class=p>(</span><span class=n>tokens</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;PAD&quot;</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int64</span><span class=p>)</span>
</span><span id=__span-9-27><a id=__codelineno-9-27 name=__codelineno-9-27 href=#__codelineno-9-27></a>            <span class=n>tokens</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>tokens</span><span class=p>,</span> <span class=n>padding</span><span class=p>])</span>
</span><span id=__span-9-28><a id=__codelineno-9-28 name=__codelineno-9-28 href=#__codelineno-9-28></a>
</span><span id=__span-9-29><a id=__codelineno-9-29 name=__codelineno-9-29 href=#__codelineno-9-29></a>        <span class=c1># Fill array</span>
</span><span id=__span-9-30><a id=__codelineno-9-30 name=__codelineno-9-30 href=#__codelineno-9-30></a>        <span class=n>token_array</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=n>tokens</span>
</span><span id=__span-9-31><a id=__codelineno-9-31 name=__codelineno-9-31 href=#__codelineno-9-31></a>
</span><span id=__span-9-32><a id=__codelineno-9-32 name=__codelineno-9-32 href=#__codelineno-9-32></a>    <span class=c1># Convert to tensors in one operation</span>
</span><span id=__span-9-33><a id=__codelineno-9-33 name=__codelineno-9-33 href=#__codelineno-9-33></a>    <span class=n>input_ids</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>token_array</span><span class=p>)</span>
</span><span id=__span-9-34><a id=__codelineno-9-34 name=__codelineno-9-34 href=#__codelineno-9-34></a>    <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>input_ids</span> <span class=o>!=</span> <span class=bp>self</span><span class=o>.</span><span class=n>special_tokens</span><span class=p>[</span><span class=s2>&quot;PAD&quot;</span><span class=p>]</span>
</span><span id=__span-9-35><a id=__codelineno-9-35 name=__codelineno-9-35 href=#__codelineno-9-35></a>
</span><span id=__span-9-36><a id=__codelineno-9-36 name=__codelineno-9-36 href=#__codelineno-9-36></a>    <span class=k>return</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>attention_mask</span>
</span></code></pre></div> <p><strong>Key benefits</strong>: Pre-allocation to avoid repeated tensor creation, vectorized vocabulary lookup for entire batches, and memory layout optimization with contiguous tensor storage.</p> <div class="admonition success"> <p class=admonition-title>The Complete Solution</p> <p>By combining these five key innovations, we achieved <strong>6.4x performance improvement</strong> over the standard PyTorch approach:</p> <ol> <li><strong>Contiguous Reads</strong>: Switch from random to contiguous I/O with two-level hierarchy</li> <li><strong>Single-Threaded Prefetching</strong>: Eliminate multiprocessing overhead with async threading</li> <li><strong>Vectorized Window Functions</strong>: Leverage Polars' Rust-accelerated columnar processing</li> <li><strong>Block Shuffling</strong>: Preserve data locality while enabling randomization</li> <li><strong>Vectorized Tokenization</strong>: Batch process sequences with numpy optimization</li> </ol> <p>The result: <strong>28,207 cells/second</strong>, enabling GPU-saturating throughput for single-cell foundation model training.</p> </div> <h2 id=the-optimized-pipeline>The Optimized Pipeline<a class=headerlink href=#the-optimized-pipeline title="Permanent link">&para;</a></h2> <pre class=mermaid><code>%%{init: {'theme': 'base', 'themeVariables': { 'fontFamily': 'Comic Sans MS, cursive', 'fontSize': '14px', 'primaryColor': '#4ecdc4', 'primaryTextColor': '#2c3e50', 'primaryBorderColor': '#27ae60', 'lineColor': '#3498db', 'secondaryColor': '#f39c12', 'tertiaryColor': '#e74c3c', 'noteBkgColor': '#f8f9fa', 'noteBorderColor': '#dee2e6', 'messageFontStyle': 'italic', 'messageFontSize': '12px'}}}%%
sequenceDiagram
    participant S as "🏗️ Lance Storage"
    participant P as "⚡ Prefetcher"
    participant T as "🔄 Transformer"
    participant Q as "📦 Queue"
    participant D as "🎯 DataLoader"
    participant G as "🚀 GPU"

    Note over S,G: Optimized Data Pipeline Flow

    S-&gt;&gt;P: 📊 Arrow Batches&lt;br/&gt;(8192 records)
    P-&gt;&gt;P: 🔄 Aggregate 100 batches → 1 chunk
    P-&gt;&gt;T: 📦 Chunk (~819k records)
    T-&gt;&gt;T: 🎲 Shuffle cells
    T-&gt;&gt;T: 📈 Window functions (gene ranking)
    T-&gt;&gt;T: 🔤 Tokenize sequences
    T-&gt;&gt;Q: 🎯 Training batches (32 cells)
    Q-&gt;&gt;D: ⚡ Pre-tokenized tensors
    D-&gt;&gt;G: 🚀 GPU-ready data

    Note over G: Ready for training! 🎉</code></pre> <h2 id=performance-comparison>Performance Comparison<a class=headerlink href=#performance-comparison title="Permanent link">&para;</a></h2> <table> <thead> <tr> <th>Approach</th> <th>Performance</th> <th>Bottleneck</th> </tr> </thead> <tbody> <tr> <td>Standard PyTorch</td> <td>4,408.4 cells/sec</td> <td>Loading and Preprocessing</td> </tr> <tr> <td>SQL Window Functions</td> <td>4,582.4 cells/sec</td> <td>Expensive range scans</td> </tr> <tr> <td>Direct Random Access</td> <td>7,086.3 cells/sec</td> <td>Pandas window functions</td> </tr> <tr> <td>Polars Window Functions</td> <td>16,619.7 cells/sec</td> <td>Small batch size</td> </tr> <tr> <td>Deconstructed DataLoader</td> <td><strong>28,207.1 cells/sec</strong></td> <td><strong>None</strong></td> </tr> </tbody> </table> <h2 id=results-and-future-directions>Results and Future Directions<a class=headerlink href=#results-and-future-directions title="Permanent link">&para;</a></h2> <p>Our deconstructed approach achieved <strong>6.4x performance improvement</strong> over standard PyTorch DataLoaders, reaching <strong>28,207 cells/second</strong>. This enables GPU-saturating throughput for single-cell foundation model training.</p> <p>The key insight is that the biggest performance gains come from <strong>rethinking what each component should be responsible for</strong> rather than optimizing within existing boundaries. By challenging PyTorch's traditional role assumptions and embracing domain-specific optimizations for genomics data, we achieved order-of-magnitude improvements.</p> <p>As Python moves toward removing the GIL and hardware continues to evolve, thread-based approaches will become even more attractive. The future of high-performance data loading lies not in adding more parallelism, but in smarter orchestration of the entire data pipeline.</p> <p><strong>What's next?</strong></p> <ul> <li> <p>We're actively collaborating with teams developing dataloaders for this corner of biology: the scanpy team on <a href=https://anndata.readthedocs.io/en/latest/generated/anndata.experimental.AnnLoader.html>AnnLoader</a>, the SCVI team on <a href=https://docs.scvi-tools.org/en/stable/api/reference/scvi.dataloaders.AnnDataLoader.html>AnnDataLoader</a>, the <a href=https://lamin.ai/ >Lamin</a> team on <a href=https://blog.lamin.ai/arrayloader-benchmarks>arrayloaders</a>, and the <a href=https://arxiv.org/abs/2506.01883>scDataset</a> team, to expand the scope of fair dataloader benchmarks. Please reach out if you're developing one and would like to participate!</p> </li> <li> <p>We're working on integration with <a href=https://docs.ray.io/en/latest/train/train.html>Ray Train</a> in order to make multi-node, multi-GPU training seamless and performant. If you're a developer or experienced user of Ray, or if you're a GPU cloud provider that wants to sponsor foundation model training run benchmarks, we want to hear from you!</p> </li> </ul> <p>The complete implementation of <code>SLAFDataLoader</code> is available on <a href=https://github.com/slaf-project/slaf>Github</a>, along with <a href=https://slaf-project.github.io/slaf/examples/ml-training/ >example notebooks</a> and comprehensive <a href=https://slaf-project.github.io/slaf/api/ml/ >API documentation</a>. Go forth and train!</p> <hr> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 22, 2025 17:17:33 UTC">August 22, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 21, 2025 18:00:05 UTC">August 21, 2025</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Contributors> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 4a4 4 0 0 1 4 4 4 4 0 0 1-4 4 4 4 0 0 1-4-4 4 4 0 0 1 4-4m0 10c4.42 0 8 1.79 8 4v2H4v-2c0-2.21 3.58-4 8-4"/></svg> </span> <nav> Pavan Ramkumar </nav> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2026 Pavan Ramkumar </div> </div> <div class=md-social> <a href=https://github.com/slaf-project/slaf target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../..", "features": ["navigation.sections", "content.code.copy"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.79ae519e.min.js></script> <script src=../../javascripts/mathjax.js></script> <script src=../../javascripts/theme-reset.js></script> <script src=../../javascripts/copy-enhancement.js></script> <script src=../../javascripts/theme-aware-assets.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>